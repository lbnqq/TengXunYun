#!/usr/bin/env python3
"""
CLIæµ‹è¯•åˆ†æå™¨ - åŸºäºé¡¹ç›®å®ªæ³•çš„å·¥ç¨‹å¯ç”¨æ€§æŒç»­æ”¹è¿›
åŠŸèƒ½ï¼šå®šæœŸå¤ç›˜æµ‹è¯•æŠ¥å‘Šï¼Œè¡¥å……é—æ¼åœºæ™¯å’Œè¾¹ç•Œç”¨ä¾‹ï¼ŒæŒç»­æå‡å·¥ç¨‹å¯ç”¨æ€§
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict


class CLITestAnalyzer:
    """CLIæµ‹è¯•åˆ†æå™¨ - å·¥ç¨‹å¯ç”¨æ€§æŒç»­æ”¹è¿›"""
    
    def __init__(self, test_results_dir: str = "cliTests/test_results"):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.test_results_dir = Path(test_results_dir)
        self.analysis_results = {
            "analysis_time": datetime.now().isoformat(),
            "test_coverage": {},
            "failure_patterns": {},
            "performance_issues": {},
            "missing_scenarios": {},
            "recommendations": [],
            "improvement_plan": {}
        }
    
    def analyze_test_reports(self, days: int = 30) -> Dict[str, Any]:
        """åˆ†ææŒ‡å®šå¤©æ•°å†…çš„æµ‹è¯•æŠ¥å‘Š"""
        print(f"ğŸ” åˆ†ææœ€è¿‘ {days} å¤©çš„CLIæµ‹è¯•æŠ¥å‘Š...")
        
        # æŸ¥æ‰¾æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶
        report_files = self._find_test_reports(days)
        
        if not report_files:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶")
            return self.analysis_results
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(report_files)} ä¸ªæµ‹è¯•æŠ¥å‘Šæ–‡ä»¶")
        
        # åˆ†ææ¯ä¸ªæŠ¥å‘Š
        all_test_results = []
        for report_file in report_files:
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    report = json.load(f)
                    all_test_results.append(report)
            except Exception as e:
                print(f"âš ï¸ è¯»å–æŠ¥å‘Šæ–‡ä»¶å¤±è´¥ {report_file}: {e}")
        
        if not all_test_results:
            print("âŒ æ— æ³•è¯»å–ä»»ä½•æµ‹è¯•æŠ¥å‘Š")
            return self.analysis_results
        
        # æ‰§è¡Œåˆ†æ
        self._analyze_test_coverage(all_test_results)
        self._analyze_failure_patterns(all_test_results)
        self._analyze_performance_issues(all_test_results)
        self._identify_missing_scenarios(all_test_results)
        self._generate_recommendations()
        self._create_improvement_plan()
        
        return self.analysis_results
    
    def _find_test_reports(self, days: int) -> List[Path]:
        """æŸ¥æ‰¾æŒ‡å®šå¤©æ•°å†…çš„æµ‹è¯•æŠ¥å‘Š"""
        cutoff_date = datetime.now() - timedelta(days=days)
        report_files = []
        
        if not self.test_results_dir.exists():
            return report_files
        
        for file_path in self.test_results_dir.glob("batch_test_report_*.json"):
            try:
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                file_date_str = file_path.stem.replace("batch_test_report_", "")
                file_date = datetime.strptime(file_date_str, "%Y%m%d_%H%M%S")
                
                if file_date >= cutoff_date:
                    report_files.append(file_path)
            except ValueError:
                # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸åŒ¹é…ï¼Œæ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                if file_mtime >= cutoff_date:
                    report_files.append(file_path)
        
        # ä¹Ÿæ£€æŸ¥æœ€æ–°çš„ batch_test_report.json
        latest_report = self.test_results_dir / "batch_test_report.json"
        if latest_report.exists():
            file_mtime = datetime.fromtimestamp(latest_report.stat().st_mtime)
            if file_mtime >= cutoff_date:
                report_files.append(latest_report)
        
        return sorted(report_files)
    
    def _analyze_test_coverage(self, reports: List[Dict]) -> None:
        """åˆ†ææµ‹è¯•è¦†ç›–ç‡"""
        print("ğŸ“ˆ åˆ†ææµ‹è¯•è¦†ç›–ç‡...")
        
        coverage_stats = {
            "total_runs": len(reports),
            "test_categories": defaultdict(lambda: {"runs": 0, "passes": 0, "failures": 0}),
            "priority_distribution": defaultdict(lambda: {"runs": 0, "passes": 0, "failures": 0}),
            "success_rate_trend": []
        }
        
        for report in reports:
            if "tests" not in report:
                continue
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            for test in report["tests"]:
                category = test.get("category", "æœªçŸ¥")
                priority = test.get("priority", "æœªçŸ¥")
                
                coverage_stats["test_categories"][category]["runs"] += 1
                coverage_stats["priority_distribution"][priority]["runs"] += 1
                
                if test.get("success", False):
                    coverage_stats["test_categories"][category]["passes"] += 1
                    coverage_stats["priority_distribution"][priority]["passes"] += 1
                else:
                    coverage_stats["test_categories"][category]["failures"] += 1
                    coverage_stats["priority_distribution"][priority]["failures"] += 1
            
            # æˆåŠŸç‡è¶‹åŠ¿
            if "test_run_info" in report:
                success_rate = report["test_run_info"].get("success_rate", 0)
                coverage_stats["success_rate_trend"].append({
                    "date": report["test_run_info"].get("start_time", ""),
                    "success_rate": success_rate
                })
        
        self.analysis_results["test_coverage"] = dict(coverage_stats)
    
    def _analyze_failure_patterns(self, reports: List[Dict]) -> None:
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        print("ğŸ” åˆ†æå¤±è´¥æ¨¡å¼...")
        
        failure_patterns = {
            "common_errors": defaultdict(int),
            "failing_tests": defaultdict(int),
            "error_categories": defaultdict(int),
            "suggestion_frequency": defaultdict(int)
        }
        
        for report in reports:
            if "tests" not in report:
                continue
            
            for test in report["tests"]:
                if not test.get("success", True):
                    test_name = test.get("name", "æœªçŸ¥")
                    error_msg = test.get("error", "")
                    suggestion = test.get("suggestion", "")
                    
                    failure_patterns["failing_tests"][test_name] += 1
                    failure_patterns["suggestion_frequency"][suggestion] += 1
                    
                    # åˆ†æé”™è¯¯ç±»å‹
                    error_lower = error_msg.lower()
                    if "import" in error_lower:
                        failure_patterns["error_categories"]["ä¾èµ–å¯¼å…¥é”™è¯¯"] += 1
                    elif "connection" in error_lower or "timeout" in error_lower:
                        failure_patterns["error_categories"]["ç½‘ç»œè¿æ¥é”™è¯¯"] += 1
                    elif "file not found" in error_lower:
                        failure_patterns["error_categories"]["æ–‡ä»¶è·¯å¾„é”™è¯¯"] += 1
                    elif "permission" in error_lower:
                        failure_patterns["error_categories"]["æƒé™é”™è¯¯"] += 1
                    elif "json" in error_lower:
                        failure_patterns["error_categories"]["æ•°æ®æ ¼å¼é”™è¯¯"] += 1
                    elif "api" in error_lower:
                        failure_patterns["error_categories"]["APIæ¥å£é”™è¯¯"] += 1
                    else:
                        failure_patterns["error_categories"]["å…¶ä»–é”™è¯¯"] += 1
                    
                    # è®°å½•å¸¸è§é”™è¯¯
                    failure_patterns["common_errors"][error_msg] += 1
        
        self.analysis_results["failure_patterns"] = dict(failure_patterns)
    
    def _analyze_performance_issues(self, reports: List[Dict]) -> None:
        """åˆ†ææ€§èƒ½é—®é¢˜"""
        print("âš¡ åˆ†ææ€§èƒ½é—®é¢˜...")
        
        performance_issues = {
            "slow_tests": [],
            "performance_trends": [],
            "timeout_issues": 0,
            "avg_duration_by_test": defaultdict(list)
        }
        
        for report in reports:
            if "tests" not in report:
                continue
            
            for test in report["tests"]:
                test_name = test.get("name", "æœªçŸ¥")
                duration = test.get("duration", 0)
                
                performance_issues["avg_duration_by_test"][test_name].append(duration)
                
                # è¯†åˆ«æ…¢é€Ÿæµ‹è¯•
                if duration > 60:  # è¶…è¿‡1åˆ†é’Ÿ
                    performance_issues["slow_tests"].append({
                        "test_name": test_name,
                        "duration": duration,
                        "date": report.get("test_run_info", {}).get("start_time", "")
                    })
                
                # è¯†åˆ«è¶…æ—¶é—®é¢˜
                if "timeout" in test.get("error", "").lower():
                    performance_issues["timeout_issues"] += 1
        
        # è®¡ç®—å¹³å‡è€—æ—¶
        for test_name, durations in performance_issues["avg_duration_by_test"].items():
            if durations:
                avg_duration = sum(durations) / len(durations)
                performance_issues["performance_trends"].append({
                    "test_name": test_name,
                    "avg_duration": avg_duration,
                    "max_duration": max(durations),
                    "min_duration": min(durations)
                })
        
        self.analysis_results["performance_issues"] = performance_issues
    
    def _identify_missing_scenarios(self, reports: List[Dict]) -> None:
        """è¯†åˆ«é—æ¼çš„æµ‹è¯•åœºæ™¯"""
        print("ğŸ¯ è¯†åˆ«é—æ¼çš„æµ‹è¯•åœºæ™¯...")
        
        missing_scenarios = {
            "edge_cases": [],
            "boundary_tests": [],
            "integration_scenarios": [],
            "error_handling": [],
            "data_validation": []
        }
        
        # åŸºäºé¡¹ç›®å®ªæ³•çš„ä¸šåŠ¡åœºæ™¯è¦æ±‚
        required_scenarios = {
            "edge_cases": [
                "ç©ºæ–‡ä»¶å¤„ç†",
                "è¶…å¤§æ–‡ä»¶å¤„ç†",
                "ç‰¹æ®Šå­—ç¬¦å¤„ç†",
                "ç¼–ç æ ¼å¼å¤„ç†",
                "ç½‘ç»œå¼‚å¸¸å¤„ç†"
            ],
            "boundary_tests": [
                "æ•°æ®è¾¹ç•Œå€¼æµ‹è¯•",
                "å¹¶å‘å¤„ç†æµ‹è¯•",
                "å†…å­˜é™åˆ¶æµ‹è¯•",
                "ç£ç›˜ç©ºé—´æµ‹è¯•"
            ],
            "integration_scenarios": [
                "å¤šæ¨¡å—é›†æˆæµ‹è¯•",
                "ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•",
                "æ•°æ®æµå®Œæ•´æ€§æµ‹è¯•",
                "çŠ¶æ€ä¸€è‡´æ€§æµ‹è¯•"
            ],
            "error_handling": [
                "å¼‚å¸¸è¾“å…¥å¤„ç†",
                "é”™è¯¯æ¢å¤æµ‹è¯•",
                "æ—¥å¿—è®°å½•æµ‹è¯•",
                "é”™è¯¯æŠ¥å‘Šæµ‹è¯•"
            ],
            "data_validation": [
                "æ•°æ®æ ¼å¼éªŒè¯",
                "æ•°æ®å®Œæ•´æ€§æ£€æŸ¥",
                "æ•°æ®ä¸€è‡´æ€§éªŒè¯",
                "æ•°æ®å®‰å…¨æ€§æ£€æŸ¥"
            ]
        }
        
        # æ£€æŸ¥ç°æœ‰æµ‹è¯•è¦†ç›–æƒ…å†µ
        existing_tests = set()
        for report in reports:
            if "tests" in report:
                for test in report["tests"]:
                    existing_tests.add(test.get("name", ""))
        
        # è¯†åˆ«ç¼ºå¤±çš„åœºæ™¯
        for category, scenarios in required_scenarios.items():
            for scenario in scenarios:
                if not any(scenario.lower() in test.lower() for test in existing_tests):
                    missing_scenarios[category].append(scenario)
        
        self.analysis_results["missing_scenarios"] = missing_scenarios
    
    def _generate_recommendations(self) -> None:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print("ğŸ’¡ ç”Ÿæˆæ”¹è¿›å»ºè®®...")
        
        recommendations = []
        
        # åŸºäºå¤±è´¥æ¨¡å¼ç”Ÿæˆå»ºè®®
        failure_patterns = self.analysis_results["failure_patterns"]
        if failure_patterns.get("failing_tests"):
            most_failing_test = max(failure_patterns["failing_tests"].items(), key=lambda x: x[1])
            recommendations.append(f"é‡ç‚¹å…³æ³¨æµ‹è¯• '{most_failing_test[0]}'ï¼Œå·²å¤±è´¥ {most_failing_test[1]} æ¬¡")
        
        # åŸºäºæ€§èƒ½é—®é¢˜ç”Ÿæˆå»ºè®®
        performance_issues = self.analysis_results["performance_issues"]
        if performance_issues.get("slow_tests"):
            recommendations.append(f"å‘ç° {len(performance_issues['slow_tests'])} ä¸ªæ…¢é€Ÿæµ‹è¯•ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½")
        
        # åŸºäºç¼ºå¤±åœºæ™¯ç”Ÿæˆå»ºè®®
        missing_scenarios = self.analysis_results["missing_scenarios"]
        total_missing = sum(len(scenarios) for scenarios in missing_scenarios.values())
        if total_missing > 0:
            recommendations.append(f"å‘ç° {total_missing} ä¸ªé—æ¼çš„æµ‹è¯•åœºæ™¯ï¼Œå»ºè®®è¡¥å……æµ‹è¯•ç”¨ä¾‹")
        
        # åŸºäºé¡¹ç›®å®ªæ³•çš„å»ºè®®
        recommendations.extend([
            "å®šæœŸè¿è¡ŒCLIæµ‹è¯•ï¼Œç¡®ä¿å·¥ç¨‹å¯ç”¨æ€§",
            "å»ºç«‹æµ‹è¯•å¤±è´¥å¿«é€Ÿå“åº”æœºåˆ¶",
            "æŒç»­ä¼˜åŒ–æµ‹è¯•æ€§èƒ½ï¼Œæé«˜åé¦ˆé€Ÿåº¦",
            "åŠ å¼ºè¾¹ç•Œç”¨ä¾‹å’Œå¼‚å¸¸åœºæ™¯æµ‹è¯•",
            "å»ºç«‹æµ‹è¯•è¦†ç›–ç‡ç›‘æ§æœºåˆ¶"
        ])
        
        self.analysis_results["recommendations"] = recommendations
    
    def _create_improvement_plan(self) -> None:
        """åˆ›å»ºæ”¹è¿›è®¡åˆ’"""
        print("ğŸ“‹ åˆ›å»ºæ”¹è¿›è®¡åˆ’...")
        
        improvement_plan = {
            "immediate_actions": [],
            "short_term_goals": [],
            "long_term_objectives": [],
            "priority_tasks": []
        }
        
        # ç«‹å³è¡ŒåŠ¨
        failure_patterns = self.analysis_results["failure_patterns"]
        if failure_patterns.get("failing_tests"):
            improvement_plan["immediate_actions"].append("ä¿®å¤é¢‘ç¹å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        
        if self.analysis_results["performance_issues"].get("slow_tests"):
            improvement_plan["immediate_actions"].append("ä¼˜åŒ–æ…¢é€Ÿæµ‹è¯•çš„æ€§èƒ½")
        
        # çŸ­æœŸç›®æ ‡
        missing_scenarios = self.analysis_results["missing_scenarios"]
        if any(missing_scenarios.values()):
            improvement_plan["short_term_goals"].append("è¡¥å……é—æ¼çš„æµ‹è¯•åœºæ™¯")
        
        improvement_plan["short_term_goals"].extend([
            "å»ºç«‹æµ‹è¯•è‡ªåŠ¨åŒ–ç›‘æ§",
            "å®Œå–„é”™è¯¯å¤„ç†æœºåˆ¶",
            "ä¼˜åŒ–æµ‹è¯•æ•°æ®ç®¡ç†"
        ])
        
        # é•¿æœŸç›®æ ‡
        improvement_plan["long_term_objectives"].extend([
            "å®ç°100%æµ‹è¯•è¦†ç›–ç‡",
            "å»ºç«‹æŒç»­æ”¹è¿›æœºåˆ¶",
            "æå‡å·¥ç¨‹å¯ç”¨æ€§åˆ°99.9%",
            "å»ºç«‹å®Œæ•´çš„æµ‹è¯•ç”Ÿæ€"
        ])
        
        # ä¼˜å…ˆçº§ä»»åŠ¡
        improvement_plan["priority_tasks"] = [
            "ä¿®å¤P1ä¼˜å…ˆçº§æµ‹è¯•å¤±è´¥",
            "è¡¥å……è¾¹ç•Œç”¨ä¾‹æµ‹è¯•",
            "ä¼˜åŒ–æµ‹è¯•æ‰§è¡Œæ€§èƒ½",
            "å»ºç«‹æµ‹è¯•æŠ¥å‘Šåˆ†ææœºåˆ¶"
        ]
        
        self.analysis_results["improvement_plan"] = improvement_plan
    
    def generate_analysis_report(self, output_file: Optional[str] = None) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"cli_test_analysis_report_{timestamp}.json"
        
        output_path = self.test_results_dir / output_file
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        return str(output_path)
    
    def generate_html_report(self, output_file: Optional[str] = None) -> str:
        """ç”ŸæˆHTMLåˆ†ææŠ¥å‘Š"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"cli_test_analysis_report_{timestamp}.html"
        
        output_path = self.test_results_dir / output_file
        
        html_content = self._generate_html_content()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(output_path)
    
    def _generate_html_content(self) -> str:
        """ç”ŸæˆHTMLå†…å®¹"""
        return f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CLIæµ‹è¯•åˆ†ææŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; text-align: center; }}
        .section {{ background: #f8f9fa; margin: 20px 0; padding: 20px; border-radius: 8px; border-left: 4px solid #007bff; }}
        .metric {{ background: white; padding: 15px; margin: 10px 0; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .success {{ border-left-color: #28a745; }}
        .warning {{ border-left-color: #ffc107; }}
        .danger {{ border-left-color: #dc3545; }}
        .chart {{ background: white; padding: 20px; margin: 15px 0; border-radius: 8px; }}
        .recommendation {{ background: #e7f3ff; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #007bff; }}
        .priority {{ font-weight: bold; color: #dc3545; }}
        .improvement {{ background: #f0f8f0; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #28a745; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¯ CLIæµ‹è¯•åˆ†ææŠ¥å‘Š</h1>
        <p>åŸºäºé¡¹ç›®å®ªæ³•çš„å·¥ç¨‹å¯ç”¨æ€§æŒç»­æ”¹è¿›åˆ†æ</p>
        <p>åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>

    <div class="section">
        <h2>ğŸ“Š æµ‹è¯•è¦†ç›–ç‡åˆ†æ</h2>
        <div class="metric">
            <h3>æ€»ä½“ç»Ÿè®¡</h3>
            <p><strong>åˆ†ææŠ¥å‘Šæ•°:</strong> {self.analysis_results['test_coverage'].get('total_runs', 0)}</p>
        </div>
        
        <div class="chart">
            <h3>æµ‹è¯•ç±»åˆ«åˆ†å¸ƒ</h3>
            {self._generate_category_chart()}
        </div>
        
        <div class="chart">
            <h3>ä¼˜å…ˆçº§åˆ†å¸ƒ</h3>
            {self._generate_priority_chart()}
        </div>
    </div>

    <div class="section">
        <h2>ğŸ” å¤±è´¥æ¨¡å¼åˆ†æ</h2>
        <div class="metric danger">
            <h3>å¸¸è§é”™è¯¯ç±»å‹</h3>
            {self._generate_error_chart()}
        </div>
        
        <div class="metric warning">
            <h3>é¢‘ç¹å¤±è´¥çš„æµ‹è¯•</h3>
            {self._generate_failing_tests_chart()}
        </div>
    </div>

    <div class="section">
        <h2>âš¡ æ€§èƒ½é—®é¢˜åˆ†æ</h2>
        <div class="metric warning">
            <h3>æ…¢é€Ÿæµ‹è¯•</h3>
            {self._generate_performance_chart()}
        </div>
    </div>

    <div class="section">
        <h2>ğŸ¯ é—æ¼åœºæ™¯è¯†åˆ«</h2>
        <div class="metric">
            <h3>ç¼ºå¤±çš„æµ‹è¯•åœºæ™¯</h3>
            {self._generate_missing_scenarios_chart()}
        </div>
    </div>

    <div class="section">
        <h2>ğŸ’¡ æ”¹è¿›å»ºè®®</h2>
        {self._generate_recommendations_html()}
    </div>

    <div class="section">
        <h2>ğŸ“‹ æ”¹è¿›è®¡åˆ’</h2>
        {self._generate_improvement_plan_html()}
    </div>

    <div class="header">
        <h2>ğŸ¯ é¡¹ç›®å®ªæ³•åˆè§„æ€§</h2>
        <p>æœ¬åˆ†æåŸºäºã€ŠAIç¼–ç¨‹é¡¹ç›®ç»ˆæå®è·µæ‰‹å†Œã€‹çš„å·¥ç¨‹å¯ç”¨æ€§è¦æ±‚</p>
        <p>æŒç»­æ”¹è¿›æ˜¯é¡¹ç›®å®ªæ³•çš„æ ¸å¿ƒåŸåˆ™</p>
    </div>
</body>
</html>
"""
    
    def _generate_category_chart(self) -> str:
        """ç”Ÿæˆç±»åˆ«åˆ†å¸ƒå›¾è¡¨"""
        categories = self.analysis_results.get("test_coverage", {}).get("test_categories", {})
        if not categories:
            return "<p>æš‚æ— æ•°æ®</p>"
        
        html = "<ul>"
        for category, stats in categories.items():
            total = stats["runs"]
            passes = stats["passes"]
            success_rate = (passes / total * 100) if total > 0 else 0
            html += f"<li><strong>{category}:</strong> {passes}/{total} ({success_rate:.1f}%)</li>"
        html += "</ul>"
        return html
    
    def _generate_priority_chart(self) -> str:
        """ç”Ÿæˆä¼˜å…ˆçº§åˆ†å¸ƒå›¾è¡¨"""
        priorities = self.analysis_results.get("test_coverage", {}).get("priority_distribution", {})
        if not priorities:
            return "<p>æš‚æ— æ•°æ®</p>"
        
        html = "<ul>"
        for priority, stats in priorities.items():
            total = stats["runs"]
            passes = stats["passes"]
            success_rate = (passes / total * 100) if total > 0 else 0
            status_class = "success" if success_rate >= 90 else "warning" if success_rate >= 70 else "danger"
            html += f"<li class='{status_class}'><strong>{priority}:</strong> {passes}/{total} ({success_rate:.1f}%)</li>"
        html += "</ul>"
        return html
    
    def _generate_error_chart(self) -> str:
        """ç”Ÿæˆé”™è¯¯ç±»å‹å›¾è¡¨"""
        error_categories = self.analysis_results.get("failure_patterns", {}).get("error_categories", {})
        if not error_categories:
            return "<p>æš‚æ— é”™è¯¯æ•°æ®</p>"
        
        html = "<ul>"
        for error_type, count in sorted(error_categories.items(), key=lambda x: x[1], reverse=True):
            html += f"<li><strong>{error_type}:</strong> {count}æ¬¡</li>"
        html += "</ul>"
        return html
    
    def _generate_failing_tests_chart(self) -> str:
        """ç”Ÿæˆå¤±è´¥æµ‹è¯•å›¾è¡¨"""
        failing_tests = self.analysis_results.get("failure_patterns", {}).get("failing_tests", {})
        if not failing_tests:
            return "<p>æš‚æ— å¤±è´¥æµ‹è¯•æ•°æ®</p>"
        
        html = "<ul>"
        for test_name, count in sorted(failing_tests.items(), key=lambda x: x[1], reverse=True)[:5]:
            html += f"<li><strong>{test_name}:</strong> å¤±è´¥{count}æ¬¡</li>"
        html += "</ul>"
        return html
    
    def _generate_performance_chart(self) -> str:
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        slow_tests = self.analysis_results.get("performance_issues", {}).get("slow_tests", [])
        if not slow_tests:
            return "<p>æš‚æ— æ…¢é€Ÿæµ‹è¯•æ•°æ®</p>"
        
        html = "<ul>"
        for test in slow_tests[:5]:
            html += f"<li><strong>{test['test_name']}:</strong> {test['duration']:.2f}ç§’</li>"
        html += "</ul>"
        return html
    
    def _generate_missing_scenarios_chart(self) -> str:
        """ç”Ÿæˆç¼ºå¤±åœºæ™¯å›¾è¡¨"""
        missing_scenarios = self.analysis_results.get("missing_scenarios", {})
        if not any(missing_scenarios.values()):
            return "<p>æš‚æ— ç¼ºå¤±åœºæ™¯</p>"
        
        html = "<ul>"
        for category, scenarios in missing_scenarios.items():
            if scenarios:
                html += f"<li><strong>{category}:</strong> {', '.join(scenarios)}</li>"
        html += "</ul>"
        return html
    
    def _generate_recommendations_html(self) -> str:
        """ç”Ÿæˆå»ºè®®HTML"""
        recommendations = self.analysis_results.get("recommendations", [])
        if not recommendations:
            return "<p>æš‚æ— å»ºè®®</p>"
        
        html = ""
        for rec in recommendations:
            html += f'<div class="recommendation">{rec}</div>'
        return html
    
    def _generate_improvement_plan_html(self) -> str:
        """ç”Ÿæˆæ”¹è¿›è®¡åˆ’HTML"""
        plan = self.analysis_results.get("improvement_plan", {})
        
        html = ""
        for section, items in plan.items():
            if items:
                html += f"<h3>{section.replace('_', ' ').title()}</h3>"
                html += "<ul>"
                for item in items:
                    html += f"<li>{item}</li>"
                html += "</ul>"
        
        return html


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CLIæµ‹è¯•åˆ†æå™¨")
    parser.add_argument("--days", type=int, default=30, help="åˆ†ææœ€è¿‘Nå¤©çš„æµ‹è¯•æŠ¥å‘Š")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("--html", action="store_true", help="ç”ŸæˆHTMLæŠ¥å‘Š")
    
    args = parser.parse_args()
    
    analyzer = CLITestAnalyzer()
    
    # æ‰§è¡Œåˆ†æ
    analysis_results = analyzer.analyze_test_reports(args.days)
    
    # ç”ŸæˆæŠ¥å‘Š
    json_report = analyzer.generate_analysis_report(args.output)
    print(f"ğŸ“„ JSONåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {json_report}")
    
    if args.html:
        html_report = analyzer.generate_html_report()
        print(f"ğŸ“„ HTMLåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {html_report}")
    
    # è¾“å‡ºå…³é”®å‘ç°
    print("\nğŸ¯ å…³é”®å‘ç°:")
    if analysis_results["failure_patterns"].get("failing_tests"):
        most_failing = max(analysis_results["failure_patterns"]["failing_tests"].items(), key=lambda x: x[1])
        print(f"  æœ€é¢‘ç¹å¤±è´¥çš„æµ‹è¯•: {most_failing[0]} (å¤±è´¥{most_failing[1]}æ¬¡)")
    
    if analysis_results["performance_issues"].get("slow_tests"):
        print(f"  å‘ç° {len(analysis_results['performance_issues']['slow_tests'])} ä¸ªæ…¢é€Ÿæµ‹è¯•")
    
    missing_count = sum(len(scenarios) for scenarios in analysis_results["missing_scenarios"].values())
    if missing_count > 0:
        print(f"  å‘ç° {missing_count} ä¸ªé—æ¼çš„æµ‹è¯•åœºæ™¯")
    
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®æ•°é‡: {len(analysis_results['recommendations'])}")
    print("âœ… åˆ†æå®Œæˆï¼Œè¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶")


if __name__ == "__main__":
    main() 