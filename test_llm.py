#!/usr/bin/env python3
"""
LLMæ¥å£è¿é€šæ€§æµ‹è¯•è„šæœ¬
"""

import os
import json
from dotenv import load_dotenv
from src.llm_clients.xingcheng_llm import XingchengLLMClient

def test_llm_connection():
    """æµ‹è¯•LLMæ¥å£è¿é€šæ€§"""
    print("=== ç§‘å¤§è®¯é£æ˜Ÿè¾°å¹³å° X1 æ¨¡å‹æ¥å£æµ‹è¯• ===\n")
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è·å–é…ç½®
    api_key = os.getenv("XINGCHENG_API_KEY")
    api_secret = os.getenv("XINGCHENG_API_SECRET")
    model_name = os.getenv("LLM_MODEL_NAME", "x1")
    
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° XINGCHENG_API_KEY ç¯å¢ƒå˜é‡")
        print("è¯·æ£€æŸ¥ .env æ–‡ä»¶é…ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡")
        print("è·å–APIå¯†é’¥åœ°å€: https://console.xfyun.cn/services/bmx1")
        return False
    
    print(f"âœ… API Key: {api_key[:10]}...{api_key[-10:] if len(api_key) > 20 else '***'}")
    print(f"âœ… Model: {model_name}")
    print(f"âœ… API URL: https://spark-api-open.xf-yun.com/v2/chat/completions")
    print()
    
    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        print("ğŸ”„ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
        llm_client = XingchengLLMClient(
            api_key=api_key,
            api_secret=api_secret,
            model_name=model_name
        )
        print("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        print()
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        print("ğŸ”„ æµ‹è¯•ç®€å•å¯¹è¯...")
        test_prompt = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        print(f"å‘é€å†…å®¹: {test_prompt}")
        
        response = llm_client.generate(test_prompt, temperature=0.7, max_tokens=100)
        
        print("âœ… æ”¶åˆ°å“åº”:")
        print("-" * 50)
        print(response)
        print("-" * 50)
        
        # æµ‹è¯•JSONæ ¼å¼å“åº”
        print("\nğŸ”„ æµ‹è¯•JSONæ ¼å¼å“åº”...")
        json_prompt = """è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
        
        æ–‡æ¡£å†…å®¹ï¼šè¿™æ˜¯ä¸€ä¸ªäº§å“éœ€æ±‚æ–‡æ¡£ï¼ŒåŒ…å«äº†ç”¨æˆ·æ•…äº‹ã€æŠ€æœ¯æ ˆé€‰æ‹©å’Œå¼€å‘è®¡åˆ’ã€‚
        
        è¯·è¿”å›JSONæ ¼å¼ï¼š
        {
            "inferred_scenario": "åœºæ™¯ç±»å‹",
            "supporting_evidence": "æ”¯æŒè¯æ®",
            "inferred_reporter_role": "ä½œè€…è§’è‰²",
            "inferred_reader_role": "è¯»è€…è§’è‰²"
        }"""
        
        print(f"å‘é€å†…å®¹: {json_prompt[:100]}...")
        
        json_response = llm_client.generate(json_prompt, temperature=0.3, max_tokens=200)
        
        print("âœ… æ”¶åˆ°JSONå“åº”:")
        print("-" * 50)
        print(json_response)
        print("-" * 50)
        
        # å°è¯•è§£æJSON
        try:
            parsed_json = json.loads(json_response)
            print("âœ… JSONè§£ææˆåŠŸ")
            print(f"æ¨æ–­åœºæ™¯: {parsed_json.get('inferred_scenario', 'N/A')}")
            print(f"ä½œè€…è§’è‰²: {parsed_json.get('inferred_reporter_role', 'N/A')}")
        except json.JSONDecodeError:
            print("âš ï¸  JSONè§£æå¤±è´¥ï¼Œä½†APIè°ƒç”¨æˆåŠŸ")
        
        print("\nğŸ‰ LLMæ¥å£æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("\nå¯èƒ½çš„åŸå› :")
        print("1. APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ")
        print("2. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("3. APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        print("4. è¯·æ±‚æ ¼å¼é”™è¯¯")
        return False

if __name__ == "__main__":
    success = test_llm_connection()
    if success:
        print("\nâœ… æ¥å£æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨LLMåŠŸèƒ½")
    else:
        print("\nâŒ æ¥å£æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®") 