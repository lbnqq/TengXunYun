import os
import sys
import json
import uuid
import traceback
from datetime import datetime
from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS
from werkzeug.utils import secure_filename
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.core.agent.agent_orchestrator import AgentOrchestrator
from src.llm_clients.xingcheng_llm import XingchengLLMClient
from src.llm_clients.multi_llm import MultiLLMClient
from src.core.tools.format_alignment_coordinator import FormatAlignmentCoordinator
from src.core.tools.document_fill_coordinator import DocumentFillCoordinator
from src.core.tools.writing_style_analyzer import WritingStyleAnalyzer

# Load environment variables
load_dotenv()

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)

# åˆ›å»ºFlaskåº”ç”¨ï¼ŒæŒ‡å®šæ¨¡æ¿å’Œé™æ€æ–‡ä»¶è·¯å¾„
app = Flask(__name__, 
           template_folder=os.path.join(project_root, 'templates'),
           static_folder=os.path.join(project_root, 'static'))
CORS(app)

# Configuration
app.config['UPLOAD_FOLDER'] = os.path.join(project_root, 'uploads')
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size
app.config['ALLOWED_EXTENSIONS'] = {'txt', 'pdf', 'docx'}

# Ensure upload directory exists
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(os.path.join(project_root, 'output'), exist_ok=True)

# å…¨å±€å˜é‡å­˜å‚¨åè°ƒå™¨å®ä¾‹
orchestrator_instance = None
format_coordinator = None
fill_coordinator = None
style_analyzer = None

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def initialize_agent(api_type="xingcheng", model_name=None):
    """Initialize the document agent with specified LLM client."""
    KB_PATH = os.path.join(project_root, "src/core/knowledge_base")
    
    if api_type == "multi":
        # ä½¿ç”¨å¤šAPIå®¢æˆ·ç«¯
        try:
            llm_client = MultiLLMClient()
            print("MultiLLMClient initialized successfully")
            return AgentOrchestrator(llm_client=llm_client, kb_path=KB_PATH)
        except Exception as e:
            print(f"Error initializing MultiLLMClient: {e}")
            print("Falling back to mock mode")
            return None
    elif api_type == "xingcheng":
        # ä½¿ç”¨è®¯é£æ˜Ÿç«API
        XINGCHENG_API_KEY = os.getenv("XINGCHENG_API_KEY")
        XINGCHENG_API_SECRET = os.getenv("XINGCHENG_API_SECRET")
        LLM_MODEL_NAME = model_name or os.getenv("LLM_MODEL_NAME", "x1")
        
        if not XINGCHENG_API_KEY:
            print("Warning: Xingcheng API key not configured, using mock mode")
            return None
        
        try:
            llm_client = XingchengLLMClient(
                api_key=XINGCHENG_API_KEY,
                api_secret=XINGCHENG_API_SECRET,
                model_name=LLM_MODEL_NAME
            )
            return AgentOrchestrator(llm_client=llm_client, kb_path=KB_PATH)
        except Exception as e:
            print(f"Error initializing XingchengLLMClient: {e}")
            print("Falling back to mock mode")
            return None
    else:
        print(f"Unknown API type: {api_type}, using mock mode")
        return None

def analyze_document_scenario(content):
    """æ™ºèƒ½åˆ†ææ–‡æ¡£åœºæ™¯å’Œç±»å‹"""
    if not content:
        return 'é€šç”¨æ–‡æ¡£', 'åŠå…¬æ–‡æ¡£', ['æ–‡æ¡£å†…å®¹åˆ†æ', 'ç»“æ„ä¼˜åŒ–å»ºè®®']

    content_lower = content.lower()

    # ä¼šè®®æ–‡æ¡£æ£€æµ‹
    meeting_keywords = ['ä¼šè®®', 'meeting', 'è®®ç¨‹', 'å†³è®®', 'è®¨è®º', 'å‚ä¼š', 'ä¼šè®®çºªè¦', 'å†³ç­–']
    if any(keyword in content_lower for keyword in meeting_keywords):
        return 'ä¼šè®®æ–‡æ¡£', 'è¿™æ˜¯ä¸€ä»½ä¼šè®®ç›¸å…³æ–‡æ¡£ï¼Œè®°å½•äº†ä¼šè®®è®®ç¨‹ã€è®¨è®ºå†…å®¹å’Œå†³ç­–ç»“æœ', [
            'ä¼šè®®ä¸»é¢˜å’Œç›®æ ‡', 'å‚ä¼šäººå‘˜ä¿¡æ¯', 'è®¨è®ºçš„å…³é”®è®®é¢˜', 'è¾¾æˆçš„å†³ç­–å’Œå…±è¯†', 'åç»­è¡ŒåŠ¨è®¡åˆ’'
        ]

    # æŠ€æœ¯æ–‡æ¡£æ£€æµ‹
    tech_keywords = ['æŠ€æœ¯', 'technical', 'ç³»ç»Ÿ', 'æ¶æ„', 'å¼€å‘', 'ä»£ç ', 'api', 'æ•°æ®åº“', 'ç®—æ³•']
    if any(keyword in content_lower for keyword in tech_keywords):
        return 'æŠ€æœ¯æ–‡æ¡£', 'è¿™æ˜¯ä¸€ä»½æŠ€æœ¯æ–‡æ¡£ï¼Œæ¶‰åŠç³»ç»Ÿè®¾è®¡ã€å¼€å‘è§„èŒƒæˆ–æŠ€æœ¯å®ç°', [
            'æŠ€æœ¯æ¶æ„è®¾è®¡', 'æ ¸å¿ƒåŠŸèƒ½æ¨¡å—', 'æ€§èƒ½å’Œå®‰å…¨è¦æ±‚', 'å¼€å‘å’Œéƒ¨ç½²æµç¨‹', 'ç»´æŠ¤å’Œä¼˜åŒ–å»ºè®®'
        ]

    # äº§å“æ–‡æ¡£æ£€æµ‹
    product_keywords = ['äº§å“', 'product', 'åŠŸèƒ½', 'éœ€æ±‚', 'ç”¨æˆ·', 'å¸‚åœº', 'ç«å“', 'è§„åˆ’']
    if any(keyword in content_lower for keyword in product_keywords):
        return 'äº§å“æ–‡æ¡£', 'è¿™æ˜¯ä¸€ä»½äº§å“ç›¸å…³æ–‡æ¡£ï¼Œæè¿°äº†äº§å“åŠŸèƒ½ã€éœ€æ±‚æˆ–å¸‚åœºç­–ç•¥', [
            'äº§å“æ ¸å¿ƒåŠŸèƒ½', 'ç›®æ ‡ç”¨æˆ·ç¾¤ä½“', 'å¸‚åœºå®šä½åˆ†æ', 'åŠŸèƒ½ä¼˜å…ˆçº§', 'äº§å“å‘å±•è·¯çº¿å›¾'
        ]

    # åˆ†ææŠ¥å‘Šæ£€æµ‹
    report_keywords = ['æŠ¥å‘Š', 'report', 'åˆ†æ', 'æ€»ç»“', 'æ•°æ®', 'ç»Ÿè®¡', 'è¶‹åŠ¿', 'ç»“è®º']
    if any(keyword in content_lower for keyword in report_keywords):
        return 'åˆ†ææŠ¥å‘Š', 'è¿™æ˜¯ä¸€ä»½åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«æ•°æ®åˆ†æã€è¶‹åŠ¿æ€»ç»“å’Œç»“è®ºå»ºè®®', [
            'å…³é”®æ•°æ®æŒ‡æ ‡', 'è¶‹åŠ¿åˆ†æç»“æœ', 'é—®é¢˜è¯†åˆ«å’ŒåŸå› ', 'æ”¹è¿›å»ºè®®æ–¹æ¡ˆ', 'é¢„æœŸæ•ˆæœè¯„ä¼°'
        ]

    # å•†ä¸šæ–‡æ¡£æ£€æµ‹
    business_keywords = ['å•†ä¸š', 'business', 'åˆåŒ', 'åè®®', 'æ–¹æ¡ˆ', 'ææ¡ˆ', 'é¢„ç®—', 'æˆæœ¬']
    if any(keyword in content_lower for keyword in business_keywords):
        return 'å•†ä¸šæ–‡æ¡£', 'è¿™æ˜¯ä¸€ä»½å•†ä¸šæ–‡æ¡£ï¼Œæ¶‰åŠå•†ä¸šè®¡åˆ’ã€åˆåŒåè®®æˆ–è´¢åŠ¡åˆ†æ', [
            'å•†ä¸šç›®æ ‡å’Œç­–ç•¥', 'è´¢åŠ¡é¢„ç®—åˆ†æ', 'é£é™©è¯„ä¼°', 'åˆä½œä¼™ä¼´å…³ç³»', 'æ‰§è¡Œæ—¶é—´è¡¨'
        ]

    return 'é€šç”¨æ–‡æ¡£', 'è¿™æ˜¯ä¸€ä»½é€šç”¨åŠå…¬æ–‡æ¡£ï¼ŒåŒ…å«å¤šç§ç±»å‹çš„ä¿¡æ¯å’Œå†…å®¹', [
        'æ–‡æ¡£ä¸»è¦å†…å®¹', 'å…³é”®ä¿¡æ¯è¦ç‚¹', 'é‡è¦è§‚ç‚¹æ€»ç»“', 'è¡ŒåŠ¨å»ºè®®äº‹é¡¹'
    ]

def generate_content_summary(content, max_length=200):
    """æ™ºèƒ½ç”Ÿæˆå†…å®¹æ‘˜è¦"""
    if not content:
        return "æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"

    # åˆ†å¥å¤„ç†
    sentences = content.replace('ã€‚', 'ã€‚\n').replace('ï¼', 'ï¼\n').replace('ï¼Ÿ', 'ï¼Ÿ\n').split('\n')
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]

    if not sentences:
        return content[:max_length] + ("..." if len(content) > max_length else "")

    # è¯†åˆ«å…³é”®å¥å­ï¼ˆåŒ…å«é‡è¦å…³é”®è¯çš„å¥å­ï¼‰
    important_keywords = [
        'å†³ç­–', 'å†³å®š', 'ç»“è®º', 'å»ºè®®', 'è®¡åˆ’', 'ç›®æ ‡', 'é‡è¦', 'å…³é”®',
        'æ ¸å¿ƒ', 'ä¸»è¦', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'æ€»ç»“', 'æ¦‚è¿°',
        'é—®é¢˜', 'è§£å†³', 'æ–¹æ¡ˆ', 'ç­–ç•¥', 'æªæ–½', 'è¡ŒåŠ¨', 'æ‰§è¡Œ'
    ]

    # ç»™å¥å­æ‰“åˆ†
    sentence_scores = []
    for i, sentence in enumerate(sentences):
        score = 0
        # ä½ç½®æƒé‡ï¼šå¼€å¤´å’Œç»“å°¾çš„å¥å­æ›´é‡è¦
        if i == 0:
            score += 3
        elif i == len(sentences) - 1:
            score += 2
        elif i < len(sentences) * 0.3:  # å‰30%
            score += 1

        # å…³é”®è¯æƒé‡
        for keyword in important_keywords:
            if keyword in sentence:
                score += 2

        # é•¿åº¦æƒé‡ï¼šé€‚ä¸­é•¿åº¦çš„å¥å­æ›´å¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯
        if 20 <= len(sentence) <= 100:
            score += 1

        # æ•°å­—æƒé‡ï¼šåŒ…å«æ•°å­—çš„å¥å­é€šå¸¸æ›´é‡è¦
        import re
        if re.search(r'\d+', sentence):
            score += 1

        sentence_scores.append((sentence, score))

    # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€é‡è¦çš„å¥å­
    sentence_scores.sort(key=lambda x: x[1], reverse=True)

    # æ„å»ºæ‘˜è¦
    summary_sentences = []
    current_length = 0

    for sentence, score in sentence_scores:
        if current_length + len(sentence) <= max_length - 20:  # ç•™ä¸€äº›ä½™é‡
            summary_sentences.append(sentence)
            current_length += len(sentence)
        else:
            break

    # å¦‚æœæ²¡æœ‰é€‰ä¸­ä»»ä½•å¥å­ï¼Œä½¿ç”¨å‰å‡ å¥
    if not summary_sentences:
        summary_sentences = sentences[:2]

    # æŒ‰åŸæ–‡é¡ºåºé‡æ–°æ’åˆ—
    final_sentences = []
    for sentence in sentences:
        if sentence in summary_sentences:
            final_sentences.append(sentence)

    summary = 'ã€‚'.join(final_sentences)
    if not summary.endswith('ã€‚'):
        summary += 'ã€‚'

    # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œæˆªæ–­
    if len(summary) > max_length:
        summary = summary[:max_length-3] + "..."

    return summary

def extract_key_entities(content):
    """æ™ºèƒ½æå–å…³é”®å®ä½“å’Œæ¦‚å¿µ"""
    if not content:
        return []

    import re
    entities = []

    # 1. æ—¶é—´å®ä½“æå–
    time_patterns = [
        (r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥', 'å…·ä½“æ—¥æœŸ'),
        (r'\d{1,2}æœˆ\d{1,2}æ—¥', 'æœˆæ—¥'),
        (r'\d{4}-\d{1,2}-\d{1,2}', 'æ—¥æœŸ'),
        (r'\d{1,2}:\d{2}', 'æ—¶é—´'),
        (r'ä¸‹å‘¨|æœ¬å‘¨|ä¸Šå‘¨|ä¸‹æœˆ|æœ¬æœˆ|ä¸Šæœˆ', 'ç›¸å¯¹æ—¶é—´')
    ]

    for pattern, type_name in time_patterns:
        matches = re.findall(pattern, content)
        for match in matches[:2]:  # é™åˆ¶æ¯ç§ç±»å‹æœ€å¤š2ä¸ª
            entities.append({'type': type_name, 'value': match})

    # 2. æ•°å€¼å®ä½“æå–
    number_patterns = [
        (r'\d+%', 'ç™¾åˆ†æ¯”'),
        (r'\d+ä¸‡å…ƒ?', 'é‡‘é¢(ä¸‡)'),
        (r'\d+äº¿å…ƒ?', 'é‡‘é¢(äº¿)'),
        (r'ï¿¥\d+', 'äººæ°‘å¸'),
        (r'\$\d+', 'ç¾å…ƒ'),
        (r'\d+äºº', 'äººæ•°'),
        (r'\d+ä¸ª', 'æ•°é‡'),
        (r'\d+é¡¹', 'é¡¹ç›®æ•°')
    ]

    for pattern, type_name in number_patterns:
        matches = re.findall(pattern, content)
        for match in matches[:2]:
            entities.append({'type': type_name, 'value': match})

    # 3. äººåæå–ï¼ˆç®€å•çš„ä¸­æ–‡äººåæ¨¡å¼ï¼‰
    name_patterns = [
        r'[å¼ ç‹æèµµåˆ˜é™ˆæ¨é»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°ä»»å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé›·é’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡][ä¸€-é¾¯]{1,2}',
    ]

    for pattern in name_patterns:
        matches = re.findall(pattern, content)
        # è¿‡æ»¤æ‰ä¸€äº›å¸¸è§çš„éäººåè¯æ±‡
        filtered_matches = [m for m in matches if len(m) >= 2 and m not in ['ä¼šè®®', 'é¡¹ç›®', 'å…¬å¸', 'éƒ¨é—¨', 'äº§å“', 'æŠ€æœ¯', 'å¸‚åœº', 'æ–¹æ¡ˆ']]
        for match in filtered_matches[:3]:
            entities.append({'type': 'äººå‘˜', 'value': match})

    # 4. ç»„ç»‡æœºæ„æå–
    org_patterns = [
        r'[ä¸€-é¾¯]*éƒ¨é—¨?',
        r'[ä¸€-é¾¯]*å…¬å¸',
        r'[ä¸€-é¾¯]*å›¢é˜Ÿ',
        r'[ä¸€-é¾¯]*å°ç»„',
        r'[ä¸€-é¾¯]*ä¸­å¿ƒ'
    ]

    for pattern in org_patterns:
        matches = re.findall(pattern, content)
        filtered_matches = [m for m in matches if len(m) >= 3 and 'éƒ¨é—¨' in m or 'å…¬å¸' in m or 'å›¢é˜Ÿ' in m or 'å°ç»„' in m or 'ä¸­å¿ƒ' in m]
        for match in filtered_matches[:2]:
            entities.append({'type': 'ç»„ç»‡', 'value': match})

    # 5. å…³é”®æ¦‚å¿µæå–
    concept_keywords = [
        'äº§å“', 'é¡¹ç›®', 'æ–¹æ¡ˆ', 'è®¡åˆ’', 'ç­–ç•¥', 'ç›®æ ‡', 'ä»»åŠ¡', 'åŠŸèƒ½',
        'ç³»ç»Ÿ', 'å¹³å°', 'æœåŠ¡', 'æŠ€æœ¯', 'æ¶æ„', 'è®¾è®¡', 'å¼€å‘', 'æµ‹è¯•',
        'å¸‚åœº', 'ç”¨æˆ·', 'å®¢æˆ·', 'éœ€æ±‚', 'åé¦ˆ', 'ä½“éªŒ', 'æ»¡æ„åº¦',
        'é¢„ç®—', 'æˆæœ¬', 'æ”¶å…¥', 'åˆ©æ¶¦', 'æŠ•èµ„', 'å›æŠ¥', 'é£é™©'
    ]

    found_concepts = []
    for keyword in concept_keywords:
        if keyword in content and keyword not in found_concepts:
            found_concepts.append(keyword)
            entities.append({'type': 'å…³é”®æ¦‚å¿µ', 'value': keyword})
            if len(found_concepts) >= 5:  # æœ€å¤š5ä¸ªæ¦‚å¿µ
                break

    # 6. å¦‚æœå®ä½“å¤ªå°‘ï¼Œæ·»åŠ ä¸€äº›ä»å†…å®¹ä¸­æå–çš„é‡è¦è¯æ±‡
    if len(entities) < 3:
        # æå–å‡ºç°é¢‘ç‡è¾ƒé«˜çš„è¯æ±‡
        words = re.findall(r'[ä¸€-é¾¯]{2,4}', content)
        word_count = {}
        for word in words:
            if len(word) >= 2 and word not in ['è¿™ä¸ª', 'é‚£ä¸ª', 'å¯ä»¥', 'éœ€è¦', 'è¿›è¡Œ', 'å®ç°', 'å®Œæˆ', 'å»ºè®®']:
                word_count[word] = word_count.get(word, 0) + 1

        # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„è¯æ±‡
        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
        for word, count in sorted_words[:3]:
            if count >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                entities.append({'type': 'é‡è¦è¯æ±‡', 'value': f"{word}(å‡ºç°{count}æ¬¡)"})

    # å»é‡å¹¶é™åˆ¶æ•°é‡
    seen = set()
    unique_entities = []
    for entity in entities:
        key = f"{entity['type']}:{entity['value']}"
        if key not in seen:
            seen.add(key)
            unique_entities.append(entity)

    return unique_entities[:10]  # æœ€å¤šè¿”å›10ä¸ªå®ä½“

def analyze_content_depth(content, key_points):
    """æ·±åº¦åˆ†ææ–‡æ¡£å†…å®¹ç‰¹å¾"""
    if not content:
        return {
            'main_topics': [],
            'sentiment': 'æ— å†…å®¹',
            'complexity': 'æ— å†…å®¹',
            'language': 'æœªçŸ¥',
            'formality': 'æœªçŸ¥',
            'urgency': 'æ— ',
            'completeness': 'ä¸å®Œæ•´'
        }

    import re

    # 1. ä¸»è¦è¯é¢˜åˆ†æ
    main_topics = key_points[:3] if key_points else ['æ–‡æ¡£å†…å®¹']

    # 2. æƒ…æ„Ÿå€¾å‘åˆ†æ
    positive_words = ['æˆåŠŸ', 'ä¼˜ç§€', 'è‰¯å¥½', 'æ»¡æ„', 'èµåŒ', 'æ”¯æŒ', 'åŒæ„', 'æ‰¹å‡†', 'é€šè¿‡', 'å®Œæˆ', 'è¾¾æˆ', 'å®ç°', 'æå‡', 'æ”¹å–„', 'ä¼˜åŒ–']
    negative_words = ['å¤±è´¥', 'é—®é¢˜', 'å›°éš¾', 'æŒ‘æˆ˜', 'é£é™©', 'å»¶è¿Ÿ', 'æ¨è¿Ÿ', 'å–æ¶ˆ', 'æ‹’ç»', 'åå¯¹', 'ä¸æ»¡', 'æŠ•è¯‰', 'é”™è¯¯', 'ç¼ºé™·', 'ä¸è¶³']
    neutral_words = ['è®¨è®º', 'åˆ†æ', 'è€ƒè™‘', 'å»ºè®®', 'è®¡åˆ’', 'å®‰æ’', 'å‡†å¤‡', 'è¿›è¡Œ', 'å®æ–½', 'æ‰§è¡Œ']

    positive_count = sum(1 for word in positive_words if word in content)
    negative_count = sum(1 for word in negative_words if word in content)
    neutral_count = sum(1 for word in neutral_words if word in content)

    if positive_count > negative_count and positive_count > 0:
        sentiment = 'ç§¯æ'
    elif negative_count > positive_count and negative_count > 0:
        sentiment = 'æ¶ˆæ'
    elif neutral_count > 0:
        sentiment = 'ä¸­æ€§'
    else:
        sentiment = 'å®¢è§‚'

    # 3. å¤æ‚åº¦åˆ†æ
    word_count = len(content.split())
    sentence_count = len([s for s in re.split(r'[ã€‚ï¼ï¼Ÿ]', content) if s.strip()])
    avg_sentence_length = word_count / max(sentence_count, 1)

    # æ£€æŸ¥ä¸“ä¸šæœ¯è¯­
    technical_terms = ['ç³»ç»Ÿ', 'æ¶æ„', 'æŠ€æœ¯', 'ç®—æ³•', 'æ•°æ®åº“', 'æ¥å£', 'API', 'æ¡†æ¶', 'æ¨¡å—', 'ç»„ä»¶']
    business_terms = ['æˆ˜ç•¥', 'ç­–ç•¥', 'å¸‚åœº', 'å®¢æˆ·', 'ç”¨æˆ·', 'äº§å“', 'æœåŠ¡', 'æ”¶ç›Š', 'æˆæœ¬', 'é¢„ç®—']
    complex_terms = technical_terms + business_terms

    term_count = sum(1 for term in complex_terms if term in content)

    if word_count > 500 or avg_sentence_length > 20 or term_count > 5:
        complexity = 'å¤æ‚'
    elif word_count > 200 or avg_sentence_length > 15 or term_count > 2:
        complexity = 'ä¸­ç­‰'
    elif word_count > 50:
        complexity = 'ç®€å•'
    else:
        complexity = 'æç®€'

    # 4. è¯­è¨€ç‰¹å¾åˆ†æ
    chinese_chars = len(re.findall(r'[ä¸€-é¾¯]', content))
    total_chars = len(content)
    chinese_ratio = chinese_chars / max(total_chars, 1)

    if chinese_ratio > 0.7:
        language = 'ä¸­æ–‡ä¸ºä¸»'
    elif chinese_ratio > 0.3:
        language = 'ä¸­è‹±æ··åˆ'
    else:
        language = 'è‹±æ–‡ä¸ºä¸»'

    # 5. æ­£å¼ç¨‹åº¦åˆ†æ
    formal_words = ['æ‚¨', 'æ•¬è¯·', 'è°¨æ­¤', 'ç‰¹æ­¤', 'å…¹', 'æ®æ­¤', 'ç»¼ä¸Š', 'é‰´äº', 'åŸºäº']
    informal_words = ['ä½ ', 'å’±ä»¬', 'å¤§å®¶', 'å“ˆå“ˆ', 'å—¯', 'å‘ƒ', 'å“¦']

    formal_count = sum(1 for word in formal_words if word in content)
    informal_count = sum(1 for word in informal_words if word in content)

    if formal_count > informal_count and formal_count > 0:
        formality = 'æ­£å¼'
    elif informal_count > 0:
        formality = 'éæ­£å¼'
    else:
        formality = 'ä¸­æ€§'

    # 6. ç´§æ€¥ç¨‹åº¦åˆ†æ
    urgent_words = ['ç´§æ€¥', 'ç«‹å³', 'é©¬ä¸Š', 'å°½å¿«', 'æ€¥éœ€', 'ç«æ€¥', 'åŠ æ€¥', 'ä¼˜å…ˆ', 'é‡è¦', 'å…³é”®']
    urgent_count = sum(1 for word in urgent_words if word in content)

    if urgent_count >= 3:
        urgency = 'é«˜'
    elif urgent_count >= 1:
        urgency = 'ä¸­'
    else:
        urgency = 'ä½'

    # 7. å®Œæ•´æ€§åˆ†æ
    structure_indicators = ['ç›®æ ‡', 'èƒŒæ™¯', 'æ–¹æ¡ˆ', 'ç»“è®º', 'å»ºè®®', 'è®¡åˆ’', 'æ—¶é—´', 'è´£ä»»äºº']
    structure_count = sum(1 for indicator in structure_indicators if indicator in content)

    if structure_count >= 5:
        completeness = 'å®Œæ•´'
    elif structure_count >= 3:
        completeness = 'è¾ƒå®Œæ•´'
    elif structure_count >= 1:
        completeness = 'åŸºæœ¬å®Œæ•´'
    else:
        completeness = 'ä¸å®Œæ•´'

    return {
        'main_topics': main_topics,
        'sentiment': sentiment,
        'complexity': complexity,
        'language': language,
        'formality': formality,
        'urgency': urgency,
        'completeness': completeness,
        'word_count': word_count,
        'sentence_count': sentence_count,
        'avg_sentence_length': round(avg_sentence_length, 1)
    }

def analyze_content_gaps(content, doc_type):
    """åˆ†ææ–‡æ¡£å†…å®¹ç¼ºå¤±å’Œæ”¹è¿›ç‚¹"""
    if not content:
        return ["æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œéœ€è¦æ·»åŠ å®è´¨æ€§å†…å®¹"]

    content_lower = content.lower()
    gaps = []

    # é€šç”¨å†…å®¹åˆ†æ
    if len(content) < 100:
        gaps.append("æ–‡æ¡£å†…å®¹è¿‡äºç®€çŸ­ï¼Œå»ºè®®è¡¥å……æ›´å¤šè¯¦ç»†ä¿¡æ¯")

    if 'ã€‚' not in content and '!' not in content and '?' not in content:
        gaps.append("å»ºè®®ä½¿ç”¨æ ‡ç‚¹ç¬¦å·æé«˜æ–‡æ¡£å¯è¯»æ€§")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜ç»“æ„
    if not any(marker in content for marker in ['#', 'ä¸€ã€', '1.', 'ï¼ˆä¸€ï¼‰', 'ç¬¬ä¸€']):
        gaps.append("å»ºè®®æ·»åŠ æ¸…æ™°çš„æ ‡é¢˜å’Œç« èŠ‚ç»“æ„")

    # æ ¹æ®æ–‡æ¡£ç±»å‹è¿›è¡Œå…·ä½“åˆ†æ
    if doc_type == 'ä¼šè®®æ–‡æ¡£':
        if 'æ—¶é—´' not in content_lower and 'æ—¥æœŸ' not in content_lower:
            gaps.append("ç¼ºå°‘ä¼šè®®æ—¶é—´ä¿¡æ¯ï¼Œå»ºè®®è¡¥å……å…·ä½“çš„ä¼šè®®æ—¥æœŸå’Œæ—¶é—´")
        if 'å‚ä¼š' not in content_lower and 'å‚ä¸' not in content_lower and 'å‡ºå¸­' not in content_lower:
            gaps.append("ç¼ºå°‘å‚ä¼šäººå‘˜ä¿¡æ¯ï¼Œå»ºè®®æ˜ç¡®è®°å½•å‚ä¼šäººå‘˜åå•")
        if 'å†³ç­–' not in content_lower and 'å†³å®š' not in content_lower and 'ç»“è®º' not in content_lower:
            gaps.append("ç¼ºå°‘æ˜ç¡®çš„å†³ç­–ç»“æœï¼Œå»ºè®®è¡¥å……ä¼šè®®è¾¾æˆçš„å…·ä½“å†³ç­–")
        if 'è¡ŒåŠ¨' not in content_lower and 'æ‰§è¡Œ' not in content_lower and 'è´Ÿè´£' not in content_lower:
            gaps.append("ç¼ºå°‘åç»­è¡ŒåŠ¨è®¡åˆ’ï¼Œå»ºè®®æ˜ç¡®è´£ä»»äººå’Œæ‰§è¡Œæ—¶é—´èŠ‚ç‚¹")

    elif doc_type == 'æŠ€æœ¯æ–‡æ¡£':
        if 'æ¶æ„' not in content_lower and 'è®¾è®¡' not in content_lower:
            gaps.append("ç¼ºå°‘æŠ€æœ¯æ¶æ„è¯´æ˜ï¼Œå»ºè®®è¡¥å……ç³»ç»Ÿè®¾è®¡å’Œæ¶æ„å›¾")
        if 'api' not in content_lower and 'æ¥å£' not in content_lower:
            gaps.append("ç¼ºå°‘æ¥å£æ–‡æ¡£ï¼Œå»ºè®®è¡¥å……APIæ¥å£è¯´æ˜å’Œç¤ºä¾‹")
        if 'æµ‹è¯•' not in content_lower and 'éªŒè¯' not in content_lower:
            gaps.append("ç¼ºå°‘æµ‹è¯•è¯´æ˜ï¼Œå»ºè®®è¡¥å……æµ‹è¯•æ–¹æ³•å’ŒéªŒè¯æ­¥éª¤")

    elif doc_type == 'äº§å“æ–‡æ¡£':
        if 'ç”¨æˆ·' not in content_lower and 'å®¢æˆ·' not in content_lower:
            gaps.append("ç¼ºå°‘ç”¨æˆ·åˆ†æï¼Œå»ºè®®è¡¥å……ç›®æ ‡ç”¨æˆ·ç¾¤ä½“å’Œéœ€æ±‚åˆ†æ")
        if 'åŠŸèƒ½' not in content_lower and 'ç‰¹æ€§' not in content_lower:
            gaps.append("ç¼ºå°‘åŠŸèƒ½è¯´æ˜ï¼Œå»ºè®®è¯¦ç»†æè¿°äº§å“æ ¸å¿ƒåŠŸèƒ½")
        if 'ç«å“' not in content_lower and 'å¸‚åœº' not in content_lower:
            gaps.append("ç¼ºå°‘å¸‚åœºåˆ†æï¼Œå»ºè®®è¡¥å……ç«å“åˆ†æå’Œå¸‚åœºå®šä½")

    elif doc_type == 'åˆ†ææŠ¥å‘Š':
        if 'æ•°æ®' not in content_lower and 'ç»Ÿè®¡' not in content_lower:
            gaps.append("ç¼ºå°‘æ•°æ®æ”¯æ’‘ï¼Œå»ºè®®è¡¥å……å…·ä½“çš„æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯")
        if 'å›¾è¡¨' not in content_lower and 'å›¾' not in content_lower:
            gaps.append("ç¼ºå°‘å¯è§†åŒ–å±•ç¤ºï¼Œå»ºè®®æ·»åŠ å›¾è¡¨å’Œæ•°æ®å¯è§†åŒ–")
        if 'ç»“è®º' not in content_lower and 'å»ºè®®' not in content_lower:
            gaps.append("ç¼ºå°‘æ˜ç¡®ç»“è®ºï¼Œå»ºè®®è¡¥å……åˆ†æç»“è®ºå’Œæ”¹è¿›å»ºè®®")

    return gaps if gaps else ["æ–‡æ¡£ç»“æ„å®Œæ•´ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–å†…å®¹è¡¨è¾¾å’Œæ ¼å¼è§„èŒƒ"]

def generate_improvement_suggestions(content, doc_type):
    """æ ¹æ®æ–‡æ¡£å®é™…å†…å®¹ç”Ÿæˆæ™ºèƒ½æ”¹è¿›å»ºè®®"""
    if not content:
        return ["æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œè¯·æ·»åŠ å®è´¨æ€§å†…å®¹"]

    # åˆ†æå†…å®¹ç¼ºå¤±
    content_gaps = analyze_content_gaps(content, doc_type)

    # åˆ†æå†…å®¹è´¨é‡
    quality_suggestions = []

    # æ£€æŸ¥å†…å®¹é•¿åº¦å’Œç»“æ„
    sentences = content.replace('ã€‚', 'ã€‚\n').replace('ï¼', 'ï¼\n').replace('ï¼Ÿ', 'ï¼Ÿ\n').split('\n')
    sentences = [s.strip() for s in sentences if s.strip()]

    if len(sentences) < 3:
        quality_suggestions.append("å»ºè®®æ‰©å±•å†…å®¹ï¼Œå¢åŠ æ›´å¤šè¯¦ç»†è¯´æ˜å’ŒèƒŒæ™¯ä¿¡æ¯")

    # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æ•°å­—ã€æ—¶é—´ç­‰å…³é”®ä¿¡æ¯
    import re
    has_numbers = bool(re.search(r'\d+', content))
    has_dates = bool(re.search(r'\d{4}å¹´|\d{1,2}æœˆ|\d{1,2}æ—¥', content))

    if not has_numbers and doc_type in ['åˆ†ææŠ¥å‘Š', 'æŠ€æœ¯æ–‡æ¡£']:
        quality_suggestions.append("å»ºè®®è¡¥å……å…·ä½“çš„æ•°å­—å’Œé‡åŒ–æŒ‡æ ‡")

    if not has_dates and doc_type in ['ä¼šè®®æ–‡æ¡£', 'åˆ†ææŠ¥å‘Š']:
        quality_suggestions.append("å»ºè®®æ·»åŠ æ˜ç¡®çš„æ—¶é—´ä¿¡æ¯å’Œæ—¶é—´èŠ‚ç‚¹")

    # æ£€æŸ¥æ˜¯å¦æœ‰è¡ŒåŠ¨å¯¼å‘çš„å†…å®¹
    action_words = ['è®¡åˆ’', 'æ‰§è¡Œ', 'å®æ–½', 'å®Œæˆ', 'è´Ÿè´£', 'å®‰æ’', 'æ¨è¿›']
    has_actions = any(word in content for word in action_words)

    if not has_actions and doc_type in ['ä¼šè®®æ–‡æ¡£', 'äº§å“æ–‡æ¡£']:
        quality_suggestions.append("å»ºè®®æ·»åŠ å…·ä½“çš„è¡ŒåŠ¨è®¡åˆ’å’Œæ‰§è¡Œæ­¥éª¤")

    # åˆå¹¶å»ºè®®å¹¶å»é‡
    all_suggestions = content_gaps + quality_suggestions

    # é™åˆ¶å»ºè®®æ•°é‡ï¼Œé€‰æ‹©æœ€é‡è¦çš„
    return all_suggestions[:6] if all_suggestions else ["æ–‡æ¡£è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒ"]

def mock_process_document(file_path):
    """Mock document processing for testing without API."""
    print(f"ğŸ­ MOCK PROCESSING STARTED")
    print(f"ğŸ­ File path: {file_path}")

    try:
        content = ""
        file_exists = os.path.exists(file_path)
        print(f"ğŸ­ File exists: {file_exists}")

        if file_exists:
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åˆé€‚çš„è¯»å–æ–¹æ³•
            try:
                file_extension = os.path.splitext(file_path)[1].lower()
                file_size = os.path.getsize(file_path)
                print(f"ğŸ­ File extension: {file_extension}")
                print(f"ğŸ­ File size: {file_size} bytes")

                if file_extension == '.docx':
                    print(f"ğŸ­ Processing Word document...")
                    # ä½¿ç”¨python-docxè¯»å–Wordæ–‡æ¡£
                    try:
                        print(f"ğŸ­ Importing python-docx...")
                        from docx import Document
                        print(f"ğŸ­ Creating Document object...")
                        doc = Document(file_path)
                        print(f"ğŸ­ Document loaded, extracting paragraphs...")
                        content_parts = []
                        paragraph_count = 0
                        for paragraph in doc.paragraphs:
                            paragraph_count += 1
                            if paragraph.text.strip():
                                content_parts.append(paragraph.text.strip())
                        print(f"ğŸ­ Found {paragraph_count} paragraphs, {len(content_parts)} with content")
                        content = '\n'.join(content_parts)
                        if not content:
                            content = "Wordæ–‡æ¡£è§£ææˆåŠŸï¼Œä½†å†…å®¹ä¸ºç©ºã€‚"
                        print(f"ğŸ­ Word document processed successfully: {len(content)} characters")
                    except Exception as docx_error:
                        print(f"âŒ Error reading DOCX file: {docx_error}")
                        print(f"âŒ Error type: {type(docx_error).__name__}")
                        import traceback
                        print(f"âŒ Traceback: {traceback.format_exc()}")
                        content = "Wordæ–‡æ¡£æ ¼å¼ï¼Œä½†è§£æå¤±è´¥ã€‚ä½¿ç”¨æ¨¡æ‹Ÿå†…å®¹è¿›è¡Œæ¼”ç¤ºã€‚"

                elif file_extension == '.pdf':
                    # ä½¿ç”¨PyPDF2è¯»å–PDFæ–‡æ¡£
                    try:
                        import PyPDF2
                        content_parts = []
                        with open(file_path, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page in reader.pages:
                                page_text = page.extract_text()
                                if page_text.strip():
                                    content_parts.append(page_text.strip())
                        content = '\n'.join(content_parts)
                        if not content:
                            content = "PDFæ–‡æ¡£è§£ææˆåŠŸï¼Œä½†å†…å®¹ä¸ºç©ºã€‚"
                    except Exception as pdf_error:
                        print(f"Error reading PDF file: {pdf_error}")
                        content = "PDFæ–‡æ¡£æ ¼å¼ï¼Œä½†è§£æå¤±è´¥ã€‚ä½¿ç”¨æ¨¡æ‹Ÿå†…å®¹è¿›è¡Œæ¼”ç¤ºã€‚"

                else:
                    # æ–‡æœ¬æ–‡ä»¶ï¼Œå°è¯•ä¸åŒçš„ç¼–ç 
                    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
                    for encoding in encodings:
                        try:
                            with open(file_path, 'r', encoding=encoding) as f:
                                content = f.read()
                            break
                        except UnicodeDecodeError:
                            continue

                    if not content:
                        content = "æ–‡æœ¬æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå†…å®¹è¿›è¡Œæ¼”ç¤ºã€‚"

            except Exception as read_error:
                print(f"Error reading file: {read_error}")
                content = f"æ–‡ä»¶è¯»å–å¤±è´¥ ({file_extension})ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå†…å®¹è¿›è¡Œæ¼”ç¤ºã€‚"
        else:
            content = "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå†…å®¹è¿›è¡Œæ¼”ç¤ºã€‚"

        # åˆ†ææ–‡æ¡£å†…å®¹
        lines = content.count('\n') + 1 if content else 1
        paragraphs = max(content.count('\n\n') + 1, 1) if content else 1
        characters = len(content) if content else 0

        # ä½¿ç”¨AIåˆ†æåŠŸèƒ½è¿›è¡Œæ™ºèƒ½æ–‡æ¡£åˆ†æ
        print(f"ğŸ­ Starting intelligent document analysis...")
        doc_type, scenario, key_points = analyze_document_scenario(content)
        print(f"ğŸ­ Document type identified: {doc_type}")

        # ç”Ÿæˆå†…å®¹æ‘˜è¦
        content_summary = generate_content_summary(content)
        print(f"ğŸ­ Content summary generated: {len(content_summary)} characters")

        # æå–å…³é”®å®ä½“
        entities = extract_key_entities(content)
        print(f"ğŸ­ Extracted {len(entities)} key entities")

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_suggestions = generate_improvement_suggestions(content, doc_type)
        print(f"ğŸ­ Generated {len(improvement_suggestions)} improvement suggestions")

        # ç”Ÿæˆæ™ºèƒ½å»ºè®®
        suggestions = [
            'å»ºè®®å¢åŠ æ›´å¤šå…·ä½“æ•°æ®æ”¯æ’‘',
            'å¯ä»¥æ·»åŠ å›¾è¡¨å’Œå¯è§†åŒ–å†…å®¹',
            'æ–‡æ¡£ç»“æ„å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–',
            'å»ºè®®æ·»åŠ ç›®å½•å’Œç´¢å¼•',
            'è€ƒè™‘å¢åŠ æ€»ç»“å’Œè¦ç‚¹æç‚¼'
        ]

        # æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´å»ºè®®
        if doc_type == 'ä¼šè®®æ–‡æ¡£':
            suggestions = [
                'å»ºè®®æ˜ç¡®ä¼šè®®å†³ç­–å’Œè´£ä»»äºº',
                'æ·»åŠ åç»­è¡ŒåŠ¨æ—¶é—´èŠ‚ç‚¹',
                'å®Œå–„ä¼šè®®å‚ä¸è€…ä¿¡æ¯'
            ]
        elif doc_type == 'æŠ€æœ¯æ–‡æ¡£':
            suggestions = [
                'å»ºè®®æ·»åŠ æŠ€æœ¯æ¶æ„å›¾',
                'è¡¥å……æ€§èƒ½æµ‹è¯•æ•°æ®',
                'å¢åŠ ä»£ç ç¤ºä¾‹å’Œé…ç½®è¯´æ˜'
            ]

        # ç”Ÿæˆå®Œæ•´çš„AIåˆ†æç»“æœ
        print(f"ğŸ­ Generating comprehensive AI analysis result...")
        print(f"ğŸ­ Content length: {len(content)} characters")
        print(f"ğŸ­ Document type: {doc_type}")
        print(f"ğŸ­ Lines: {lines}, Paragraphs: {paragraphs}")

        result = {
            # åŸå§‹æ–‡æ¡£å†…å®¹
            'text_content': content,
            'content_summary': content_summary,

            # æ–‡æ¡£ç»“æ„ä¿¡æ¯
            'structure_info': {
                'lines': lines,
                'paragraphs': paragraphs,
                'characters': characters,
                'words': len(content.split()) if content else 0,
                'file_exists': file_exists,
                'file_readable': bool(content and content != "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå†…å®¹è¿›è¡Œæ¼”ç¤ºã€‚"),
                'estimated_reading_time': f"{max(1, len(content.split()) // 200)} åˆ†é’Ÿ" if content else "0 åˆ†é’Ÿ"
            },

            # æ™ºèƒ½åœºæ™¯åˆ†æ
            'scenario_analysis': {
                'document_type': doc_type,
                'scenario': scenario,
                'key_points': key_points,
                'confidence': 0.85 if content else 0.5,
                'analysis_depth': 'comprehensive'
            },

            # å…³é”®å®ä½“æå–
            'key_entities': entities,

            # æ™ºèƒ½å†…å®¹åˆ†æ
            'content_analysis': analyze_content_depth(content, key_points),

            # æ”¹è¿›å»ºè®®
            'suggestions': improvement_suggestions,

            # è´¨é‡è¯„ä¼°
            'quality_assessment': {
                'completeness': 0.8 if len(content.split()) > 50 else 0.5,
                'clarity': 0.7,
                'structure': 0.6 if paragraphs > 1 else 0.4,
                'overall_score': 0.7,
                'areas_for_improvement': ['ç»“æ„ä¼˜åŒ–', 'å†…å®¹è¡¥å……', 'æ ¼å¼è§„èŒƒ']
            },

            # å¤„ç†çŠ¶æ€
            'processing_status': 'completed',
            'mock_mode': True,
            'processing_time': 'æ¨¡æ‹Ÿå¤„ç†æ—¶é—´: 2.3ç§’',
            'ai_features_used': [
                'æ™ºèƒ½æ–‡æ¡£åˆ†ç±»',
                'å†…å®¹æ‘˜è¦ç”Ÿæˆ',
                'å…³é”®å®ä½“æå–',
                'åœºæ™¯åˆ†æ',
                'æ”¹è¿›å»ºè®®ç”Ÿæˆ',
                'è´¨é‡è¯„ä¼°'
            ]
        }

        print(f"ğŸ­ Mock result generated successfully")
        print(f"ğŸ­ Result keys: {list(result.keys())}")
        return result

    except Exception as e:
        print(f"âŒ MOCK PROCESSING ERROR:")
        print(f"   - Error type: {type(e).__name__}")
        print(f"   - Error message: {str(e)}")
        import traceback
        print(f"   - Full traceback:")
        print(traceback.format_exc())

        # è¿”å›åŸºæœ¬çš„é”™è¯¯æ¢å¤ç»“æœ
        print(f"ğŸ­ Returning error recovery result...")
        return {
            'text_content': 'å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¿”å›åŸºæœ¬æ¨¡æ‹Ÿç»“æœ',
            'structure_info': {
                'lines': 1,
                'paragraphs': 1,
                'characters': 20,
                'error': str(e)
            },
            'scenario_analysis': {
                'document_type': 'é”™è¯¯æ¢å¤æ¨¡å¼',
                'scenario': 'ç³»ç»Ÿå¼‚å¸¸',
                'key_points': ['å¤„ç†å¼‚å¸¸', 'é”™è¯¯æ¢å¤'],
                'confidence': 0.1
            },
            'suggestions': [
                'è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®',
                'å°è¯•é‡æ–°ä¸Šä¼ æ–‡ä»¶',
                'è”ç³»æŠ€æœ¯æ”¯æŒ'
            ],
            'processing_status': 'error_recovery',
            'mock_mode': True,
            'error': str(e)
        }

@app.route('/')
def index():
    """Serve the main web interface."""
    return render_template('index.html')

@app.route('/demo')
def demo():
    """Serve the demo page."""
    return render_template('demo.html')

@app.route('/examples/<filename>')
def download_example(filename):
    """Download example files."""
    try:
        examples_dir = os.path.join(project_root, 'examples')
        return send_from_directory(examples_dir, filename, as_attachment=True)
    except Exception as e:
        return jsonify({'error': f'æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}'}), 404

@app.route('/debug_frontend.html')
def debug_frontend():
    """Serve the debug frontend page."""
    try:
        with open('debug_frontend.html', 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except FileNotFoundError:
        return "Debug frontend page not found", 404

@app.route('/test_ai_features.html')
def test_ai_features():
    """Serve the AI features test page."""
    try:
        with open('test_ai_features.html', 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except FileNotFoundError:
        return "AI features test page not found", 404

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """Handle file upload and processing."""
    print("=" * 80)
    print("ğŸš€ UPLOAD REQUEST RECEIVED")
    print("=" * 80)

    try:
        # Debug request information
        print(f"ğŸ“‹ Request method: {request.method}")
        print(f"ğŸ“‹ Request content type: {request.content_type}")
        print(f"ğŸ“‹ Request files: {list(request.files.keys())}")
        print(f"ğŸ“‹ Request form data: {dict(request.form)}")
        print(f"ğŸ“‹ Request headers: {dict(request.headers)}")

        if 'file' not in request.files:
            print("âŒ ERROR: No file provided in request")
            return jsonify({'error': 'No file provided'}), 400

        file = request.files['file']
        if file.filename == '':
            print("âŒ ERROR: No file selected")
            return jsonify({'error': 'No file selected'}), 400

        print(f"ğŸ“„ File info:")
        print(f"   - Filename: {file.filename}")
        print(f"   - Content type: {file.content_type}")

        # Check file size
        file.seek(0, 2)  # Seek to end
        file_size = file.tell()
        file.seek(0)  # Reset to beginning
        print(f"   - File size: {file_size} bytes")

        if not allowed_file(file.filename):
            print(f"âŒ ERROR: File type not allowed: {file.filename}")
            return jsonify({'error': 'Unsupported file type'}), 400

        # è·å–APIç±»å‹å’Œæ¨¡å‹åç§°
        api_type = request.form.get('api_type', 'xingcheng')
        model_name = request.form.get('model_name', None)

        print(f"ğŸ”§ Processing configuration:")
        print(f"   - API type: {api_type}")
        print(f"   - Model name: {model_name}")
        
        # Generate unique filename
        filename = secure_filename(file.filename)
        unique_filename = f"{uuid.uuid4().hex}_{filename}"
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)
        
        print(f"Processing file: {filename}")
        print(f"File path: {filepath}")
        print(f"API type: {api_type}, Model: {model_name}")
        
        # Save file
        print(f"ğŸ’¾ Saving file to: {filepath}")
        file.save(filepath)
        print(f"âœ… File saved successfully")

        # Verify file exists and is readable
        if os.path.exists(filepath):
            file_size_on_disk = os.path.getsize(filepath)
            print(f"âœ… File verified on disk: {file_size_on_disk} bytes")
        else:
            print(f"âŒ ERROR: File not found on disk after save!")
            return jsonify({'error': 'File save failed'}), 500

        # Process document
        print(f"ğŸ”„ Starting document processing...")
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜ç¡®é€‰æ‹©äº†æ¨¡æ‹Ÿæ¨¡å¼
            if api_type == 'mock':
                print("ğŸ­ Using explicit mock processing mode")
                print(f"ğŸ­ Calling mock_process_document with: {filepath}")
                result = mock_process_document(file_path=filepath)
                print(f"ğŸ­ Mock processing returned: {type(result)}")
                if isinstance(result, dict):
                    print(f"ğŸ­ Result keys: {list(result.keys())}")
                    if 'error' in result:
                        print(f"âŒ Mock processing error: {result['error']}")
                else:
                    print(f"âŒ Unexpected result type: {type(result)}")
            else:
                print(f"ğŸŒ Initializing real API: {api_type}")
                orchestrator = initialize_agent(api_type, model_name)

                if orchestrator is None:
                    # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ä½œä¸ºå›é€€
                    print("ğŸ­ API not available, using mock processing mode as fallback")
                    result = mock_process_document(file_path=filepath)
                else:
                    # ä½¿ç”¨çœŸå®API
                    print("ğŸŒ Using real API processing")
                    result = orchestrator.process_document(file_path=filepath)

            # è¯¦ç»†æ£€æŸ¥å¤„ç†ç»“æœ
            print(f"ğŸ“Š Processing result analysis:")
            print(f"   - Result type: {type(result)}")
            if isinstance(result, dict):
                print(f"   - Result keys: {list(result.keys())}")
                status = result.get('processing_status', 'unknown')
                mock_mode = result.get('mock_mode', False)
                error = result.get('error', None)
                print(f"   - Status: {status}")
                print(f"   - Mock mode: {mock_mode}")
                if error:
                    print(f"   - Error: {error}")

                # Check if result has required fields
                required_fields = ['text_content', 'structure_info', 'scenario_analysis']
                missing_fields = [field for field in required_fields if field not in result]
                if missing_fields:
                    print(f"âš ï¸  Missing required fields: {missing_fields}")
            else:
                print(f"âŒ Result is not a dictionary: {result}")

            print(f"âœ… Processing completed successfully")
            
            # Clean up uploaded file
            print(f"ğŸ§¹ Cleaning up file: {filepath}")
            os.remove(filepath)
            print(f"âœ… File cleaned up successfully")

            # Prepare response
            response_data = {
                'success': True,
                'result': result,
                'filename': filename,
                'processed_at': datetime.now().isoformat(),
                'api_type': api_type,
                'model_name': model_name
            }

            print(f"ğŸ“¤ Preparing JSON response:")
            print(f"   - Success: {response_data['success']}")
            print(f"   - Filename: {response_data['filename']}")
            print(f"   - API type: {response_data['api_type']}")
            print(f"   - Result type: {type(response_data['result'])}")

            # Try to serialize to JSON to catch any issues
            try:
                import json
                json_str = json.dumps(response_data, ensure_ascii=False, default=str)
                print(f"âœ… JSON serialization successful: {len(json_str)} characters")
            except Exception as json_error:
                print(f"âŒ JSON serialization failed: {json_error}")
                return jsonify({'error': f'Response serialization failed: {str(json_error)}'}), 500

            print(f"ğŸ‰ Returning successful response")
            print("=" * 80)
            return jsonify(response_data)
            
        except Exception as e:
            print(f"âŒ PROCESSING ERROR OCCURRED:")
            print(f"   - Error type: {type(e).__name__}")
            print(f"   - Error message: {str(e)}")
            print(f"   - Full traceback:")
            print(traceback.format_exc())

            # Clean up file on error
            if os.path.exists(filepath):
                print(f"ğŸ§¹ Cleaning up file after error: {filepath}")
                os.remove(filepath)
                print(f"âœ… File cleaned up after error")
            
            # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ï¼Œå°è¯•ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
            if "timeout" in str(e).lower() or "connection" in str(e).lower() or "network" in str(e).lower():
                print("Network error detected, trying mock mode")
                try:
                    # é‡æ–°è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡Œæ¨¡æ‹Ÿå¤„ç†
                    if os.path.exists(filepath):
                        result = mock_process_document(file_path=filepath)
                    else:
                        # å¦‚æœæ–‡ä»¶å·²è¢«åˆ é™¤ï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„æ¨¡æ‹Ÿç»“æœ
                        result = {
                            'text_content': 'Mock content for network error fallback',
                            'structure_info': {'lines': 1, 'paragraphs': 1, 'characters': 40},
                            'scenario_analysis': {
                                'document_type': 'é€šç”¨æ–‡æ¡£',
                                'scenario': 'ç½‘ç»œé”™è¯¯å›é€€æ¨¡å¼',
                                'key_points': ['ç½‘ç»œè¿æ¥é—®é¢˜', 'ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼'],
                                'confidence': 0.5
                            },
                            'suggestions': ['æ£€æŸ¥ç½‘ç»œè¿æ¥', 'ç¨åé‡è¯•'],
                            'processing_status': 'completed_with_fallback',
                            'mock_mode': True,
                            'fallback_reason': 'network_error'
                        }

                    return jsonify({
                        'success': True,
                        'result': result,
                        'filename': filename,
                        'processed_at': datetime.now().isoformat(),
                        'note': 'Used mock mode due to network issues',
                        'api_type': 'mock_fallback',
                        'model_name': 'fallback'
                    })
                except Exception as mock_error:
                    return jsonify({'error': f'Processing failed: {str(e)} (Mock mode also failed: {str(mock_error)})'}), 500
            
            return jsonify({'error': f'Processing failed: {str(e)}'}), 500
            
    except Exception as e:
        print(f"âŒ UPLOAD ERROR OCCURRED:")
        print(f"   - Error type: {type(e).__name__}")
        print(f"   - Error message: {str(e)}")
        print(f"   - Full traceback:")
        print(traceback.format_exc())
        print("=" * 80)
        return jsonify({'error': f'Upload failed: {str(e)}'}), 500

@app.route('/api/health')
def health_check():
    """Health check endpoint."""
    try:
        # Test different API types
        api_status = {}
        
        # Test Xingcheng API
        try:
            orchestrator = initialize_agent("xingcheng")
            api_status['xingcheng'] = {
                'status': 'available' if orchestrator else 'unavailable',
                'mock_mode': orchestrator is None
            }
        except Exception as e:
            api_status['xingcheng'] = {'status': 'error', 'error': str(e)}
        
        # Test Multi API
        try:
            orchestrator = initialize_agent("multi")
            api_status['multi'] = {
                'status': 'available' if orchestrator else 'unavailable',
                'mock_mode': orchestrator is None
            }
        except Exception as e:
            api_status['multi'] = {'status': 'error', 'error': str(e)}
        
        return jsonify({
            'status': 'healthy',
            'api_status': api_status,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/api/config')
def get_config():
    """Get current configuration (without sensitive data)."""
    return jsonify({
        'allowed_extensions': list(app.config['ALLOWED_EXTENSIONS']),
        'max_file_size_mb': app.config['MAX_CONTENT_LENGTH'] // (1024 * 1024),
        'api_types': ['xingcheng', 'multi', 'mock'],
        'xingcheng_configured': bool(os.getenv("XINGCHENG_API_KEY")),
        'qiniu_configured': bool(os.getenv("QINIU_API_KEY")),
        'together_configured': bool(os.getenv("TOGETHER_API_KEY")),
        'openrouter_configured': bool(os.getenv("OPENROUTER_API_KEY")),
        'mock_mode_available': True,
        'format_alignment_available': True,
        'document_fill_available': True
    })

@app.route('/api/format-alignment', methods=['POST'])
def format_alignment():
    """å¤„ç†æ–‡æ¡£æ ¼å¼å¯¹é½è¯·æ±‚"""
    global format_coordinator

    try:
        # åˆå§‹åŒ–æ ¼å¼åè°ƒå™¨
        if format_coordinator is None:
            format_coordinator = FormatAlignmentCoordinator()

        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        user_input = data.get('user_input', '')
        uploaded_files = data.get('uploaded_files', {})

        # å¤„ç†ç”¨æˆ·è¯·æ±‚
        result = format_coordinator.process_user_request(user_input, uploaded_files)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'æ ¼å¼å¯¹é½å¤„ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/api/format-templates', methods=['GET'])
def list_format_templates():
    """è·å–æ‰€æœ‰æ ¼å¼æ¨¡æ¿"""
    global format_coordinator

    try:
        if format_coordinator is None:
            format_coordinator = FormatAlignmentCoordinator()

        templates = format_coordinator.format_extractor.list_format_templates()

        return jsonify({
            'success': True,
            'templates': templates,
            'count': len(templates)
        })

    except Exception as e:
        return jsonify({'error': f'è·å–æ¨¡æ¿åˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/api/format-templates/<template_id>', methods=['GET'])
def get_format_template(template_id):
    """è·å–ç‰¹å®šæ ¼å¼æ¨¡æ¿"""
    global format_coordinator

    try:
        if format_coordinator is None:
            format_coordinator = FormatAlignmentCoordinator()

        template_data = format_coordinator.format_extractor.load_format_template(template_id)

        if 'error' in template_data:
            return jsonify(template_data), 404

        return jsonify({
            'success': True,
            'template': template_data
        })

    except Exception as e:
        return jsonify({'error': f'è·å–æ¨¡æ¿å¤±è´¥: {str(e)}'}), 500

@app.route('/api/format-templates/<template_id>/apply', methods=['POST'])
def apply_format_template(template_id):
    """åº”ç”¨æ ¼å¼æ¨¡æ¿åˆ°æ–‡æ¡£"""
    global format_coordinator

    try:
        if format_coordinator is None:
            format_coordinator = FormatAlignmentCoordinator()

        data = request.get_json()
        document_name = data.get('document_name', '')
        document_content = data.get('document_content', '')

        if not document_name or not document_content:
            return jsonify({'error': 'ç¼ºå°‘æ–‡æ¡£åç§°æˆ–å†…å®¹'}), 400

        # æ·»åŠ æ–‡æ¡£åˆ°ä¼šè¯
        format_coordinator.add_document(document_name, document_content)

        # åº”ç”¨æ¨¡æ¿
        result = format_coordinator.use_saved_template(template_id, document_name)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'åº”ç”¨æ¨¡æ¿å¤±è´¥: {str(e)}'}), 500

@app.route('/api/models')
def get_available_models():
    """Get available models for each API type."""
    try:
        # è·å–å¤šAPIå®¢æˆ·ç«¯çš„å¯ç”¨æ¨¡å‹
        multi_client = MultiLLMClient()
        available_models = multi_client.get_available_models()
        
        models_by_api = {
            'xingcheng': ['x1', 'x2', 'x3'],
            'multi': available_models,
            'mock': ['mock-model']
        }
        
        return jsonify({
            'models': models_by_api,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            'error': str(e),
            'models': {
                'xingcheng': ['x1'],
                'mock': ['mock-model']
            }
        }), 500

@app.route('/api/document-fill/start', methods=['POST'])
def start_document_fill():
    """å¼€å§‹æ–‡æ¡£å¡«å……æµç¨‹"""
    global fill_coordinator

    try:
        if fill_coordinator is None:
            fill_coordinator = DocumentFillCoordinator()

        data = request.get_json()
        document_content = data.get('document_content', '')
        document_name = data.get('document_name', '')

        if not document_content:
            return jsonify({'error': 'ç¼ºå°‘æ–‡æ¡£å†…å®¹'}), 400

        result = fill_coordinator.start_document_fill(document_content, document_name)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'å¯åŠ¨æ–‡æ¡£å¡«å……å¤±è´¥: {str(e)}'}), 500

@app.route('/api/document-fill/respond', methods=['POST'])
def respond_to_fill_question():
    """å“åº”æ–‡æ¡£å¡«å……é—®é¢˜"""
    global fill_coordinator

    try:
        if fill_coordinator is None:
            return jsonify({'error': 'æ–‡æ¡£å¡«å……ä¼šè¯æœªåˆå§‹åŒ–'}), 400

        data = request.get_json()
        user_input = data.get('user_input', '')

        if not user_input:
            return jsonify({'error': 'ç¼ºå°‘ç”¨æˆ·è¾“å…¥'}), 400

        result = fill_coordinator.process_user_response(user_input)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'å¤„ç†ç”¨æˆ·å›å¤å¤±è´¥: {str(e)}'}), 500

@app.route('/api/document-fill/status', methods=['GET'])
def get_fill_status():
    """è·å–æ–‡æ¡£å¡«å……çŠ¶æ€"""
    global fill_coordinator

    try:
        if fill_coordinator is None:
            return jsonify({'error': 'æ–‡æ¡£å¡«å……ä¼šè¯æœªåˆå§‹åŒ–'}), 400

        status = fill_coordinator.get_session_status()

        return jsonify({
            'success': True,
            'status': status
        })

    except Exception as e:
        return jsonify({'error': f'è·å–çŠ¶æ€å¤±è´¥: {str(e)}'}), 500

@app.route('/api/document-fill/result', methods=['GET'])
def get_fill_result():
    """è·å–æ–‡æ¡£å¡«å……ç»“æœ"""
    global fill_coordinator

    try:
        if fill_coordinator is None:
            return jsonify({'error': 'æ–‡æ¡£å¡«å……ä¼šè¯æœªåˆå§‹åŒ–'}), 400

        result = fill_coordinator.get_fill_result()

        if not result:
            return jsonify({'error': 'å¡«å……ç»“æœä¸å¯ç”¨'}), 404

        return jsonify({
            'success': True,
            'result': result
        })

    except Exception as e:
        return jsonify({'error': f'è·å–å¡«å……ç»“æœå¤±è´¥: {str(e)}'}), 500

@app.route('/api/document-fill/download', methods=['GET'])
def download_filled_document():
    """ä¸‹è½½å¡«å……åçš„æ–‡æ¡£"""
    global fill_coordinator

    try:
        if fill_coordinator is None:
            return jsonify({'error': 'æ–‡æ¡£å¡«å……ä¼šè¯æœªåˆå§‹åŒ–'}), 400

        result = fill_coordinator.get_fill_result()

        if not result or not result.get('html_content'):
            return jsonify({'error': 'å¡«å……ç»“æœä¸å¯ç”¨'}), 404

        html_content = result['html_content']

        # åˆ›å»ºå“åº”
        from flask import make_response
        response = make_response(html_content)
        response.headers['Content-Type'] = 'text/html; charset=utf-8'
        response.headers['Content-Disposition'] = 'attachment; filename=filled_document.html'

        return response

    except Exception as e:
        return jsonify({'error': f'ä¸‹è½½å¤±è´¥: {str(e)}'}), 500

@app.route('/api/document-fill/add-material', methods=['POST'])
def add_supplementary_material():
    """æ·»åŠ è¡¥å……ææ–™"""
    global fill_coordinator

    try:
        if fill_coordinator is None:
            return jsonify({'error': 'æ–‡æ¡£å¡«å……ä¼šè¯æœªåˆå§‹åŒ–'}), 400

        data = request.get_json()
        material_name = data.get('material_name', '')
        material_content = data.get('material_content', '')

        if not material_name or not material_content:
            return jsonify({'error': 'ç¼ºå°‘ææ–™åç§°æˆ–å†…å®¹'}), 400

        result = fill_coordinator.add_supplementary_material(material_name, material_content)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'æ·»åŠ è¡¥å……ææ–™å¤±è´¥: {str(e)}'}), 500

@app.route('/api/writing-style/analyze', methods=['POST'])
def analyze_writing_style():
    """åˆ†ææ–‡æ¡£å†™ä½œé£æ ¼"""
    global style_analyzer

    try:
        if style_analyzer is None:
            style_analyzer = WritingStyleAnalyzer()

        data = request.get_json()
        document_content = data.get('document_content', '')
        document_name = data.get('document_name', '')

        if not document_content:
            return jsonify({'error': 'ç¼ºå°‘æ–‡æ¡£å†…å®¹'}), 400

        result = style_analyzer.analyze_writing_style(document_content, document_name)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'æ–‡é£åˆ†æå¤±è´¥: {str(e)}'}), 500

@app.route('/api/writing-style/save-template', methods=['POST'])
def save_writing_style_template():
    """ä¿å­˜æ–‡é£æ¨¡æ¿"""
    global style_analyzer, fill_coordinator

    try:
        if style_analyzer is None:
            style_analyzer = WritingStyleAnalyzer()

        data = request.get_json()
        reference_content = data.get('reference_content', '')
        reference_name = data.get('reference_name', '')

        if not reference_content:
            return jsonify({'error': 'ç¼ºå°‘å‚è€ƒæ–‡æ¡£å†…å®¹'}), 400

        # å¦‚æœæœ‰æ´»è·ƒçš„å¡«å……ä¼šè¯ï¼Œä½¿ç”¨ä¼šè¯ä¸­çš„æ–¹æ³•
        if fill_coordinator is not None:
            result = fill_coordinator.analyze_and_save_writing_style(reference_content, reference_name)
        else:
            # ç›´æ¥ä½¿ç”¨åˆ†æå™¨
            analysis_result = style_analyzer.analyze_writing_style(reference_content, reference_name)
            if "error" in analysis_result:
                return jsonify(analysis_result), 500

            result = style_analyzer.save_style_template(analysis_result)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'ä¿å­˜æ–‡é£æ¨¡æ¿å¤±è´¥: {str(e)}'}), 500

@app.route('/api/writing-style/templates', methods=['GET'])
def list_writing_style_templates():
    """è·å–æ‰€æœ‰æ–‡é£æ¨¡æ¿"""
    global style_analyzer

    try:
        if style_analyzer is None:
            style_analyzer = WritingStyleAnalyzer()

        templates = style_analyzer.list_style_templates()

        return jsonify({
            'success': True,
            'templates': templates,
            'count': len(templates)
        })

    except Exception as e:
        return jsonify({'error': f'è·å–æ–‡é£æ¨¡æ¿åˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/api/writing-style/templates/<template_id>', methods=['GET'])
def get_writing_style_template(template_id):
    """è·å–ç‰¹å®šæ–‡é£æ¨¡æ¿"""
    global style_analyzer

    try:
        if style_analyzer is None:
            style_analyzer = WritingStyleAnalyzer()

        template_data = style_analyzer.load_style_template(template_id)

        if 'error' in template_data:
            return jsonify(template_data), 404

        return jsonify({
            'success': True,
            'template': template_data
        })

    except Exception as e:
        return jsonify({'error': f'è·å–æ–‡é£æ¨¡æ¿å¤±è´¥: {str(e)}'}), 500

@app.route('/api/document-fill/set-style', methods=['POST'])
def set_writing_style_template():
    """è®¾ç½®æ–‡æ¡£å¡«å……çš„æ–‡é£æ¨¡æ¿"""
    global fill_coordinator

    try:
        if fill_coordinator is None:
            return jsonify({'error': 'æ–‡æ¡£å¡«å……ä¼šè¯æœªåˆå§‹åŒ–'}), 400

        data = request.get_json()
        template_id = data.get('template_id', '')

        if not template_id:
            return jsonify({'error': 'ç¼ºå°‘æ¨¡æ¿ID'}), 400

        result = fill_coordinator.set_writing_style_template(template_id)

        return jsonify(result)

    except Exception as e:
        return jsonify({'error': f'è®¾ç½®æ–‡é£æ¨¡æ¿å¤±è´¥: {str(e)}'}), 500

if __name__ == '__main__':
    print("Starting Office Document Agent Web Server...")
    print(f"Project root: {project_root}")
    print(f"Template folder: {app.template_folder}")
    print(f"Static folder: {app.static_folder}")
    print("Web interface will be available at: http://localhost:5000")
    print("API endpoints:")
    print("  - GET  /api/health     - Health check")
    print("  - GET  /api/config     - Configuration info")
    print("  - GET  /api/models     - Available models")
    print("  - POST /api/upload     - Upload and process document")
    
    # æ£€æŸ¥APIé…ç½®
    api_keys = {
        'XINGCHENG_API_KEY': os.getenv("XINGCHENG_API_KEY"),
        'QINIU_API_KEY': os.getenv("QINIU_API_KEY"),
        'TOGETHER_API_KEY': os.getenv("TOGETHER_API_KEY"),
        'OPENROUTER_API_KEY': os.getenv("OPENROUTER_API_KEY")
    }
    
    configured_apis = [name for name, key in api_keys.items() if key]
    if configured_apis:
        print(f"Configured APIs: {', '.join(configured_apis)}")
    else:
        print("Warning: No API keys found in .env file")
        print("The system will use mock mode for testing")
    
    app.run(debug=True, host='0.0.0.0', port=5000) 