#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Semantic Unit Identifier - æ ¸å¿ƒæ¨¡å—

Author: AI Assistant (Claude)
Created: 2025-01-28
Last Modified: 2025-01-28
Modified By: AI Assistant (Claude)
AI Assisted: æ˜¯ - Claude 3.5 Sonnet
Version: v1.0
License: MIT
"""


import json
import re
from typing import Dict, Any, List, Optional, Union
from datetime import datetime


class SemanticUnitIdentifier:
    """è¯­ä¹‰å•å…ƒè¯†åˆ«å™¨ - è®¯é£å¤§æ¨¡å‹ä½œä¸ºè¯­ä¹‰åˆ†æåŠ©æ‰‹"""
    
    def __init__(self, llm_client=None):
        """
        åˆå§‹åŒ–è¯­ä¹‰å•å…ƒè¯†åˆ«å™¨
        
        Args:
            llm_client: è®¯é£å¤§æ¨¡å‹å®¢æˆ·ç«¯
        """
        self.llm_client = llm_client
        self.identification_templates = self._init_prompt_templates()
    
    def _init_prompt_templates(self) -> Dict[str, str]:
        """åˆå§‹åŒ–æç¤ºè¯æ¨¡æ¿"""
        return {
            "comprehensive_analysis": """è¯·å¯¹ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œå…¨é¢çš„è¯­ä¹‰å•å…ƒè¯†åˆ«ï¼Œä»¥JSONæ ¼å¼è¾“å‡ºç»“æœï¼š

æ–‡æœ¬å†…å®¹ï¼š
{text}

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¾“å‡ºåˆ†æç»“æœï¼š
{{
  "concepts": [
    {{"text": "æ¦‚å¿µåç§°", "role": "æ ¸å¿ƒæ¦‚å¿µ/ç›¸å…³æ¦‚å¿µ", "importance": 1-5}},
    ...
  ],
  "named_entities": [
    {{"text": "å®ä½“åç§°", "type": "äººå/åœ°å/ç»„ç»‡å/äº§å“å/å…¶ä»–", "context": "å‡ºç°è¯­å¢ƒ"}},
    ...
  ],
  "key_adjectives": [
    {{"text": "å½¢å®¹è¯", "context": "ä¿®é¥°è¯­å¢ƒ", "sentiment_intensity": 1-5, "sentiment_polarity": "ç§¯æ/æ¶ˆæ/ä¸­æ€§"}},
    ...
  ],
  "key_verbs": [
    {{"text": "åŠ¨è¯", "context": "ä½¿ç”¨è¯­å¢ƒ", "action_type": "çŠ¶æ€/åŠ¨ä½œ/å˜åŒ–", "intensity": 1-5}},
    ...
  ],
  "key_phrases": [
    {{"text": "å…³é”®çŸ­è¯­", "role": "æŠ€æœ¯æœ¯è¯­/ä¸“ä¸šè¡¨è¾¾/ç‰¹è‰²ç”¨æ³•", "domain": "æ‰€å±é¢†åŸŸ"}},
    ...
  ],
  "semantic_relations": [
    {{"entity1": "æ¦‚å¿µ1", "relation": "å…³ç³»ç±»å‹", "entity2": "æ¦‚å¿µ2", "strength": 1-5}},
    ...
  ]
}}

è¦æ±‚ï¼š
1. è¯†åˆ«å‡ºæœ€é‡è¦çš„5-10ä¸ªæ ¸å¿ƒæ¦‚å¿µ
2. æå–æ‰€æœ‰å‘½åå®ä½“å¹¶åˆ†ç±»
3. æ‰¾å‡ºå…·æœ‰å¼ºçƒˆæƒ…æ„Ÿè‰²å½©æˆ–ä¿®é¥°ä½œç”¨çš„å½¢å®¹è¯
4. è¯†åˆ«è¡¨è¾¾åŠ¨ä½œã€çŠ¶æ€å˜åŒ–çš„å…³é”®åŠ¨è¯
5. æå–ä¸“ä¸šæœ¯è¯­å’Œç‰¹è‰²è¡¨è¾¾
6. åˆ†ææ¦‚å¿µé—´çš„è¯­ä¹‰å…³ç³»

è¯·ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸¥æ ¼ç¬¦åˆJSONè§„èŒƒã€‚""",

            "concept_extraction": """è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ ¸å¿ƒæ¦‚å¿µå’Œç›¸å…³æ¦‚å¿µï¼š

æ–‡æœ¬ï¼š{text}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "core_concepts": [
    {{"concept": "æ¦‚å¿µå", "importance": 1-5, "category": "ä¸»é¢˜/æ–¹æ³•/å·¥å…·/ç†è®º"}},
    ...
  ],
  "related_concepts": [
    {{"concept": "æ¦‚å¿µå", "relation_to_core": "æ”¯æ’‘/åº”ç”¨/å¯¹æ¯”/æ‰©å±•", "importance": 1-5}},
    ...
  ],
  "concept_clusters": [
    {{"cluster_name": "æ¦‚å¿µç¾¤åç§°", "concepts": ["æ¦‚å¿µ1", "æ¦‚å¿µ2"], "theme": "ä¸»é¢˜æè¿°"}},
    ...
  ]
}}""",

            "entity_recognition": """è¯·è¯†åˆ«ä»¥ä¸‹æ–‡æœ¬ä¸­çš„å‘½åå®ä½“ï¼š

æ–‡æœ¬ï¼š{text}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "persons": [
    {{"name": "äººå", "role": "è§’è‰²/èŒä½", "context": "å‡ºç°è¯­å¢ƒ"}},
    ...
  ],
  "organizations": [
    {{"name": "ç»„ç»‡å", "type": "å…¬å¸/æœºæ„/å›¢ä½“", "context": "å‡ºç°è¯­å¢ƒ"}},
    ...
  ],
  "locations": [
    {{"name": "åœ°å", "type": "å›½å®¶/åŸå¸‚/åœ°åŒº", "context": "å‡ºç°è¯­å¢ƒ"}},
    ...
  ],
  "products": [
    {{"name": "äº§å“å", "category": "è½¯ä»¶/ç¡¬ä»¶/æœåŠ¡", "context": "å‡ºç°è¯­å¢ƒ"}},
    ...
  ],
  "others": [
    {{"name": "å…¶ä»–å®ä½“", "type": "ç±»å‹", "context": "å‡ºç°è¯­å¢ƒ"}},
    ...
  ]
}}""",

            "sentiment_analysis": """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸­å…³é”®è¯æ±‡çš„æƒ…æ„Ÿè‰²å½©ï¼š

æ–‡æœ¬ï¼š{text}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "emotional_adjectives": [
    {{"word": "å½¢å®¹è¯", "sentiment": "ç§¯æ/æ¶ˆæ/ä¸­æ€§", "intensity": 1-5, "context": "ä½¿ç”¨è¯­å¢ƒ"}},
    ...
  ],
  "emotional_verbs": [
    {{"word": "åŠ¨è¯", "sentiment": "ç§¯æ/æ¶ˆæ/ä¸­æ€§", "intensity": 1-5, "context": "ä½¿ç”¨è¯­å¢ƒ"}},
    ...
  ],
  "emotional_phrases": [
    {{"phrase": "çŸ­è¯­", "sentiment": "ç§¯æ/æ¶ˆæ/ä¸­æ€§", "intensity": 1-5, "context": "ä½¿ç”¨è¯­å¢ƒ"}},
    ...
  ],
  "overall_sentiment": {{"polarity": "ç§¯æ/æ¶ˆæ/ä¸­æ€§", "intensity": 1-5, "confidence": 1-5}}
}}""",

            "semantic_relations": """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸­æ¦‚å¿µä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼š

æ–‡æœ¬ï¼š{text}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "hierarchical_relations": [
    {{"parent": "ä¸Šçº§æ¦‚å¿µ", "child": "ä¸‹çº§æ¦‚å¿µ", "relation_type": "åŒ…å«/å±äº"}},
    ...
  ],
  "associative_relations": [
    {{"concept1": "æ¦‚å¿µ1", "concept2": "æ¦‚å¿µ2", "relation_type": "ç›¸å…³/å¯¹æ¯”/å› æœ", "strength": 1-5}},
    ...
  ],
  "temporal_relations": [
    {{"before": "å‰åºæ¦‚å¿µ", "after": "åç»­æ¦‚å¿µ", "relation_type": "æ—¶é—´é¡ºåº/å‘å±•è¿‡ç¨‹"}},
    ...
  ],
  "causal_relations": [
    {{"cause": "åŸå› ", "effect": "ç»“æœ", "strength": 1-5}},
    ...
  ]
}}"""
        }
    
    def identify_semantic_units(self, text: str, analysis_type: str = "comprehensive") -> Dict[str, Any]:
        """
        è¯†åˆ«æ–‡æœ¬ä¸­çš„è¯­ä¹‰å•å…ƒ
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            analysis_type: åˆ†æç±»å‹ ("comprehensive", "concept", "entity", "sentiment", "relation")
        
        Returns:
            è¯­ä¹‰å•å…ƒè¯†åˆ«ç»“æœ
        """
        if not self.llm_client:
            return {"error": "LLM client not available"}
        
        result = {
            "analysis_time": datetime.now().isoformat(),
            "text_length": len(text),
            "analysis_type": analysis_type,
            "semantic_units": {},
            "raw_response": "",
            "parsing_success": False,
            "success": False
        }
        
        try:
            # é€‰æ‹©åˆé€‚çš„æç¤ºè¯æ¨¡æ¿
            if analysis_type == "comprehensive":
                prompt = self.identification_templates["comprehensive_analysis"].format(text=text)
            elif analysis_type == "concept":
                prompt = self.identification_templates["concept_extraction"].format(text=text)
            elif analysis_type == "entity":
                prompt = self.identification_templates["entity_recognition"].format(text=text)
            elif analysis_type == "sentiment":
                prompt = self.identification_templates["sentiment_analysis"].format(text=text)
            elif analysis_type == "relation":
                prompt = self.identification_templates["semantic_relations"].format(text=text)
            else:
                raise ValueError(f"Unknown analysis type: {analysis_type}")
            
            # è°ƒç”¨è®¯é£å¤§æ¨¡å‹
            print(f"ğŸ” æ­£åœ¨ä½¿ç”¨è®¯é£å¤§æ¨¡å‹è¿›è¡Œ{analysis_type}è¯­ä¹‰åˆ†æ...")
            response = self.llm_client.generate(prompt)
            result["raw_response"] = response
            
            # è§£æJSONå“åº”
            parsed_units = self._parse_llm_response(response, analysis_type)
            result["semantic_units"] = parsed_units
            result["parsing_success"] = True
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            result["analysis_summary"] = self._generate_analysis_summary(parsed_units, analysis_type)
            result["success"] = True
            
            print("âœ… è¯­ä¹‰å•å…ƒè¯†åˆ«å®Œæˆ")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰å•å…ƒè¯†åˆ«å¤±è´¥: {str(e)}")
        
        return result
    
    def _parse_llm_response(self, response: str, analysis_type: str) -> Dict[str, Any]:
        """è§£æLLMå“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–ç»“æ„åŒ–ä¿¡æ¯
            return self._extract_structured_info(response, analysis_type)
            
        except json.JSONDecodeError:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•æ–‡æœ¬è§£æ
            return self._extract_structured_info(response, analysis_type)
    
    def _extract_structured_info(self, response: str, analysis_type: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬å“åº”ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯"""
        extracted = {}
        
        try:
            if analysis_type == "comprehensive":
                # æå–æ¦‚å¿µ
                concepts = re.findall(r'æ¦‚å¿µ[ï¼š:]\s*([^\n]+)', response)
                extracted["concepts"] = [{"text": concept.strip(), "role": "æ ¸å¿ƒæ¦‚å¿µ", "importance": 3} 
                                       for concept in concepts[:10]]
                
                # æå–å®ä½“
                entities = re.findall(r'å®ä½“[ï¼š:]\s*([^\n]+)', response)
                extracted["named_entities"] = [{"text": entity.strip(), "type": "å…¶ä»–", "context": ""} 
                                             for entity in entities[:10]]
                
                # æå–å…³é”®è¯
                keywords = re.findall(r'å…³é”®è¯[ï¼š:]\s*([^\n]+)', response)
                extracted["key_phrases"] = [{"text": keyword.strip(), "role": "å…³é”®è¡¨è¾¾", "domain": "é€šç”¨"} 
                                          for keyword in keywords[:10]]
            
            elif analysis_type == "concept":
                concepts = re.findall(r'([^ï¼Œ,ã€‚ï¼›\n]+)', response)
                extracted["core_concepts"] = [{"concept": concept.strip(), "importance": 3, "category": "ä¸»é¢˜"} 
                                            for concept in concepts[:10] if len(concept.strip()) > 1]
            
            elif analysis_type == "entity":
                # ç®€å•çš„å®ä½“æå–
                lines = response.split('\n')
                persons = []
                organizations = []
                for line in lines:
                    if any(keyword in line for keyword in ['äººå', 'å§“å', 'äººç‰©']):
                        names = re.findall(r'([^\sï¼Œ,ã€‚ï¼›]+)', line)
                        persons.extend([{"name": name, "role": "æœªçŸ¥", "context": ""} for name in names[:5]])
                    elif any(keyword in line for keyword in ['å…¬å¸', 'ç»„ç»‡', 'æœºæ„']):
                        orgs = re.findall(r'([^\sï¼Œ,ã€‚ï¼›]+)', line)
                        organizations.extend([{"name": org, "type": "ç»„ç»‡", "context": ""} for org in orgs[:5]])
                
                extracted["persons"] = persons[:5]
                extracted["organizations"] = organizations[:5]
            
            # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›åŸºæœ¬ç»“æ„
            if not extracted:
                extracted = {"extracted_text": response[:500], "parsing_method": "fallback"}
        
        except Exception as e:
            extracted = {"error": f"Text parsing failed: {str(e)}", "raw_text": response[:200]}
        
        return extracted
    
    def _generate_analysis_summary(self, semantic_units: Dict[str, Any], analysis_type: str) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            "analysis_type": analysis_type,
            "total_units_identified": 0,
            "unit_categories": [],
            "key_findings": []
        }
        
        try:
            if analysis_type == "comprehensive":
                # ç»Ÿè®¡å„ç±»è¯­ä¹‰å•å…ƒæ•°é‡
                concepts_count = len(semantic_units.get("concepts", []))
                entities_count = len(semantic_units.get("named_entities", []))
                adjectives_count = len(semantic_units.get("key_adjectives", []))
                verbs_count = len(semantic_units.get("key_verbs", []))
                phrases_count = len(semantic_units.get("key_phrases", []))
                
                summary["total_units_identified"] = (concepts_count + entities_count + 
                                                   adjectives_count + verbs_count + phrases_count)
                summary["unit_categories"] = [
                    f"æ¦‚å¿µ: {concepts_count}ä¸ª",
                    f"å®ä½“: {entities_count}ä¸ª", 
                    f"å½¢å®¹è¯: {adjectives_count}ä¸ª",
                    f"åŠ¨è¯: {verbs_count}ä¸ª",
                    f"çŸ­è¯­: {phrases_count}ä¸ª"
                ]
                
                # å…³é”®å‘ç°
                if concepts_count > 0:
                    core_concepts = [c.get("text", "") for c in semantic_units.get("concepts", [])[:3]]
                    summary["key_findings"].append(f"æ ¸å¿ƒæ¦‚å¿µ: {', '.join(core_concepts)}")
                
                if entities_count > 0:
                    entities = [e.get("text", "") for e in semantic_units.get("named_entities", [])[:3]]
                    summary["key_findings"].append(f"ä¸»è¦å®ä½“: {', '.join(entities)}")
            
            elif analysis_type == "concept":
                core_concepts = semantic_units.get("core_concepts", [])
                summary["total_units_identified"] = len(core_concepts)
                summary["unit_categories"] = ["æ ¸å¿ƒæ¦‚å¿µ"]
                if core_concepts:
                    top_concepts = [c.get("concept", "") for c in core_concepts[:3]]
                    summary["key_findings"].append(f"ä¸»è¦æ¦‚å¿µ: {', '.join(top_concepts)}")
            
            elif analysis_type == "entity":
                persons = semantic_units.get("persons", [])
                orgs = semantic_units.get("organizations", [])
                summary["total_units_identified"] = len(persons) + len(orgs)
                summary["unit_categories"] = [f"äººå: {len(persons)}ä¸ª", f"ç»„ç»‡: {len(orgs)}ä¸ª"]
            
            elif analysis_type == "sentiment":
                emotional_adj = semantic_units.get("emotional_adjectives", [])
                overall_sentiment = semantic_units.get("overall_sentiment", {})
                summary["total_units_identified"] = len(emotional_adj)
                summary["unit_categories"] = ["æƒ…æ„Ÿå½¢å®¹è¯"]
                if overall_sentiment:
                    polarity = overall_sentiment.get("polarity", "ä¸­æ€§")
                    intensity = overall_sentiment.get("intensity", 3)
                    summary["key_findings"].append(f"æ•´ä½“æƒ…æ„Ÿ: {polarity} (å¼ºåº¦: {intensity})")
            
            elif analysis_type == "relation":
                hierarchical = semantic_units.get("hierarchical_relations", [])
                associative = semantic_units.get("associative_relations", [])
                summary["total_units_identified"] = len(hierarchical) + len(associative)
                summary["unit_categories"] = ["å±‚æ¬¡å…³ç³»", "å…³è”å…³ç³»"]
        
        except Exception as e:
            summary["error"] = str(e)
        
        return summary
    
    def batch_identify_semantic_units(self, texts: List[str], analysis_type: str = "comprehensive") -> Dict[str, Any]:
        """æ‰¹é‡è¯†åˆ«è¯­ä¹‰å•å…ƒ"""
        batch_result = {
            "batch_time": datetime.now().isoformat(),
            "total_texts": len(texts),
            "analysis_type": analysis_type,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "results": [],
            "batch_summary": {}
        }
        
        for i, text in enumerate(texts):
            print(f"å¤„ç†æ–‡æœ¬ {i+1}/{len(texts)}")
            
            try:
                result = self.identify_semantic_units(text, analysis_type)
                result["batch_index"] = i
                
                if result.get("success"):
                    batch_result["successful_analyses"] += 1
                else:
                    batch_result["failed_analyses"] += 1
                
                batch_result["results"].append(result)
                
            except Exception as e:
                batch_result["failed_analyses"] += 1
                batch_result["results"].append({
                    "batch_index": i,
                    "success": False,
                    "error": str(e),
                    "text_preview": text[:100]
                })
        
        # ç”Ÿæˆæ‰¹é‡æ‘˜è¦
        batch_result["batch_summary"] = self._generate_batch_summary(batch_result)
        
        return batch_result
    
    def _generate_batch_summary(self, batch_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æ‘˜è¦"""
        summary = {
            "success_rate": batch_result["successful_analyses"] / batch_result["total_texts"] if batch_result["total_texts"] > 0 else 0,
            "total_units_identified": 0,
            "average_units_per_text": 0,
            "common_concepts": [],
            "common_entities": []
        }
        
        try:
            # ç»Ÿè®¡æ€»çš„è¯­ä¹‰å•å…ƒæ•°é‡
            successful_results = [r for r in batch_result["results"] if r.get("success")]
            
            total_units = 0
            all_concepts = []
            all_entities = []
            
            for result in successful_results:
                analysis_summary = result.get("analysis_summary", {})
                total_units += analysis_summary.get("total_units_identified", 0)
                
                # æ”¶é›†æ¦‚å¿µå’Œå®ä½“
                semantic_units = result.get("semantic_units", {})
                concepts = semantic_units.get("concepts", [])
                entities = semantic_units.get("named_entities", [])
                
                all_concepts.extend([c.get("text", "") for c in concepts])
                all_entities.extend([e.get("text", "") for e in entities])
            
            summary["total_units_identified"] = total_units
            summary["average_units_per_text"] = total_units / len(successful_results) if successful_results else 0
            
            # æ‰¾å‡ºå¸¸è§çš„æ¦‚å¿µå’Œå®ä½“
            from collections import Counter
            concept_counts = Counter(all_concepts)
            entity_counts = Counter(all_entities)
            
            summary["common_concepts"] = [concept for concept, count in concept_counts.most_common(5)]
            summary["common_entities"] = [entity for entity, count in entity_counts.most_common(5)]
        
        except Exception as e:
            summary["error"] = str(e)
        
        return summary
    
    def get_semantic_unit_statistics(self, semantic_units: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–è¯­ä¹‰å•å…ƒç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "concept_count": 0,
            "entity_count": 0,
            "adjective_count": 0,
            "verb_count": 0,
            "phrase_count": 0,
            "relation_count": 0,
            "sentiment_distribution": {},
            "importance_distribution": {}
        }
        
        try:
            # ç»Ÿè®¡å„ç±»è¯­ä¹‰å•å…ƒæ•°é‡
            stats["concept_count"] = len(semantic_units.get("concepts", []))
            stats["entity_count"] = len(semantic_units.get("named_entities", []))
            stats["adjective_count"] = len(semantic_units.get("key_adjectives", []))
            stats["verb_count"] = len(semantic_units.get("key_verbs", []))
            stats["phrase_count"] = len(semantic_units.get("key_phrases", []))
            stats["relation_count"] = len(semantic_units.get("semantic_relations", []))
            
            # æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡
            adjectives = semantic_units.get("key_adjectives", [])
            sentiment_counts = {"ç§¯æ": 0, "æ¶ˆæ": 0, "ä¸­æ€§": 0}
            for adj in adjectives:
                sentiment = adj.get("sentiment_polarity", "ä¸­æ€§")
                sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
            stats["sentiment_distribution"] = sentiment_counts
            
            # é‡è¦æ€§åˆ†å¸ƒç»Ÿè®¡
            concepts = semantic_units.get("concepts", [])
            importance_counts = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
            for concept in concepts:
                importance = concept.get("importance", 3)
                importance_counts[importance] = importance_counts.get(importance, 0) + 1
            stats["importance_distribution"] = importance_counts
        
        except Exception as e:
            stats["error"] = str(e)
        
        return stats
