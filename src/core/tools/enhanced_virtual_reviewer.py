#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Virtual Reviewer - æ ¸å¿ƒæ¨¡å—

Author: AI Assistant (Claude)
Created: 2025-01-28
Last Modified: 2025-01-28
Modified By: AI Assistant (Claude)
AI Assisted: æ˜¯ - Claude 3.5 Sonnet
Version: v1.0
License: MIT
"""


import json
import re
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import logging

from .intelligent_role_selector import IntelligentRoleSelector
from .smart_prompt_generator import SmartPromptGenerator
from .base_tool import BaseTool

class EnhancedVirtualReviewer(BaseTool):
    """
    å¢å¼ºçš„è™šæ‹Ÿè¯„å®¡å™¨ - æœ€å¤§åŒ–åˆ©ç”¨AIæ™ºèƒ½è¿›è¡Œå¤šè§’è‰²æ–‡æ¡£è¯„å®¡
    """
    
    def __init__(self, llm_client=None, knowledge_base: dict = None, **kwargs):
        super().__init__(**kwargs)
        self.llm_client = llm_client
        self.knowledge_base = knowledge_base or {}
        
        # åˆå§‹åŒ–æ™ºèƒ½ç»„ä»¶
        self.role_selector = IntelligentRoleSelector()
        self.prompt_generator = SmartPromptGenerator()
        
        # è¯„å®¡é…ç½®
        self.review_config = {
            "max_roles": 5,  # æœ€å¤§è§’è‰²æ•°é‡
            "min_role_score": 0.3,  # æœ€å°è§’è‰²åŒ¹é…åˆ†æ•°
            "consensus_threshold": 0.7,  # å…±è¯†é˜ˆå€¼
            "priority_weights": {
                "critical": 1.0,
                "high": 0.7,
                "medium": 0.4,
                "low": 0.2
            }
        }
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def execute(self, operation: str = "smart_review", **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¢å¼ºçš„è™šæ‹Ÿè¯„å®¡æ“ä½œ
        
        Args:
            operation: æ“ä½œç±»å‹ (smart_review, analyze_consensus, generate_repair_guide)
            **kwargs: æ“ä½œç‰¹å®šå‚æ•°
        """
        try:
            if operation == "smart_review":
                return self.smart_review_document(**kwargs)
            elif operation == "analyze_consensus":
                return self.analyze_reviewer_consensus(**kwargs)
            elif operation == "generate_repair_guide":
                return self.generate_repair_guidance(**kwargs)
            else:
                return {"error": f"Unknown operation: {operation}"}
        except Exception as e:
            self.logger.error(f"Error in enhanced virtual review: {e}")
            return {"error": f"Error in enhanced virtual review: {e}"}
    
    def smart_review_document(self, document_content: str, 
                            document_type: str = None,
                            review_focus: str = None,
                            custom_roles: List[str] = None) -> Dict[str, Any]:
        """
        æ™ºèƒ½æ–‡æ¡£è¯„å®¡ - è‡ªåŠ¨é€‰æ‹©è§’è‰²å¹¶ç”Ÿæˆä¸“ä¸šè¯„å®¡æ„è§
        
        Args:
            document_content: æ–‡æ¡£å†…å®¹
            document_type: æ–‡æ¡£ç±»å‹ï¼ˆå¯é€‰ï¼‰
            review_focus: è¯„å®¡é‡ç‚¹ï¼ˆå¯é€‰ï¼‰
            custom_roles: è‡ªå®šä¹‰è§’è‰²åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: æ™ºèƒ½è¯„å®¡ç»“æœ
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½æ–‡æ¡£è¯„å®¡")
            
            # 1. æ–‡æ¡£åˆ†æ
            document_analysis = self._analyze_document_characteristics(document_content)
            self.logger.info(f"ğŸ“Š æ–‡æ¡£åˆ†æå®Œæˆ: {document_analysis.get('document_type', 'unknown')}")
            
            # 2. æ™ºèƒ½è§’è‰²é€‰æ‹©
            if custom_roles:
                selected_roles = custom_roles
                self.logger.info(f"ğŸ¯ ä½¿ç”¨è‡ªå®šä¹‰è§’è‰²: {selected_roles}")
            else:
                selected_roles = self.role_selector.select_roles_for_document(
                    document_content, document_type
                )
                self.logger.info(f"ğŸ¯ æ™ºèƒ½é€‰æ‹©è§’è‰²: {selected_roles}")
            
            # 3. è·å–è§’è‰²è¯¦ç»†ä¿¡æ¯
            role_profiles = self.role_selector.get_role_details(selected_roles)
            
            # 4. ç”Ÿæˆä¸“ä¸šæç¤ºè¯
            prompts = self.prompt_generator.generate_multi_role_prompt(
                role_profiles, document_content, document_analysis, review_focus
            )
            
            # 5. æ‰§è¡Œå¤šè§’è‰²è¯„å®¡
            reviewer_results = []
            for role_id, role_profile in zip(selected_roles, role_profiles):
                self.logger.info(f"ğŸ‘¤ å¼€å§‹ {role_profile.get('role_name', role_id)} è¯„å®¡")
                
                review_result = self._execute_single_role_review(
                    role_id, role_profile, prompts.get(role_id, ""), 
                    document_content, document_analysis
                )
                
                if review_result.get("success"):
                    reviewer_results.append(review_result)
                    self.logger.info(f"âœ… {role_profile.get('role_name', role_id)} è¯„å®¡å®Œæˆ")
                else:
                    self.logger.warning(f"âš ï¸ {role_profile.get('role_name', role_id)} è¯„å®¡å¤±è´¥: {review_result.get('error')}")
            
            # 6. æ™ºèƒ½æ„è§ç»¼åˆ
            consensus_analysis = self.analyze_reviewer_consensus(reviewer_results)
            
            # 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = self._generate_comprehensive_report(
                reviewer_results, consensus_analysis, document_analysis
            )
            
            # 8. ç”Ÿæˆä¿®å¤æŒ‡å¯¼
            repair_guidance = self.generate_repair_guidance(
                comprehensive_report, document_content
            )
            
            result = {
                "success": True,
                "session_id": f"smart_review_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "document_analysis": document_analysis,
                "selected_roles": selected_roles,
                "role_profiles": role_profiles,
                "reviewer_results": reviewer_results,
                "consensus_analysis": consensus_analysis,
                "comprehensive_report": comprehensive_report,
                "repair_guidance": repair_guidance,
                "review_metadata": {
                    "total_reviewers": len(reviewer_results),
                    "review_duration": "å®æ—¶è®¡ç®—",
                    "consensus_level": consensus_analysis.get("consensus_level", "unknown"),
                    "critical_issues_count": comprehensive_report.get("critical_issues_count", 0),
                    "overall_quality_score": comprehensive_report.get("overall_quality_score", 0)
                }
            }
            
            self.logger.info("ğŸ‰ æ™ºèƒ½æ–‡æ¡£è¯„å®¡å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ™ºèƒ½æ–‡æ¡£è¯„å®¡å¤±è´¥: {e}")
            return {"error": f"æ™ºèƒ½æ–‡æ¡£è¯„å®¡å¤±è´¥: {str(e)}"}
    
    def _execute_single_role_review(self, role_id: str, role_profile: Dict[str, Any],
                                  prompt: str, document_content: str,
                                  document_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªè§’è‰²çš„è¯„å®¡
        
        Args:
            role_id: è§’è‰²ID
            role_profile: è§’è‰²é…ç½®
            prompt: è¯„å®¡æç¤ºè¯
            document_content: æ–‡æ¡£å†…å®¹
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            
        Returns:
            Dict: å•ä¸ªè§’è‰²è¯„å®¡ç»“æœ
        """
        try:
            # å¦‚æœæœ‰LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨AIè¯„å®¡
            if self.llm_client:
                review_text = self.llm_client.generate(prompt)
                review_analysis = self._parse_review_response(review_text, role_id)
            else:
                # å›é€€åˆ°è§„åˆ™è¯„å®¡
                review_analysis = self._rule_based_review(document_content, role_profile, document_analysis)
            
            return {
                "success": True,
                "role_id": role_id,
                "role_name": role_profile.get("role_name", role_id),
                "review_prompt": prompt,
                "review_response": review_analysis.get("review_text", ""),
                "review_analysis": review_analysis,
                "review_timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ è§’è‰² {role_id} è¯„å®¡å¤±è´¥: {e}")
            return {
                "success": False,
                "role_id": role_id,
                "error": str(e)
            }
    
    def _parse_review_response(self, review_text: str, role_id: str) -> Dict[str, Any]:
        """
        è§£æAIè¯„å®¡å“åº”
        
        Args:
            review_text: AIè¯„å®¡æ–‡æœ¬
            role_id: è§’è‰²ID
            
        Returns:
            Dict: è§£æåçš„è¯„å®¡åˆ†æ
        """
        try:
            # æå–é—®é¢˜å’Œå»ºè®®
            issues = self._extract_issues_from_text(review_text)
            recommendations = self._extract_recommendations_from_text(review_text)
            risks = self._extract_risks_from_text(review_text)
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = self._calculate_quality_score(issues, recommendations, risks)
            
            return {
                "review_text": review_text,
                "issues": issues,
                "recommendations": recommendations,
                "risks": risks,
                "quality_score": quality_score,
                "overall_assessment": self._extract_overall_assessment(review_text)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ è§£æè¯„å®¡å“åº”å¤±è´¥: {e}")
            return {
                "review_text": review_text,
                "issues": [],
                "recommendations": [],
                "risks": [],
                "quality_score": 0.0,
                "overall_assessment": "è¯„å®¡è§£æå¤±è´¥"
            }
    
    def _rule_based_review(self, document_content: str, role_profile: Dict[str, Any],
                          document_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        åŸºäºè§„åˆ™çš„è¯„å®¡ï¼ˆLLMä¸å¯ç”¨æ—¶çš„å›é€€æ–¹æ¡ˆï¼‰
        
        Args:
            document_content: æ–‡æ¡£å†…å®¹
            role_profile: è§’è‰²é…ç½®
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            
        Returns:
            Dict: è§„åˆ™è¯„å®¡ç»“æœ
        """
        issues = []
        recommendations = []
        risks = []
        
        # åŸºäºè§’è‰²ç±»å‹è¿›è¡Œè§„åˆ™è¯„å®¡
        role_id = role_profile.get("role_id", "")
        
        if "technical" in role_id:
            # æŠ€æœ¯è¯„å®¡è§„åˆ™
            if len(document_content) < 100:
                issues.append({
                    "severity": "high",
                    "category": "å†…å®¹å®Œæ•´æ€§",
                    "description": "æ–‡æ¡£å†…å®¹è¿‡äºç®€çŸ­ï¼Œç¼ºä¹å¿…è¦çš„æŠ€æœ¯ç»†èŠ‚",
                    "suggestion": "å¢åŠ æŠ€æœ¯å®ç°ç»†èŠ‚å’Œæ¶æ„è¯´æ˜"
                })
            
            if "API" in document_content and "é”™è¯¯å¤„ç†" not in document_content:
                issues.append({
                    "severity": "medium",
                    "category": "æŠ€æœ¯å®Œæ•´æ€§",
                    "description": "APIè®¾è®¡ç¼ºå°‘é”™è¯¯å¤„ç†æœºåˆ¶",
                    "suggestion": "æ·»åŠ å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œå¼‚å¸¸ç®¡ç†"
                })
        
        elif "business" in role_id:
            # å•†ä¸šè¯„å®¡è§„åˆ™
            if "æˆæœ¬" not in document_content and "æ”¶ç›Š" not in document_content:
                issues.append({
                    "severity": "high",
                    "category": "å•†ä¸šåˆ†æ",
                    "description": "ç¼ºå°‘æˆæœ¬æ•ˆç›Šåˆ†æ",
                    "suggestion": "æ·»åŠ è¯¦ç»†çš„æˆæœ¬åˆ†æå’Œæ”¶ç›Šé¢„æµ‹"
                })
        
        elif "legal" in role_id:
            # æ³•å¾‹è¯„å®¡è§„åˆ™
            if "è´£ä»»" not in document_content and "é£é™©" not in document_content:
                issues.append({
                    "severity": "critical",
                    "category": "æ³•å¾‹åˆè§„",
                    "description": "ç¼ºå°‘æ³•å¾‹è´£ä»»å’Œé£é™©æ¡æ¬¾",
                    "suggestion": "æ˜ç¡®å„æ–¹è´£ä»»å’Œé£é™©æ‰¿æ‹…"
                })
        
        # é€šç”¨è§„åˆ™
        if len(document_content.split()) < 50:
            issues.append({
                "severity": "medium",
                "category": "å†…å®¹è´¨é‡",
                "description": "æ–‡æ¡£å†…å®¹ä¸è¶³ï¼Œéœ€è¦è¡¥å……è¯¦ç»†ä¿¡æ¯",
                "suggestion": "å¢åŠ æ›´å¤šå…·ä½“å†…å®¹å’Œç¤ºä¾‹"
            })
        
        return {
            "review_text": f"åŸºäºè§„åˆ™çš„{role_profile.get('role_name', 'ä¸“ä¸š')}è¯„å®¡å®Œæˆ",
            "issues": issues,
            "recommendations": recommendations,
            "risks": risks,
            "quality_score": max(0, 100 - len(issues) * 10),
            "overall_assessment": f"æ–‡æ¡£éœ€è¦{len(issues)}ä¸ªæ–¹é¢çš„æ”¹è¿›"
        }
    
    def analyze_reviewer_consensus(self, reviewer_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†æè¯„å®¡è€…å…±è¯†
        
        Args:
            reviewer_results: è¯„å®¡ç»“æœåˆ—è¡¨
            
        Returns:
            Dict: å…±è¯†åˆ†æç»“æœ
        """
        try:
            if not reviewer_results:
                return {"error": "æ²¡æœ‰è¯„å®¡ç»“æœå¯ä¾›åˆ†æ"}
            
            # æ”¶é›†æ‰€æœ‰é—®é¢˜
            all_issues = []
            reviewer_scores = {}
            
            for result in reviewer_results:
                role_name = result.get("role_name", "Unknown")
                analysis = result.get("review_analysis", {})
                
                # è®°å½•è´¨é‡åˆ†æ•°
                reviewer_scores[role_name] = analysis.get("quality_score", 0)
                
                # æ”¶é›†é—®é¢˜
                issues = analysis.get("issues", [])
                for issue in issues:
                    issue["reviewer"] = role_name
                    all_issues.append(issue)
            
            # åˆ†æå…±è¯†
            consensus_areas = self._find_consensus_areas(all_issues)
            conflict_areas = self._find_conflict_areas(reviewer_results)
            
            # è®¡ç®—å…±è¯†åˆ†æ•°
            consensus_score = self._calculate_consensus_score(reviewer_scores, consensus_areas)
            
            return {
                "total_reviewers": len(reviewer_results),
                "reviewer_scores": reviewer_scores,
                "consensus_areas": consensus_areas,
                "conflict_areas": conflict_areas,
                "consensus_score": consensus_score,
                "consensus_level": self._get_consensus_level(consensus_score),
                "agreement_analysis": self._analyze_agreement_patterns(all_issues)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ å…±è¯†åˆ†æå¤±è´¥: {e}")
            return {"error": f"å…±è¯†åˆ†æå¤±è´¥: {str(e)}"}
    
    def generate_repair_guidance(self, comprehensive_report: Dict[str, Any],
                               document_content: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆä¿®å¤æŒ‡å¯¼
        
        Args:
            comprehensive_report: ç»¼åˆè¯„å®¡æŠ¥å‘Š
            document_content: åŸæ–‡æ¡£å†…å®¹
            
        Returns:
            Dict: ä¿®å¤æŒ‡å¯¼
        """
        try:
            # æå–ä¼˜å…ˆçº§é—®é¢˜
            critical_issues = comprehensive_report.get("critical_issues", [])
            high_issues = comprehensive_report.get("high_issues", [])
            
            # ç”Ÿæˆä¿®å¤å»ºè®®
            repair_suggestions = []
            for issue in critical_issues + high_issues:
                repair_suggestions.append({
                    "issue": issue.get("description", ""),
                    "priority": issue.get("severity", "medium"),
                    "suggestion": issue.get("suggestion", ""),
                    "estimated_effort": self._estimate_repair_effort(issue.get("severity", "medium"))
                })
            
            # ç”Ÿæˆä¿®æ”¹åçš„æ–‡æ¡£å¤§çº²
            modified_outline = self._generate_modified_outline(document_content, repair_suggestions)
            
            return {
                "repair_suggestions": repair_suggestions,
                "modified_outline": modified_outline,
                "priority_ranking": self._rank_repair_priorities(repair_suggestions),
                "estimated_total_effort": self._calculate_total_effort(repair_suggestions),
                "repair_timeline": self._generate_repair_timeline(repair_suggestions)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆä¿®å¤æŒ‡å¯¼å¤±è´¥: {e}")
            return {"error": f"ç”Ÿæˆä¿®å¤æŒ‡å¯¼å¤±è´¥: {str(e)}"}
    
    # è¾…åŠ©æ–¹æ³•
    def _analyze_document_characteristics(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£ç‰¹å¾"""
        words = content.split()
        sentences = content.split('.')
        
        return {
            "document_type": self._classify_document_type(content),
            "complexity_level": self._assess_complexity(content),
            "target_audience": self._identify_target_audience(content),
            "word_count": len(words),
            "sentence_count": len(sentences),
            "avg_sentence_length": len(words) / max(len(sentences), 1),
            "formality_level": self._assess_formality(content)
        }
    
    def _classify_document_type(self, content: str) -> str:
        """åˆ†ç±»æ–‡æ¡£ç±»å‹"""
        content_lower = content.lower()
        
        if any(term in content_lower for term in ["æŠ€æœ¯", "ç³»ç»Ÿ", "æ¶æ„", "API", "æ•°æ®åº“"]):
            return "technical_report"
        elif any(term in content_lower for term in ["å¸‚åœº", "å•†ä¸š", "å®¢æˆ·", "æ”¶ç›Š", "æˆ˜ç•¥"]):
            return "business_proposal"
        elif any(term in content_lower for term in ["åˆåŒ", "æ¡æ¬¾", "è´£ä»»", "æ³•å¾‹", "åˆè§„"]):
            return "legal_document"
        elif any(term in content_lower for term in ["æ”¿åºœ", "æ”¿ç­–", "æ³•è§„", "é€šçŸ¥", "å†³å®š"]):
            return "government_document"
        elif any(term in content_lower for term in ["ç ”ç©¶", "å®éªŒ", "æ–¹æ³•", "ç»“è®º", "å¼•ç”¨"]):
            return "academic_paper"
        else:
            return "general_document"
    
    def _assess_complexity(self, content: str) -> str:
        """è¯„ä¼°å¤æ‚åº¦"""
        word_count = len(content.split())
        if word_count > 1000:
            return "high"
        elif word_count > 500:
            return "medium"
        else:
            return "low"
    
    def _identify_target_audience(self, content: str) -> str:
        """è¯†åˆ«ç›®æ ‡å—ä¼—"""
        content_lower = content.lower()
        
        if any(term in content_lower for term in ["æŠ€æœ¯å›¢é˜Ÿ", "å¼€å‘", "å·¥ç¨‹å¸ˆ"]):
            return "technical_team"
        elif any(term in content_lower for term in ["ç®¡ç†å±‚", "é¢†å¯¼", "å†³ç­–"]):
            return "management"
        elif any(term in content_lower for term in ["å®¢æˆ·", "ç”¨æˆ·", "æ¶ˆè´¹è€…"]):
            return "customers"
        else:
            return "general_audience"
    
    def _assess_formality(self, content: str) -> str:
        """è¯„ä¼°æ­£å¼ç¨‹åº¦"""
        informal_words = ["æˆ‘è§‰å¾—", "æŒºå¥½çš„", "åº”è¯¥å¯ä»¥"]
        informal_count = sum(1 for word in informal_words if word in content)
        
        if informal_count > 2:
            return "informal"
        elif informal_count > 0:
            return "semi_formal"
        else:
            return "formal"
    
    def _extract_issues_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–é—®é¢˜"""
        issues = []
        # ç®€åŒ–çš„è§„åˆ™æå–
        if "é—®é¢˜" in text or "issue" in text.lower():
            issues.append({
                "severity": "medium",
                "category": "å†…å®¹è´¨é‡",
                "description": "å‘ç°å†…å®¹é—®é¢˜",
                "suggestion": "éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›"
            })
        return issues
    
    def _extract_recommendations_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å»ºè®®"""
        recommendations = []
        # ç®€åŒ–çš„è§„åˆ™æå–
        if "å»ºè®®" in text or "recommend" in text.lower():
            recommendations.append("æ ¹æ®è¯„å®¡æ„è§è¿›è¡Œæ”¹è¿›")
        return recommendations
    
    def _extract_risks_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–é£é™©"""
        risks = []
        # ç®€åŒ–çš„è§„åˆ™æå–
        if "é£é™©" in text or "risk" in text.lower():
            risks.append({
                "risk_type": "general",
                "severity": "medium",
                "description": "å­˜åœ¨æ½œåœ¨é£é™©",
                "mitigation": "éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°"
            })
        return risks
    
    def _calculate_quality_score(self, issues: List[Dict], recommendations: List[str], risks: List[Dict]) -> float:
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        base_score = 100.0
        
        # æ ¹æ®é—®é¢˜æ•°é‡æ‰£åˆ†
        for issue in issues:
            severity = issue.get("severity", "medium")
            if severity == "critical":
                base_score -= 20
            elif severity == "high":
                base_score -= 10
            elif severity == "medium":
                base_score -= 5
            else:
                base_score -= 2
        
        return max(0.0, base_score)
    
    def _extract_overall_assessment(self, text: str) -> str:
        """æå–æ€»ä½“è¯„ä»·"""
        if "ä¼˜ç§€" in text or "excellent" in text.lower():
            return "ä¼˜ç§€"
        elif "è‰¯å¥½" in text or "good" in text.lower():
            return "è‰¯å¥½"
        elif "ä¸€èˆ¬" in text or "fair" in text.lower():
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦æ”¹è¿›"
    
    def _find_consensus_areas(self, all_issues: List[Dict]) -> Dict[str, List[Dict]]:
        """æ‰¾åˆ°å…±è¯†é¢†åŸŸ"""
        consensus_areas = {}
        
        # æŒ‰é—®é¢˜ç±»åˆ«åˆ†ç»„
        for issue in all_issues:
            category = issue.get("category", "general")
            if category not in consensus_areas:
                consensus_areas[category] = []
            consensus_areas[category].append(issue)
        
        # è¿‡æ»¤å‡ºå¤šä¸ªè¯„å®¡è€…éƒ½æåˆ°çš„é—®é¢˜
        return {k: v for k, v in consensus_areas.items() if len(v) > 1}
    
    def _find_conflict_areas(self, reviewer_results: List[Dict]) -> List[Dict]:
        """æ‰¾åˆ°å†²çªé¢†åŸŸ"""
        # ç®€åŒ–çš„å†²çªæ£€æµ‹
        return []
    
    def _calculate_consensus_score(self, reviewer_scores: Dict[str, float], consensus_areas: Dict) -> float:
        """è®¡ç®—å…±è¯†åˆ†æ•°"""
        if not reviewer_scores:
            return 0.0
        
        # åŸºäºåˆ†æ•°å·®å¼‚å’Œå…±è¯†é¢†åŸŸè®¡ç®—
        scores = list(reviewer_scores.values())
        score_variance = max(scores) - min(scores) if len(scores) > 1 else 0
        consensus_bonus = len(consensus_areas) * 10
        
        return min(100.0, 100 - score_variance + consensus_bonus)
    
    def _get_consensus_level(self, consensus_score: float) -> str:
        """è·å–å…±è¯†ç­‰çº§"""
        if consensus_score >= 80:
            return "high"
        elif consensus_score >= 60:
            return "medium"
        else:
            return "low"
    
    def _analyze_agreement_patterns(self, all_issues: List[Dict]) -> Dict[str, Any]:
        """åˆ†æåŒæ„æ¨¡å¼"""
        return {
            "total_issues": len(all_issues),
            "severity_distribution": self._get_severity_distribution(all_issues),
            "category_distribution": self._get_category_distribution(all_issues)
        }
    
    def _get_severity_distribution(self, issues: List[Dict]) -> Dict[str, int]:
        """è·å–ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ"""
        distribution = {"critical": 0, "high": 0, "medium": 0, "low": 0}
        for issue in issues:
            severity = issue.get("severity", "medium")
            distribution[severity] = distribution.get(severity, 0) + 1
        return distribution
    
    def _get_category_distribution(self, issues: List[Dict]) -> Dict[str, int]:
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        distribution = {}
        for issue in issues:
            category = issue.get("category", "general")
            distribution[category] = distribution.get(category, 0) + 1
        return distribution
    
    def _generate_comprehensive_report(self, reviewer_results: List[Dict], 
                                     consensus_analysis: Dict, 
                                     document_analysis: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        # æ”¶é›†æ‰€æœ‰é—®é¢˜
        all_issues = []
        for result in reviewer_results:
            analysis = result.get("review_analysis", {})
            issues = analysis.get("issues", [])
            all_issues.extend(issues)
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        critical_issues = [i for i in all_issues if i.get("severity") == "critical"]
        high_issues = [i for i in all_issues if i.get("severity") == "high"]
        medium_issues = [i for i in all_issues if i.get("severity") == "medium"]
        low_issues = [i for i in all_issues if i.get("severity") == "low"]
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        total_score = sum(result.get("review_analysis", {}).get("quality_score", 0) for result in reviewer_results)
        avg_score = total_score / len(reviewer_results) if reviewer_results else 0
        
        return {
            "critical_issues": critical_issues,
            "high_issues": high_issues,
            "medium_issues": medium_issues,
            "low_issues": low_issues,
            "critical_issues_count": len(critical_issues),
            "high_issues_count": len(high_issues),
            "medium_issues_count": len(medium_issues),
            "low_issues_count": len(low_issues),
            "overall_quality_score": avg_score,
            "consensus_level": consensus_analysis.get("consensus_level", "unknown"),
            "document_analysis": document_analysis
        }
    
    def _estimate_repair_effort(self, severity: str) -> str:
        """ä¼°ç®—ä¿®å¤å·¥ä½œé‡"""
        effort_map = {
            "critical": "é«˜å·¥ä½œé‡",
            "high": "ä¸­ç­‰å·¥ä½œé‡", 
            "medium": "ä½å·¥ä½œé‡",
            "low": "è½»å¾®å·¥ä½œé‡"
        }
        return effort_map.get(severity, "æœªçŸ¥")
    
    def _generate_modified_outline(self, content: str, repair_suggestions: List[Dict]) -> List[str]:
        """ç”Ÿæˆä¿®æ”¹åçš„å¤§çº²"""
        outline = ["æ–‡æ¡£ä¿®æ”¹å»ºè®®å¤§çº²:"]
        
        for i, suggestion in enumerate(repair_suggestions, 1):
            outline.append(f"{i}. {suggestion['issue']} - {suggestion['suggestion']}")
        
        return outline
    
    def _rank_repair_priorities(self, repair_suggestions: List[Dict]) -> List[Dict]:
        """æ’åºä¿®å¤ä¼˜å…ˆçº§"""
        return sorted(repair_suggestions, 
                     key=lambda x: {"critical": 4, "high": 3, "medium": 2, "low": 1}.get(x.get("priority", "medium"), 0),
                     reverse=True)
    
    def _calculate_total_effort(self, repair_suggestions: List[Dict]) -> str:
        """è®¡ç®—æ€»å·¥ä½œé‡"""
        critical_count = len([s for s in repair_suggestions if s.get("priority") == "critical"])
        high_count = len([s for s in repair_suggestions if s.get("priority") == "high"])
        
        if critical_count > 0:
            return "é«˜å·¥ä½œé‡"
        elif high_count > 2:
            return "ä¸­ç­‰å·¥ä½œé‡"
        else:
            return "ä½å·¥ä½œé‡"
    
    def _generate_repair_timeline(self, repair_suggestions: List[Dict]) -> List[str]:
        """ç”Ÿæˆä¿®å¤æ—¶é—´çº¿"""
        timeline = []
        
        critical_issues = [s for s in repair_suggestions if s.get("priority") == "critical"]
        high_issues = [s for s in repair_suggestions if s.get("priority") == "high"]
        
        if critical_issues:
            timeline.append("ç«‹å³å¤„ç†: ä¸¥é‡é—®é¢˜éœ€è¦ä¼˜å…ˆè§£å†³")
        if high_issues:
            timeline.append("æœ¬å‘¨å†…: é‡è¦é—®é¢˜éœ€è¦åŠæ—¶å¤„ç†")
        if len(repair_suggestions) > len(critical_issues) + len(high_issues):
            timeline.append("ä¸‹å‘¨å†…: å…¶ä»–é—®é¢˜å¯ä»¥é€æ­¥æ”¹è¿›")
        
        return timeline 