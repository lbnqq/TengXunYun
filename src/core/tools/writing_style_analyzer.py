#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Writing Style Analyzer - æ ¸å¿ƒæ¨¡å—

Author: AI Assistant (Claude)
Created: 2025-01-28
Last Modified: 2025-01-28
Modified By: AI Assistant (Claude)
AI Assisted: æ˜¯ - Claude 3.5 Sonnet
Version: v1.0
License: MIT
"""

import re
import json
import os
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime
import hashlib
import difflib

# å¯¼å…¥å¢å¼ºçš„æ–‡é£åˆ†æç»„ä»¶
try:
    from .comprehensive_style_processor import ComprehensiveStyleProcessor
    from .enhanced_style_extractor import EnhancedStyleExtractor
    from .style_alignment_engine import StyleAlignmentEngine
    ENHANCED_FEATURES_AVAILABLE = True
except ImportError:
    ENHANCED_FEATURES_AVAILABLE = False
    print("Warning: Enhanced style analysis features not available. Using basic functionality.")

class WritingStyleAnalyzer:

    def __init__(self, storage_path: str = "src/core/knowledge_base/writing_style_templates", llm_client=None):
        self.tool_name = "æ–‡é£åˆ†æå™¨"
        self.description = "åˆ†ææ–‡æ¡£å†™ä½œé£æ ¼ï¼Œç”Ÿæˆæ–‡é£æ¨¡æ¿ï¼Œæ”¯æŒæ–‡é£å¯¹é½å’Œæ¶¦è‰²åŠŸèƒ½"
        self.storage_path = storage_path
        self.llm_client = llm_client

        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        os.makedirs(storage_path, exist_ok=True)

        # åˆå§‹åŒ–å¢å¼ºåŠŸèƒ½ç»„ä»¶
        if ENHANCED_FEATURES_AVAILABLE and llm_client:
            self.enhanced_processor = ComprehensiveStyleProcessor(
                llm_client=llm_client,
                storage_path=os.path.join(storage_path, "enhanced_analysis")
            )
            self.use_enhanced_features = True
            print("âœ… å¢å¼ºæ–‡é£åˆ†æåŠŸèƒ½å·²å¯ç”¨")
        else:
            self.enhanced_processor = None
            self.use_enhanced_features = False
            print("âš ï¸ ä½¿ç”¨åŸºç¡€æ–‡é£åˆ†æåŠŸèƒ½")
        
        # æ–‡é£ç‰¹å¾åˆ†æç»´åº¦
        self.style_dimensions = {
            "sentence_structure": {
                "name": "å¥å¼ç»“æ„",
                "features": ["å¹³å‡å¥é•¿", "é•¿çŸ­å¥æ¯”ä¾‹", "å¤åˆå¥ä½¿ç”¨", "å¹¶åˆ—å¥ä½¿ç”¨"]
            },
            "vocabulary_choice": {
                "name": "è¯æ±‡é€‰æ‹©", 
                "features": ["æ­£å¼ç¨‹åº¦", "ä¸“ä¸šæœ¯è¯­", "ä¿®é¥°è¯ä½¿ç”¨", "åŠ¨è¯ç±»å‹åå¥½"]
            },
            "expression_style": {
                "name": "è¡¨è¾¾æ–¹å¼",
                "features": ["ä¸»è¢«åŠ¨è¯­æ€", "äººç§°ä½¿ç”¨", "è¯­æ°”å¼ºåº¦", "æƒ…æ„Ÿè‰²å½©"]
            },
            "text_organization": {
                "name": "æ–‡æœ¬ç»„ç»‡",
                "features": ["æ®µè½ç»“æ„", "é€»è¾‘è¿æ¥", "è¿‡æ¸¡æ–¹å¼", "æ€»ç»“ä¹ æƒ¯"]
            },
            "language_habits": {
                "name": "è¯­è¨€ä¹ æƒ¯",
                "features": ["å£è¯­åŒ–ç¨‹åº¦", "ä¹¦é¢è¯­è§„èŒƒ", "åœ°åŸŸç‰¹è‰²", "è¡Œä¸šç‰¹è‰²"]
            }
        }
        
        # å¸¸è§æ–‡é£ç±»å‹
        self.style_types = {
            "formal_official": {
                "name": "æ­£å¼å…¬æ–‡é£æ ¼",
                "characteristics": ["ä¸¥è°¨è§„èŒƒ", "ç”¨è¯å‡†ç¡®", "é€»è¾‘æ¸…æ™°", "æ ¼å¼æ ‡å‡†"],
                "typical_patterns": ["æ ¹æ®", "æŒ‰ç…§", "ç°å°†", "ç‰¹æ­¤", "åŠ¡å¿…"]
            },
            "business_professional": {
                "name": "å•†åŠ¡ä¸“ä¸šé£æ ¼", 
                "characteristics": ["ç®€æ´æ˜äº†", "é‡ç‚¹çªå‡º", "æ•°æ®å¯¼å‘", "ç»“æœå¯¼å‘"],
                "typical_patterns": ["æå‡", "ä¼˜åŒ–", "å®ç°", "è¾¾æˆ", "æ¨è¿›"]
            },
            "academic_research": {
                "name": "å­¦æœ¯ç ”ç©¶é£æ ¼",
                "characteristics": ["å®¢è§‚ä¸¥è°¨", "é€»è¾‘ä¸¥å¯†", "è®ºè¯å……åˆ†", "å¼•ç”¨è§„èŒƒ"],
                "typical_patterns": ["ç ”ç©¶è¡¨æ˜", "åˆ†æå‘ç°", "ç»¼åˆè€ƒè™‘", "æ·±å…¥æ¢è®¨"]
            },
            "narrative_descriptive": {
                "name": "å™è¿°æè¿°é£æ ¼",
                "characteristics": ["ç”ŸåŠ¨å½¢è±¡", "ç»†èŠ‚ä¸°å¯Œ", "æƒ…æ„ŸçœŸå®", "æ•…äº‹æ€§å¼º"],
                "typical_patterns": ["ç”ŸåŠ¨åœ°", "è¯¦ç»†åœ°", "æ·±åˆ»åœ°", "çœŸå®åœ°"]
            },
            "concise_practical": {
                "name": "ç®€æ´å®ç”¨é£æ ¼",
                "characteristics": ["è¨€ç®€æ„èµ…", "ç›´æ¥æœ‰æ•ˆ", "æ“ä½œæ€§å¼º", "æ˜“äºç†è§£"],
                "typical_patterns": ["ç›´æ¥", "ç«‹å³", "é©¬ä¸Š", "ç®€å•", "å¿«é€Ÿ"]
            }
        }
    
        # åˆå§‹åŒ–é£æ ¼è¡Œä¸ºç¼“å­˜/å¯¼å‡ºç›®å½•
        self.semantic_behavior_dir = os.path.join(self.storage_path, "semantic_behavior")
        os.makedirs(os.path.join(self.semantic_behavior_dir, "profiles"), exist_ok=True)

    def analyze_writing_style(self, document_content: str, document_name: str = None,
                             use_enhanced: bool = None) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£çš„å†™ä½œé£æ ¼

        Args:
            document_content: æ–‡æ¡£å†…å®¹
            document_name: æ–‡æ¡£åç§°
            use_enhanced: æ˜¯å¦ä½¿ç”¨å¢å¼ºåŠŸèƒ½ï¼ˆNoneæ—¶è‡ªåŠ¨åˆ¤æ–­ï¼‰

        Returns:
            æ–‡é£åˆ†æç»“æœ
        """
        # å†³å®šæ˜¯å¦ä½¿ç”¨å¢å¼ºåŠŸèƒ½
        if use_enhanced is None:
            use_enhanced = self.use_enhanced_features

        if use_enhanced and self.enhanced_processor:
            return self._analyze_with_enhanced_features(document_content, document_name)
        else:
            return self._analyze_with_basic_features(document_content, document_name)

    def _analyze_with_enhanced_features(self, document_content: str, document_name: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨å¢å¼ºåŠŸèƒ½è¿›è¡Œæ–‡é£åˆ†æ"""
        try:
            print(f"ğŸ” ä½¿ç”¨å¢å¼ºåŠŸèƒ½åˆ†ææ–‡æ¡£: {document_name or 'æœªå‘½åæ–‡æ¡£'}")

            # ä½¿ç”¨ç»¼åˆæ–‡é£å¤„ç†å™¨è¿›è¡Œåˆ†æ
            enhanced_result = self.enhanced_processor.extract_comprehensive_style_features(
                document_content, document_name, include_advanced_analysis=True
            )

            # è½¬æ¢ä¸ºå…¼å®¹çš„æ ¼å¼
            analysis_result = {
                "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
                "analysis_time": datetime.now().isoformat(),
                "analysis_method": "enhanced",
                "document_stats": self._get_document_statistics(document_content),
                "enhanced_analysis": enhanced_result,
                "style_features": self._extract_style_features_from_enhanced(enhanced_result),
                "style_type": self._determine_style_type_from_enhanced(enhanced_result),
                "confidence_score": self._calculate_confidence_from_enhanced(enhanced_result),
                "style_prompt": self._generate_style_prompt_from_enhanced(enhanced_result),
                "template_id": self._generate_template_id(document_name, {}),
                "detailed_analysis": enhanced_result.get("advanced_features", {}),
                "writing_recommendations": self._generate_recommendations_from_enhanced(enhanced_result),
                "style_comparison": {}
            }

            print("âœ… å¢å¼ºæ–‡é£åˆ†æå®Œæˆ")
            return analysis_result

        except Exception as e:
            print(f"âŒ å¢å¼ºåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€åˆ†æ: {str(e)}")
            return self._analyze_with_basic_features(document_content, document_name)

    def _analyze_with_basic_features(self, document_content: str, document_name: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨åŸºç¡€åŠŸèƒ½è¿›è¡Œæ–‡é£åˆ†æ"""
        try:
            analysis_result = {
                "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
                "analysis_time": datetime.now().isoformat(),
                "analysis_method": "basic",
                "document_stats": self._get_document_statistics(document_content),
                "style_features": {},
                "style_type": None,
                "confidence_score": 0.0,
                "style_prompt": "",
                "template_id": None,
                "detailed_analysis": {},
                "writing_recommendations": [],
                "style_comparison": {}
            }

            # åˆ†æå„ä¸ªç»´åº¦çš„æ–‡é£ç‰¹å¾
            style_features = self._analyze_style_features(document_content)
            analysis_result["style_features"] = style_features

            # è¯†åˆ«æ–‡é£ç±»å‹
            style_type, confidence = self._identify_style_type(document_content, style_features)
            analysis_result["style_type"] = style_type
            analysis_result["confidence_score"] = confidence

            # ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
            detailed_analysis = self._generate_detailed_analysis(document_content, style_features, style_type)
            analysis_result["detailed_analysis"] = detailed_analysis

            # ç”Ÿæˆå†™ä½œå»ºè®®
            recommendations = self._generate_writing_recommendations(style_features, style_type)
            analysis_result["writing_recommendations"] = recommendations

            # ç”Ÿæˆé£æ ¼å¯¹æ¯”
            style_comparison = self._generate_style_comparison(style_features)
            analysis_result["style_comparison"] = style_comparison

            # ç”Ÿæˆæ–‡é£æç¤ºè¯
            style_prompt = self._generate_enhanced_style_prompt(style_features, style_type, detailed_analysis)
            analysis_result["style_prompt"] = style_prompt

            # ç”Ÿæˆæ¨¡æ¿ID
            template_id = self._generate_template_id(document_name, style_features)
            analysis_result["template_id"] = template_id

            return analysis_result

        except Exception as e:
            return {"error": f"æ–‡é£åˆ†æå¤±è´¥: {str(e)}"}

    def _extract_style_features_from_enhanced(self, enhanced_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­æå–é£æ ¼ç‰¹å¾"""
        style_features = {}

        try:
            basic_features = enhanced_result.get("basic_features", {})

            # æå–é‡åŒ–ç‰¹å¾
            quant_features = basic_features.get("quantitative_features", {})
            if quant_features:
                lexical = quant_features.get("lexical_features", {})
                syntactic = quant_features.get("syntactic_features", {})

                style_features["lexical_richness"] = lexical.get("ttr", 0)
                style_features["avg_word_length"] = lexical.get("avg_word_length", 0)
                style_features["formal_density"] = lexical.get("formal_word_density", 0)
                style_features["avg_sentence_length"] = syntactic.get("avg_sentence_length", 0)
                style_features["sentence_variety"] = syntactic.get("sentence_length_std", 0)

            # æå–LLMç‰¹å¾
            llm_features = basic_features.get("llm_features", {})
            if llm_features:
                evaluations = llm_features.get("evaluations", {})
                for dimension, eval_data in evaluations.items():
                    if isinstance(eval_data, dict) and "score" in eval_data:
                        style_features[f"llm_{dimension}"] = eval_data["score"]

        except Exception as e:
            style_features["extraction_error"] = str(e)

        return style_features

    def _determine_style_type_from_enhanced(self, enhanced_result: Dict[str, Any]) -> str:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­ç¡®å®šé£æ ¼ç±»å‹"""
        try:
            advanced_features = enhanced_result.get("advanced_features", {})
            comprehensive_analysis = advanced_features.get("comprehensive_analysis", {})

            if comprehensive_analysis.get("success"):
                parsed_analysis = comprehensive_analysis.get("parsed_analysis", {})
                overall_style = parsed_analysis.get("overall_style", {})

                # ä»LLMåˆ†æä¸­æå–é£æ ¼ç±»å‹
                style_type = overall_style.get("ä¸»è¦é£æ ¼ç±»å‹", "")
                if style_type:
                    # æ˜ å°„åˆ°å†…éƒ¨é£æ ¼ç±»å‹
                    style_mapping = {
                        "æ­£å¼å…¬æ–‡": "formal_official",
                        "å•†åŠ¡ä¸“ä¸š": "business_professional",
                        "å­¦æœ¯ç ”ç©¶": "academic_research",
                        "å™è¿°æè¿°": "narrative_descriptive",
                        "ç®€æ´å®ç”¨": "concise_practical"
                    }
                    return style_mapping.get(style_type, "business_professional")

            # å›é€€åˆ°åŸºäºç‰¹å¾çš„åˆ¤æ–­
            style_features = self._extract_style_features_from_enhanced(enhanced_result)
            formal_score = style_features.get("llm_æ­£å¼ç¨‹åº¦", 3.0)

            if formal_score >= 4.0:
                return "formal_official"
            elif formal_score >= 3.5:
                return "business_professional"
            else:
                return "concise_practical"

        except Exception:
            return "business_professional"

    def _calculate_confidence_from_enhanced(self, enhanced_result: Dict[str, Any]) -> float:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­è®¡ç®—ç½®ä¿¡åº¦"""
        try:
            basic_features = enhanced_result.get("basic_features", {})

            # åŸºäºæˆåŠŸæå–çš„ç‰¹å¾æ•°é‡è®¡ç®—ç½®ä¿¡åº¦
            feature_vector = basic_features.get("feature_vector", [])
            if feature_vector:
                base_confidence = min(len(feature_vector) / 20.0, 1.0)  # å‡è®¾20ä¸ªç‰¹å¾ä¸ºæ»¡åˆ†
            else:
                base_confidence = 0.5

            # å¦‚æœæœ‰LLMåˆ†æï¼Œæé«˜ç½®ä¿¡åº¦
            llm_features = basic_features.get("llm_features", {})
            if llm_features.get("evaluations"):
                base_confidence = min(base_confidence + 0.2, 1.0)

            # å¦‚æœæœ‰é«˜çº§åˆ†æï¼Œè¿›ä¸€æ­¥æé«˜ç½®ä¿¡åº¦
            advanced_features = enhanced_result.get("advanced_features", {})
            if advanced_features:
                base_confidence = min(base_confidence + 0.1, 1.0)

            return round(base_confidence, 3)

        except Exception:
            return 0.7  # é»˜è®¤ç½®ä¿¡åº¦

    def _generate_style_prompt_from_enhanced(self, enhanced_result: Dict[str, Any]) -> str:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­ç”Ÿæˆé£æ ¼æç¤ºè¯"""
        try:
            style_features = self._extract_style_features_from_enhanced(enhanced_result)
            style_type = self._determine_style_type_from_enhanced(enhanced_result)

            # è·å–é£æ ¼ç±»å‹ä¿¡æ¯
            style_info = self.style_types.get(style_type, {})
            style_name = style_info.get("name", "æ ‡å‡†é£æ ¼")
            characteristics = style_info.get("characteristics", [])

            # æ„å»ºæç¤ºè¯
            prompt_parts = [f"è¯·æŒ‰ç…§{style_name}è¿›è¡Œå†™ä½œ"]

            if characteristics:
                prompt_parts.append(f"ç‰¹ç‚¹ï¼š{', '.join(characteristics)}")

            # æ·»åŠ å…·ä½“çš„é£æ ¼æŒ‡å¯¼
            if style_features.get("formal_density", 0) > 10:
                prompt_parts.append("ä½¿ç”¨æ­£å¼è¯æ±‡å’Œè¡¨è¾¾")

            if style_features.get("avg_sentence_length", 0) > 15:
                prompt_parts.append("é‡‡ç”¨è¾ƒé•¿çš„å¤åˆå¥ç»“æ„")
            elif style_features.get("avg_sentence_length", 0) < 10:
                prompt_parts.append("ä½¿ç”¨ç®€æ´æ˜äº†çš„çŸ­å¥")

            return "ï¼›".join(prompt_parts)

        except Exception:
            return "è¯·ä¿æŒåŸæœ‰çš„å†™ä½œé£æ ¼"

    def _generate_recommendations_from_enhanced(self, enhanced_result: Dict[str, Any]) -> List[str]:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­ç”Ÿæˆå†™ä½œå»ºè®®"""
        recommendations = []

        try:
            style_features = self._extract_style_features_from_enhanced(enhanced_result)

            # åŸºäºç‰¹å¾ç»™å‡ºå»ºè®®
            if style_features.get("lexical_richness", 0) < 0.5:
                recommendations.append("å»ºè®®å¢åŠ è¯æ±‡å¤šæ ·æ€§ï¼Œé¿å…é‡å¤ä½¿ç”¨ç›¸åŒè¯æ±‡")

            if style_features.get("sentence_variety", 0) < 3:
                recommendations.append("å»ºè®®å¢åŠ å¥å¼å˜åŒ–ï¼Œä½¿ç”¨é•¿çŸ­å¥ç»“åˆçš„æ–¹å¼")

            if style_features.get("formal_density", 0) < 5:
                recommendations.append("å¦‚éœ€æé«˜æ­£å¼ç¨‹åº¦ï¼Œå¯å¢åŠ æ­£å¼è¯æ±‡çš„ä½¿ç”¨")

            # ä»LLMåˆ†æä¸­æå–å»ºè®®
            advanced_features = enhanced_result.get("advanced_features", {})
            comprehensive_analysis = advanced_features.get("comprehensive_analysis", {})

            if comprehensive_analysis.get("success"):
                parsed_analysis = comprehensive_analysis.get("parsed_analysis", {})
                style_summary = parsed_analysis.get("style_summary", {})

                improvement_suggestions = style_summary.get("æ”¹è¿›å»ºè®®", "")
                if improvement_suggestions and improvement_suggestions != "æ— ":
                    recommendations.append(improvement_suggestions)

            return recommendations[:5]  # æœ€å¤šè¿”å›5æ¡å»ºè®®

        except Exception:
            return ["å»ºè®®ä¿æŒå½“å‰å†™ä½œé£æ ¼çš„ä¸€è‡´æ€§"]

    def analyze_with_semantic_behavior(self, document_content: str, document_name: str = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•è¿›è¡Œæ–‡é£åˆ†æ

        Args:
            document_content: æ–‡æ¡£å†…å®¹
            document_name: æ–‡æ¡£åç§°

        Returns:
            è¯­ä¹‰è¡Œä¸ºåˆ†æç»“æœ
        """
        if not self.use_enhanced_features or not self.enhanced_processor:
            return {"error": "å¢å¼ºåŠŸèƒ½æœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰è¡Œä¸ºåˆ†æ"}

        try:
            print(f"ğŸ§  å¼€å§‹è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ: {document_name or 'æœªå‘½åæ–‡æ¡£'}")

            # ä½¿ç”¨ç»¼åˆå¤„ç†å™¨çš„è¯­ä¹‰åˆ†æåŠŸèƒ½
            semantic_result = self.enhanced_processor.analyze_semantic_behavior(
                document_content, document_name, "comprehensive"
            )

            if semantic_result.get("success"):
                # è½¬æ¢ä¸ºå…¼å®¹çš„æ ¼å¼
                analysis_result = {
                    "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
                    "analysis_time": datetime.now().isoformat(),
                    "analysis_method": "semantic_behavior",
                    "semantic_analysis": semantic_result,
                    "style_features": self._extract_semantic_style_features(semantic_result),
                    "style_type": self._determine_semantic_style_type(semantic_result),
                    "confidence_score": self._calculate_semantic_confidence(semantic_result),
                    "style_prompt": self._generate_semantic_style_prompt(semantic_result),
                    "template_id": self._generate_template_id(document_name, {}),
                    "detailed_analysis": semantic_result.get("comprehensive_insights", {}),
                    "writing_recommendations": self._generate_semantic_recommendations(semantic_result),
                    "style_comparison": {}
                }

                print("âœ… è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå®Œæˆ")
                return analysis_result
            else:
                return {"error": f"è¯­ä¹‰åˆ†æå¤±è´¥: {semantic_result.get('error', 'æœªçŸ¥é”™è¯¯')}"}

        except Exception as e:
            return {"error": f"è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå¤±è´¥: {str(e)}"}

    def _extract_semantic_style_features(self, semantic_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»è¯­ä¹‰åˆ†æç»“æœä¸­æå–é£æ ¼ç‰¹å¾"""
        style_features = {}

        try:
            # ä»æœ€ç»ˆç”»åƒä¸­æå–ç‰¹å¾
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                style_scores = final_profile.get("style_scores", {})

                # æ˜ å°„åˆ°ä¼ ç»Ÿç‰¹å¾åç§°
                style_features.update({
                    "conceptual_organization": style_scores.get("conceptual_organization", 3.0),
                    "semantic_coherence": style_scores.get("semantic_coherence", 3.0),
                    "creative_association": style_scores.get("creative_association", 3.0),
                    "emotional_expression": style_scores.get("emotional_expression", 3.0),
                    "cognitive_complexity": style_scores.get("cognitive_complexity", 3.0),
                    "thematic_focus": style_scores.get("thematic_focus", 3.0)
                })

                # æ·»åŠ ç‰¹å¾å‘é‡é•¿åº¦
                feature_vector = final_profile.get("feature_vector", [])
                style_features["feature_vector_length"] = len(feature_vector)
                style_features["feature_vector_norm"] = final_profile.get("comparative_metrics", {}).get("feature_vector_norm", 0.0)

        except Exception as e:
            style_features["extraction_error"] = str(e)

        return style_features

    def _determine_semantic_style_type(self, semantic_result: Dict[str, Any]) -> str:
        """ä»è¯­ä¹‰åˆ†æç»“æœä¸­ç¡®å®šé£æ ¼ç±»å‹"""
        try:
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                classification = final_profile.get("style_classification", {})
                primary_style = classification.get("primary_style", "")

                # æ˜ å°„åˆ°å†…éƒ¨é£æ ¼ç±»å‹
                style_mapping = {
                    "ç³»ç»Ÿæ€§æ€ç»´å‹": "academic_research",
                    "é€»è¾‘è¿è´¯å‹": "business_professional",
                    "åˆ›æ–°è”æƒ³å‹": "creative_narrative",
                    "æƒ…æ„Ÿè¡¨è¾¾å‹": "narrative_descriptive",
                    "å¤æ‚æ€ç»´å‹": "academic_research",
                    "ä¸“æ³¨èšç„¦å‹": "formal_official"
                }

                return style_mapping.get(primary_style, "business_professional")

            return "business_professional"

        except Exception:
            return "business_professional"

    def _calculate_semantic_confidence(self, semantic_result: Dict[str, Any]) -> float:
        """è®¡ç®—è¯­ä¹‰åˆ†æçš„ç½®ä¿¡åº¦"""
        try:
            # åŸºäºåˆ†ææˆåŠŸçš„é˜¶æ®µæ•°é‡
            analysis_summary = semantic_result.get("semantic_analysis", {}).get("analysis_summary", {})
            stages_completed = analysis_summary.get("stages_completed", 0)
            max_stages = 4

            base_confidence = stages_completed / max_stages

            # å¦‚æœæœ‰æœ€ç»ˆç”»åƒï¼Œæé«˜ç½®ä¿¡åº¦
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                profile_confidence = final_profile.get("comparative_metrics", {}).get("style_score_average", 3.0) / 5.0
                base_confidence = (base_confidence + profile_confidence) / 2

            return min(1.0, max(0.0, base_confidence))

        except Exception:
            return 0.7

    def _generate_semantic_style_prompt(self, semantic_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯­ä¹‰é£æ ¼æç¤ºè¯"""
        try:
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                classification = final_profile.get("style_classification", {})
                primary_style = classification.get("primary_style", "ç»¼åˆå‹")
                characteristics = classification.get("style_characteristics", [])

                prompt_parts = [f"è¯·æŒ‰ç…§{primary_style}è¿›è¡Œå†™ä½œ"]

                if characteristics:
                    prompt_parts.append(f"ç‰¹ç‚¹ï¼š{', '.join(characteristics)}")

                # æ·»åŠ å…·ä½“çš„è¯­ä¹‰æŒ‡å¯¼
                style_scores = final_profile.get("style_scores", {})
                if style_scores.get("conceptual_organization", 0) > 4.0:
                    prompt_parts.append("æ³¨é‡æ¦‚å¿µçš„ç³»ç»Ÿæ€§ç»„ç»‡")
                if style_scores.get("creative_association", 0) > 4.0:
                    prompt_parts.append("å‘æŒ¥åˆ›æ–°è”æƒ³èƒ½åŠ›")
                if style_scores.get("emotional_expression", 0) > 4.0:
                    prompt_parts.append("å¢å¼ºæƒ…æ„Ÿè¡¨è¾¾åŠ›")

                return "ï¼›".join(prompt_parts)

            return "è¯·ä¿æŒè¯­ä¹‰è¿è´¯å’Œé€»è¾‘æ¸…æ™°çš„å†™ä½œé£æ ¼"

        except Exception:
            return "è¯·ä¿æŒåŸæœ‰çš„å†™ä½œé£æ ¼"

    def _generate_semantic_recommendations(self, semantic_result: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¯­ä¹‰åˆ†æå»ºè®®"""
        recommendations = []

        try:
            # ä»ç»¼åˆæ´å¯Ÿä¸­æå–å»ºè®®
            comprehensive_insights = semantic_result.get("comprehensive_insights", {})
            actionable_recommendations = comprehensive_insights.get("actionable_recommendations", [])
            recommendations.extend(actionable_recommendations[:3])

            # ä»æœ€ç»ˆç”»åƒä¸­æå–æ”¹è¿›å»ºè®®
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                profile_summary = final_profile.get("profile_summary", {})
                improvements = profile_summary.get("potential_improvements", [])
                for improvement in improvements[:2]:
                    recommendations.append(f"å»ºè®®æå‡{improvement}")

            # å¦‚æœæ²¡æœ‰å…·ä½“å»ºè®®ï¼Œæä¾›é€šç”¨å»ºè®®
            if not recommendations:
                recommendations = [
                    "å»ºè®®ä¿æŒæ¦‚å¿µç»„ç»‡çš„ç³»ç»Ÿæ€§",
                    "æ³¨æ„è¯­ä¹‰è¿è´¯æ€§å’Œé€»è¾‘æ€§",
                    "é€‚å½“å¢åŠ åˆ›æ–°æ€§è¡¨è¾¾"
                ]

            return recommendations[:5]

        except Exception:
            return ["å»ºè®®ä¿æŒå½“å‰çš„è¯­ä¹‰é£æ ¼ç‰¹å¾"]

    def compare_semantic_styles(self, document1_content: str, document2_content: str,
                              doc1_name: str = None, doc2_name: str = None) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£çš„è¯­ä¹‰é£æ ¼

        Args:
            document1_content: ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹
            document2_content: ç¬¬äºŒä¸ªæ–‡æ¡£å†…å®¹
            doc1_name: ç¬¬ä¸€ä¸ªæ–‡æ¡£åç§°
            doc2_name: ç¬¬äºŒä¸ªæ–‡æ¡£åç§°

        Returns:
            è¯­ä¹‰é£æ ¼æ¯”è¾ƒç»“æœ
        """
        if not self.use_enhanced_features or not self.enhanced_processor:
            return {"error": "å¢å¼ºåŠŸèƒ½æœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰é£æ ¼æ¯”è¾ƒ"}

        comparison_result = {
            "comparison_time": datetime.now().isoformat(),
            "document1_name": doc1_name or "æ–‡æ¡£1",
            "document2_name": doc2_name or "æ–‡æ¡£2",
            "document1_analysis": {},
            "document2_analysis": {},
            "semantic_comparison": {},
            "style_compatibility": "unknown",
            "comparison_summary": {},
            "success": False
        }

        try:
            print(f"ğŸ” å¼€å§‹è¯­ä¹‰é£æ ¼æ¯”è¾ƒ: {doc1_name or 'æ–‡æ¡£1'} vs {doc2_name or 'æ–‡æ¡£2'}")

            # åˆ†æç¬¬ä¸€ä¸ªæ–‡æ¡£
            doc1_analysis = self.analyze_with_semantic_behavior(document1_content, doc1_name)
            comparison_result["document1_analysis"] = doc1_analysis

            # åˆ†æç¬¬äºŒä¸ªæ–‡æ¡£
            doc2_analysis = self.analyze_with_semantic_behavior(document2_content, doc2_name)
            comparison_result["document2_analysis"] = doc2_analysis

            # å¦‚æœä¸¤ä¸ªåˆ†æéƒ½æˆåŠŸï¼Œè¿›è¡Œæ¯”è¾ƒ
            if (not doc1_analysis.get("error") and not doc2_analysis.get("error") and
                self.enhanced_processor.semantic_analysis_enabled):

                # ä½¿ç”¨ç»¼åˆå¤„ç†å™¨çš„è¯­ä¹‰æ¯”è¾ƒåŠŸèƒ½
                semantic_comparison = self.enhanced_processor.compare_semantic_profiles(
                    document1_content, document2_content, doc1_name, doc2_name
                )
                comparison_result["semantic_comparison"] = semantic_comparison

                # ç”Ÿæˆå…¼å®¹æ€§è¯„ä¼°
                if semantic_comparison.get("success"):
                    profile_comparison = semantic_comparison.get("profile_comparison", {})
                    similarity_score = profile_comparison.get("similarity_score", 0.0)

                    if similarity_score > 0.8:
                        comparison_result["style_compatibility"] = "é«˜åº¦å…¼å®¹"
                    elif similarity_score > 0.6:
                        comparison_result["style_compatibility"] = "è¾ƒä¸ºå…¼å®¹"
                    elif similarity_score > 0.4:
                        comparison_result["style_compatibility"] = "éƒ¨åˆ†å…¼å®¹"
                    else:
                        comparison_result["style_compatibility"] = "å·®å¼‚è¾ƒå¤§"

                # ç”Ÿæˆæ¯”è¾ƒæ‘˜è¦
                comparison_result["comparison_summary"] = self._generate_semantic_comparison_summary(
                    doc1_analysis, doc2_analysis, semantic_comparison
                )

                comparison_result["success"] = True
                print("âœ… è¯­ä¹‰é£æ ¼æ¯”è¾ƒå®Œæˆ")
            else:
                comparison_result["error"] = "æ–‡æ¡£åˆ†æå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰æ¯”è¾ƒ"
                print("âŒ è¯­ä¹‰é£æ ¼æ¯”è¾ƒå¤±è´¥")

        except Exception as e:
            comparison_result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰é£æ ¼æ¯”è¾ƒå¤±è´¥: {str(e)}")

        return comparison_result

    def _generate_semantic_comparison_summary(self, doc1_analysis: Dict[str, Any],
                                            doc2_analysis: Dict[str, Any],
                                            semantic_comparison: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯­ä¹‰æ¯”è¾ƒæ‘˜è¦"""
        summary = {
            "overall_similarity": 0.0,
            "style_differences": [],
            "common_characteristics": [],
            "recommendation": ""
        }

        try:
            # æ•´ä½“ç›¸ä¼¼åº¦
            if semantic_comparison.get("success"):
                profile_comparison = semantic_comparison.get("profile_comparison", {})
                summary["overall_similarity"] = profile_comparison.get("similarity_score", 0.0)

                # ç»´åº¦å·®å¼‚
                dimension_diffs = profile_comparison.get("dimension_differences", {})
                differences = []
                similarities = []

                for dimension, diff_data in dimension_diffs.items():
                    difference = diff_data.get("difference", 0)
                    if difference > 1.0:  # å·®å¼‚è¾ƒå¤§
                        differences.append(f"{dimension}å·®å¼‚è¾ƒå¤§")
                    elif difference < 0.5:  # ç›¸ä¼¼åº¦è¾ƒé«˜
                        similarities.append(f"{dimension}è¾ƒä¸ºç›¸ä¼¼")

                summary["style_differences"] = differences[:3]
                summary["common_characteristics"] = similarities[:3]

            # ç”Ÿæˆå»ºè®®
            similarity_score = summary["overall_similarity"]
            if similarity_score > 0.7:
                summary["recommendation"] = "ä¸¤ä¸ªæ–‡æ¡£é£æ ¼ç›¸è¿‘ï¼Œå¯ä»¥è¿›è¡Œé£æ ¼å¯¹é½"
            elif similarity_score > 0.4:
                summary["recommendation"] = "ä¸¤ä¸ªæ–‡æ¡£é£æ ¼æœ‰ä¸€å®šå·®å¼‚ï¼Œå»ºè®®é‡ç‚¹è°ƒæ•´å·®å¼‚è¾ƒå¤§çš„ç»´åº¦"
            else:
                summary["recommendation"] = "ä¸¤ä¸ªæ–‡æ¡£é£æ ¼å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦å…¨é¢çš„é£æ ¼è¿ç§»"

        except Exception as e:
            summary["error"] = str(e)

        return summary

    def _get_document_statistics(self, content: str) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        lines = content.split('\n')
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        words = re.findall(r'[\u4e00-\u9fff]+', content)

        return {
            "total_characters": len(content),
            "total_lines": len(lines),
            "total_sentences": len([s for s in sentences if s.strip()]),
            "total_paragraphs": len(paragraphs),
            "total_words": len(words),
            "average_sentence_length": round(len(content) / max(len([s for s in sentences if s.strip()]), 1), 1),
            "average_paragraph_length": round(len(content) / max(len(paragraphs), 1), 1),
            "reading_time_minutes": round(len(words) / 200, 1)  # å‡è®¾æ¯åˆ†é’Ÿ200å­—
        }

    def _generate_detailed_analysis(self, content: str, features: Dict[str, Any], style_type: str) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        return {
            "readability_analysis": self._analyze_readability(content, features),
            "tone_analysis": self._analyze_tone_details(content, features),
            "structure_analysis": self._analyze_structure_details(content, features),
            "vocabulary_analysis": self._analyze_vocabulary_details(content, features),
            "style_consistency": self._analyze_style_consistency(content, features)
        }

    def _generate_writing_recommendations(self, features: Dict[str, Any], style_type: str) -> List[str]:
        """ç”Ÿæˆå†™ä½œå»ºè®®"""
        recommendations = []

        # åŸºäºå¥å¼ç»“æ„çš„å»ºè®®
        sentence_features = features.get("sentence_structure", {})
        avg_length = sentence_features.get("average_length", 15)

        if avg_length > 30:
            recommendations.append("å¥å­åé•¿ï¼Œå»ºè®®é€‚å½“æ‹†åˆ†ä¸ºçŸ­å¥ä»¥æé«˜å¯è¯»æ€§")
        elif avg_length < 10:
            recommendations.append("å¥å­åçŸ­ï¼Œå¯ä»¥é€‚å½“å¢åŠ å¥å­çš„å®Œæ•´æ€§å’Œè¡¨è¾¾åŠ›")

        # åŸºäºè¯æ±‡é€‰æ‹©çš„å»ºè®®
        vocab_features = features.get("vocabulary_choice", {})
        modifier_usage = vocab_features.get("modifier_usage", 0)

        if modifier_usage > 50:
            recommendations.append("ä¿®é¥°è¯ä½¿ç”¨è¾ƒå¤šï¼Œå»ºè®®ç²¾ç®€è¡¨è¾¾ï¼Œçªå‡ºé‡ç‚¹")
        elif modifier_usage < 10:
            recommendations.append("å¯ä»¥é€‚å½“å¢åŠ ä¿®é¥°è¯ï¼Œä¸°å¯Œè¡¨è¾¾å±‚æ¬¡")

        # åŸºäºæ–‡é£ç±»å‹çš„å»ºè®®
        if style_type == "business_professional":
            recommendations.append("ä¿æŒä¸“ä¸šæ€§ï¼Œæ³¨æ„ç”¨è¯å‡†ç¡®æ€§å’Œé€»è¾‘æ¸…æ™°")
        elif style_type == "academic_research":
            recommendations.append("å¢å¼ºè®ºè¯ä¸¥å¯†æ€§ï¼Œæ³¨æ„å¼•ç”¨å’Œæ•°æ®æ”¯æ’‘")
        elif style_type == "concise_practical":
            recommendations.append("ç»§ç»­ä¿æŒç®€æ´æ˜äº†ï¼Œçªå‡ºå®ç”¨æ€§")

        return recommendations

    def _generate_style_comparison(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé£æ ¼å¯¹æ¯”åˆ†æ"""
        return {
            "formal_vs_informal": self._compare_formality(features),
            "technical_vs_general": self._compare_technicality(features),
            "objective_vs_subjective": self._compare_objectivity(features),
            "concise_vs_elaborate": self._compare_conciseness(features)
        }

    def _analyze_style_features(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡é£ç‰¹å¾"""
        if not content or not content.strip():
            return self._get_empty_features()

        features = {}

        try:
            # å¥å¼ç»“æ„åˆ†æ
            features["sentence_structure"] = self._analyze_sentence_structure(content)

            # è¯æ±‡é€‰æ‹©åˆ†æ
            features["vocabulary_choice"] = self._analyze_vocabulary_choice(content)

            # è¡¨è¾¾æ–¹å¼åˆ†æ
            features["expression_style"] = self._analyze_expression_style(content)

            # æ–‡æœ¬ç»„ç»‡åˆ†æ
            features["text_organization"] = self._analyze_text_organization(content)

            # è¯­è¨€ä¹ æƒ¯åˆ†æ
            features["language_habits"] = self._analyze_language_habits(content)

            # æ–°å¢ï¼šæƒ…æ„Ÿè‰²å½©åˆ†æ
            features["emotional_tone"] = self._analyze_emotional_tone(content)

            # æ–°å¢ï¼šä¸“ä¸šæ€§åˆ†æ
            features["professionalism"] = self._analyze_professionalism(content)

            # æ–°å¢ï¼šä¿®è¾æ‰‹æ³•åˆ†æ
            features["rhetorical_devices"] = self._analyze_rhetorical_devices(content)

        except Exception as e:
            print(f"æ–‡é£ç‰¹å¾åˆ†æå‡ºé”™: {str(e)}")
            features = self._get_empty_features()

        return features

    def _get_empty_features(self) -> Dict[str, Any]:
        """è·å–ç©ºçš„ç‰¹å¾ç»“æ„"""
        return {
            "sentence_structure": {"average_length": 0, "long_short_ratio": 0, "complex_ratio": 0, "total_sentences": 0},
            "vocabulary_choice": {"formality_score": 0, "technical_density": 0, "modifier_usage": 0, "action_verb_ratio": 0},
            "expression_style": {"passive_active_ratio": 0, "person_usage": {"first_person": 0, "second_person": 0, "third_person": 0}, "tone_strength": 0},
            "text_organization": {"paragraph_count": 0, "average_paragraph_length": 0, "connector_density": 0, "summary_usage": 0},
            "language_habits": {"colloquial_level": 0, "formal_structure_usage": 0, "de_structure_density": 0},
            "emotional_tone": {"positive_ratio": 0, "negative_ratio": 0, "neutral_ratio": 1.0, "intensity": 0},
            "professionalism": {"domain_specificity": 0, "authority_indicators": 0, "precision_level": 0},
            "rhetorical_devices": {"metaphor_usage": 0, "parallel_structure": 0, "question_usage": 0}
        }
    
    def _analyze_sentence_structure(self, content: str) -> Dict[str, Any]:
        """åˆ†æå¥å¼ç»“æ„"""
        # æ¸…ç†å†…å®¹ï¼Œç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content.strip())

        # æ›´ç²¾ç¡®çš„å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\.!?;]', content)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]

        if not sentences:
            return {
                "average_length": 0,
                "long_short_ratio": 0,
                "complex_ratio": 0,
                "total_sentences": 0
            }

        # è®¡ç®—å¹³å‡å¥é•¿
        total_chars = sum(len(s) for s in sentences)
        average_length = total_chars / len(sentences)

        # é•¿çŸ­å¥æ¯”ä¾‹ï¼ˆé•¿å¥å®šä¹‰ä¸ºè¶…è¿‡25å­—ï¼‰
        long_sentences = [s for s in sentences if len(s) > 25]
        short_sentences = [s for s in sentences if len(s) <= 15]
        long_short_ratio = len(long_sentences) / len(sentences) if sentences else 0

        # å¤åˆå¥æ¯”ä¾‹ï¼ˆåŒ…å«é€—å·ã€åˆ†å·ã€è¿è¯ç­‰çš„å¥å­ï¼‰
        complex_patterns = [r'ï¼Œ', r'ï¼›', r'ï¼š', r'è€Œä¸”', r'ä½†æ˜¯', r'ç„¶è€Œ', r'å› ä¸º', r'æ‰€ä»¥', r'å¦‚æœ', r'è™½ç„¶']
        complex_sentences = []
        for sentence in sentences:
            if any(re.search(pattern, sentence) for pattern in complex_patterns):
                complex_sentences.append(sentence)
        complex_ratio = len(complex_sentences) / len(sentences) if sentences else 0

        return {
            "average_length": round(average_length, 1),
            "long_short_ratio": round(long_short_ratio, 3),
            "complex_ratio": round(complex_ratio, 3),
            "total_sentences": len(sentences),
            "long_sentences": len(long_sentences),
            "short_sentences": len(short_sentences)
        }
    
    def _analyze_vocabulary_choice(self, content: str) -> Dict[str, Any]:
        """åˆ†æè¯æ±‡é€‰æ‹©"""
        # æ¸…ç†å†…å®¹
        content_clean = re.sub(r'\s+', '', content)
        total_chars = len(content_clean)

        if total_chars == 0:
            return {
                "formality_score": 0,
                "technical_density": 0,
                "modifier_usage": 0,
                "action_verb_ratio": 0
            }

        # æ­£å¼è¯æ±‡æ¨¡å¼
        formal_words = ["æ ¹æ®", "æŒ‰ç…§", "ä¾æ®", "é‰´äº", "åŸºäº", "å…³äº", "é’ˆå¯¹", "ä¸ºäº†", "é€šè¿‡", "é‡‡ç”¨",
                       "è¿›è¡Œ", "å®æ–½", "å¼€å±•", "è½å®", "ç¡®ä¿", "ä¿è¯", "ç»´æŠ¤", "ä¿ƒè¿›", "æ¨åŠ¨", "åŠ å¼º"]
        formal_count = sum(content.count(word) for word in formal_words)

        # ä¸“ä¸šæœ¯è¯­æ¨¡å¼
        technical_patterns = [
            r'[\u4e00-\u9fff]+ç³»ç»Ÿ', r'[\u4e00-\u9fff]+æŠ€æœ¯', r'[\u4e00-\u9fff]+æ–¹æ¡ˆ',
            r'[\u4e00-\u9fff]+æ ‡å‡†', r'[\u4e00-\u9fff]+è§„èŒƒ', r'[\u4e00-\u9fff]+å¹³å°',
            r'[\u4e00-\u9fff]+æ¨¡å¼', r'[\u4e00-\u9fff]+æœºåˆ¶', r'[\u4e00-\u9fff]+æµç¨‹'
        ]
        technical_count = sum(len(re.findall(pattern, content)) for pattern in technical_patterns)

        # ä¿®é¥°è¯ä½¿ç”¨
        modifiers = ["å¾ˆ", "éå¸¸", "ç‰¹åˆ«", "æå…¶", "ç›¸å½“", "æ¯”è¾ƒ", "è¾ƒä¸º", "ååˆ†", "æ›´åŠ ", "è¿›ä¸€æ­¥"]
        modifier_count = sum(content.count(word) for word in modifiers)

        # åŠ¨è¯ç±»å‹ï¼ˆåŠ¨ä½œåŠ¨è¯ï¼‰
        action_verbs = ["å®æ–½", "æ‰§è¡Œ", "å¼€å±•", "æ¨è¿›", "è½å®", "å®Œæˆ", "è¾¾æˆ", "å®ç°", "æå‡", "ä¼˜åŒ–",
                       "æ”¹è¿›", "å»ºè®¾", "æ„å»º", "å‘å±•", "åˆ›æ–°", "çªç ´", "è§£å†³", "å¤„ç†", "ç®¡ç†", "è¿è¥"]
        action_count = sum(content.count(word) for word in action_verbs)

        # è®¡ç®—å¯†åº¦ï¼ˆæ¯åƒå­—ï¼‰
        multiplier = 1000 / total_chars if total_chars > 0 else 0

        return {
            "formality_score": round(formal_count * multiplier, 2),
            "technical_density": round(technical_count * multiplier, 2),
            "modifier_usage": round(modifier_count * multiplier, 2),
            "action_verb_ratio": round(action_count * multiplier, 2)
        }
    
    def _analyze_expression_style(self, content: str) -> Dict[str, Any]:
        """åˆ†æè¡¨è¾¾æ–¹å¼"""
        total_chars = len(content)

        if total_chars == 0:
            return {
                "passive_active_ratio": 0,
                "person_usage": {"first_person": 0, "second_person": 0, "third_person": 0},
                "tone_strength": 0
            }

        # è¢«åŠ¨è¯­æ€æ£€æµ‹ï¼ˆæ›´ç²¾ç¡®çš„æ¨¡å¼ï¼‰
        passive_patterns = [
            r'[\u4e00-\u9fff]+è¢«[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+å—åˆ°[\u4e00-\u9fff]+',
            r'[\u4e00-\u9fff]+å¾—åˆ°[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+è·å¾—[\u4e00-\u9fff]+',
            r'[\u4e00-\u9fff]+é­åˆ°[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+å—[\u4e00-\u9fff]+å½±å“'
        ]
        passive_count = sum(len(re.findall(pattern, content)) for pattern in passive_patterns)

        # ä¸»åŠ¨è¯­æ€æ£€æµ‹
        active_patterns = [
            r'[\u4e00-\u9fff]+è¿›è¡Œ[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+å¼€å±•[\u4e00-\u9fff]+',
            r'[\u4e00-\u9fff]+å®æ–½[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+æ¨è¿›[\u4e00-\u9fff]+',
            r'[\u4e00-\u9fff]+å®Œæˆ[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+å»ºè®¾[\u4e00-\u9fff]+'
        ]
        active_count = sum(len(re.findall(pattern, content)) for pattern in active_patterns)

        # äººç§°ä½¿ç”¨ç»Ÿè®¡
        first_person = content.count('æˆ‘') + content.count('æˆ‘ä»¬') + content.count('æœ¬äºº') + content.count('ç¬”è€…')
        second_person = content.count('ä½ ') + content.count('æ‚¨') + content.count('ä½ ä»¬') + content.count('å„ä½')
        third_person = content.count('ä»–') + content.count('å¥¹') + content.count('å®ƒ') + content.count('ä»–ä»¬') + content.count('å¥¹ä»¬')

        # è¯­æ°”å¼ºåº¦åˆ†æ
        strong_words = ['å¿…é¡»', 'åŠ¡å¿…', 'ä¸¥ç¦', 'ç»å¯¹', 'ä¸€å®š', 'åšå†³', 'ä¸¥æ ¼', 'åˆ‡å®']
        mild_words = ['å»ºè®®', 'å¸Œæœ›', 'å¯ä»¥', 'å°½é‡', 'é€‚å½“', 'é…Œæƒ…', 'å¯èƒ½', 'æˆ–è®¸']

        strong_tone = sum(content.count(word) for word in strong_words)
        mild_tone = sum(content.count(word) for word in mild_words)

        # è®¡ç®—æ¯”ä¾‹
        total_voice_patterns = passive_count + active_count
        passive_ratio = passive_count / total_voice_patterns if total_voice_patterns > 0 else 0

        return {
            "passive_active_ratio": round(passive_ratio, 3),
            "person_usage": {
                "first_person": first_person,
                "second_person": second_person,
                "third_person": third_person
            },
            "tone_strength": round((strong_tone - mild_tone) / total_chars * 1000, 2) if total_chars > 0 else 0,
            "passive_count": passive_count,
            "active_count": active_count
        }
    
    def _analyze_text_organization(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬ç»„ç»‡"""
        # æ®µè½åˆ†æï¼ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²ï¼‰
        paragraphs = re.split(r'\n\s*\n', content)
        paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 10]

        total_chars = len(content)

        if total_chars == 0:
            return {
                "paragraph_count": 0,
                "average_paragraph_length": 0,
                "connector_density": 0,
                "summary_usage": 0
            }

        # é€»è¾‘è¿æ¥è¯ï¼ˆæ›´å…¨é¢ï¼‰
        connectors = [
            "é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æœ€å", "å› æ­¤", "æ‰€ä»¥", "ä½†æ˜¯", "ç„¶è€Œ", "æ­¤å¤–", "å¦å¤–",
            "åŒæ—¶", "è€Œä¸”", "å¹¶ä¸”", "ä¸è¿‡", "è™½ç„¶", "å°½ç®¡", "ç”±äº", "é‰´äº", "åŸºäº", "æ ¹æ®",
            "æ€»ä¹‹", "ç»¼ä¸Š", "å¦ä¸€æ–¹é¢", "ä¸æ­¤åŒæ—¶", "ç›¸æ¯”ä¹‹ä¸‹", "æ¢è¨€ä¹‹", "ä¹Ÿå°±æ˜¯è¯´"
        ]
        connector_count = sum(content.count(word) for word in connectors)

        # æ€»ç»“æ€§è¡¨è¾¾
        summary_words = ["æ€»ä¹‹", "ç»¼ä¸Š", "æ€»çš„æ¥è¯´", "ç»¼åˆä»¥ä¸Š", "æ€»è€Œè¨€ä¹‹", "ç»¼ä¸Šæ‰€è¿°", "ç”±æ­¤å¯è§", "å¯ä»¥çœ‹å‡º"]
        summary_count = sum(content.count(word) for word in summary_words)

        # åˆ—ä¸¾æ ‡è¯†
        enumeration_patterns = [r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€', r'\d+[\.ã€]', r'[ï¼ˆ(]\d+[ï¼‰)]']
        enumeration_count = sum(len(re.findall(pattern, content)) for pattern in enumeration_patterns)

        return {
            "paragraph_count": len(paragraphs),
            "average_paragraph_length": round(sum(len(p) for p in paragraphs) / len(paragraphs), 1) if paragraphs else 0,
            "connector_density": round(connector_count / total_chars * 1000, 2),
            "summary_usage": summary_count,
            "enumeration_usage": enumeration_count
        }
    
    def _analyze_language_habits(self, content: str) -> Dict[str, Any]:
        """åˆ†æè¯­è¨€ä¹ æƒ¯"""
        total_chars = len(content)

        if total_chars == 0:
            return {
                "colloquial_level": 0,
                "formal_structure_usage": 0,
                "de_structure_density": 0
            }

        # å£è¯­åŒ–è¯æ±‡
        colloquial_words = [
            "æŒº", "è›®", "ç‰¹åˆ«", "è¶…çº§", "å¥½åƒ", "æ„Ÿè§‰", "è§‰å¾—", "åº”è¯¥", "å¯èƒ½", "å¤§æ¦‚",
            "å·®ä¸å¤š", "åŸºæœ¬ä¸Š", "ä¸€èˆ¬æ¥è¯´", "è¯´å®è¯", "è€å®è¯´", "å¦ç™½è¯´", "çœŸçš„", "ç¡®å®"
        ]
        colloquial_count = sum(content.count(word) for word in colloquial_words)

        # ä¹¦é¢è¯­ç»“æ„è¯
        formal_structures = [
            "ä¹‹", "å…¶", "æ‰€", "ä¹ƒ", "å³", "äº¦", "ä¸”", "è€Œ", "äº", "ä»¥", "ä¸º", "ä¸",
            "åŠ", "æˆ–", "è‹¥", "åˆ™", "æ•…", "å› ", "ç”±", "è‡ª", "ä»", "å‘", "è‡³", "åŠå…¶"
        ]
        formal_count = sum(content.count(word) for word in formal_structures)

        # "çš„"å­—ç»“æ„å¯†åº¦
        de_count = content.count('çš„')

        # ä¹¦é¢è¯­å¥å¼
        formal_patterns = [r'[\u4e00-\u9fff]+ä¹‹[\u4e00-\u9fff]+', r'æ‰€[\u4e00-\u9fff]+', r'å…¶[\u4e00-\u9fff]+']
        formal_pattern_count = sum(len(re.findall(pattern, content)) for pattern in formal_patterns)

        return {
            "colloquial_level": round(colloquial_count / total_chars * 1000, 2),
            "formal_structure_usage": round((formal_count + formal_pattern_count) / total_chars * 1000, 2),
            "de_structure_density": round(de_count / total_chars * 1000, 2)
        }

    def _analyze_emotional_tone(self, content: str) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿè‰²å½©"""
        total_chars = len(content)
        if total_chars == 0:
            return {"positive_ratio": 0, "negative_ratio": 0, "neutral_ratio": 1.0, "intensity": 0}

        # ç§¯æè¯æ±‡
        positive_words = ["ä¼˜ç§€", "å“è¶Š", "æˆåŠŸ", "è¿›æ­¥", "æå‡", "æ”¹å–„", "åˆ›æ–°", "çªç ´", "å‘å±•", "å¢é•¿",
                         "æ»¡æ„", "é«˜å…´", "å–œæ‚¦", "èµæ‰¬", "è¡¨å½°", "è‚¯å®š", "æ”¯æŒ", "é¼“åŠ±", "å¸Œæœ›", "ä¿¡å¿ƒ"]

        # æ¶ˆæè¯æ±‡
        negative_words = ["é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜", "ä¸è¶³", "ç¼ºé™·", "é”™è¯¯", "å¤±è´¥", "ä¸‹é™", "å‡å°‘", "æŸå¤±",
                         "æ‹…å¿ƒ", "å¿§è™‘", "æ‰¹è¯„", "è´¨ç–‘", "åå¯¹", "æ‹’ç»", "å¦å®š", "è­¦å‘Š", "é£é™©", "å±æœº"]

        # å¼ºåº¦è¯æ±‡
        intensity_words = ["éå¸¸", "æå…¶", "ç‰¹åˆ«", "ååˆ†", "ç›¸å½“", "å¾ˆ", "æœ€", "æ›´", "è¿›ä¸€æ­¥", "å¤§å¹…"]

        positive_count = sum(content.count(word) for word in positive_words)
        negative_count = sum(content.count(word) for word in negative_words)
        intensity_count = sum(content.count(word) for word in intensity_words)

        total_emotional = positive_count + negative_count
        if total_emotional == 0:
            return {"positive_ratio": 0, "negative_ratio": 0, "neutral_ratio": 1.0, "intensity": 0}

        return {
            "positive_ratio": round(positive_count / total_emotional, 2),
            "negative_ratio": round(negative_count / total_emotional, 2),
            "neutral_ratio": round(1 - (positive_count + negative_count) / total_emotional, 2),
            "intensity": round(intensity_count / total_chars * 1000, 2)
        }

    def _analyze_professionalism(self, content: str) -> Dict[str, Any]:
        """åˆ†æä¸“ä¸šæ€§ç‰¹å¾"""
        total_chars = len(content)
        if total_chars == 0:
            return {"domain_specificity": 0, "authority_indicators": 0, "precision_level": 0}

        # æƒå¨æ€§æŒ‡æ ‡è¯æ±‡
        authority_words = ["æ ¹æ®", "ä¾æ®", "æŒ‰ç…§", "ç ”ç©¶è¡¨æ˜", "æ•°æ®æ˜¾ç¤º", "ç»Ÿè®¡", "è°ƒæŸ¥", "åˆ†æ",
                          "ä¸“å®¶", "å­¦è€…", "æƒå¨", "å®˜æ–¹", "æ­£å¼", "æ³•è§„", "æ ‡å‡†", "è§„èŒƒ"]

        # ç²¾ç¡®æ€§æŒ‡æ ‡
        precision_patterns = [r'\d+%', r'\d+\.\d+', r'ç¬¬\d+', r'\d+å¹´\d+æœˆ', r'\d+ä¸‡', r'\d+äº¿']

        # é¢†åŸŸç‰¹å®šè¯æ±‡ï¼ˆç¤ºä¾‹ï¼‰
        domain_words = ["æŠ€æœ¯", "ç³»ç»Ÿ", "å¹³å°", "æ–¹æ¡ˆ", "ç­–ç•¥", "æœºåˆ¶", "æ¨¡å¼", "æ¡†æ¶", "ä½“ç³»", "æµç¨‹"]

        authority_count = sum(content.count(word) for word in authority_words)
        domain_count = sum(content.count(word) for word in domain_words)
        precision_count = sum(len(re.findall(pattern, content)) for pattern in precision_patterns)

        return {
            "domain_specificity": round(domain_count / total_chars * 1000, 2),
            "authority_indicators": round(authority_count / total_chars * 1000, 2),
            "precision_level": round(precision_count / total_chars * 1000, 2)
        }

    def _analyze_rhetorical_devices(self, content: str) -> Dict[str, Any]:
        """åˆ†æä¿®è¾æ‰‹æ³•"""
        total_chars = len(content)
        if total_chars == 0:
            return {"metaphor_usage": 0, "parallel_structure": 0, "question_usage": 0}

        # æ¯”å–»è¯æ±‡
        metaphor_words = ["å¦‚åŒ", "å¥½æ¯”", "çŠ¹å¦‚", "ä»¿ä½›", "åƒ", "ä¼¼", "å®›å¦‚", "æ°ä¼¼"]

        # æ’æ¯”ç»“æ„æ¨¡å¼
        parallel_patterns = [r'[\u4e00-\u9fff]+ï¼Œ[\u4e00-\u9fff]+ï¼Œ[\u4e00-\u9fff]+',
                           r'ä¸ä»…[\u4e00-\u9fff]+ï¼Œè€Œä¸”[\u4e00-\u9fff]+',
                           r'æ—¢[\u4e00-\u9fff]+ï¼Œåˆ[\u4e00-\u9fff]+']

        # ç–‘é—®å¥
        question_count = content.count('ï¼Ÿ') + content.count('?')

        metaphor_count = sum(content.count(word) for word in metaphor_words)
        parallel_count = sum(len(re.findall(pattern, content)) for pattern in parallel_patterns)

        return {
            "metaphor_usage": round(metaphor_count / total_chars * 1000, 2),
            "parallel_structure": round(parallel_count / total_chars * 1000, 2),
            "question_usage": round(question_count / total_chars * 1000, 2)
        }

    def _analyze_readability(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå¯è¯»æ€§"""
        sentence_features = features.get("sentence_structure", {})
        vocab_features = features.get("vocabulary_choice", {})

        # è®¡ç®—å¯è¯»æ€§åˆ†æ•° (ç®€åŒ–ç‰ˆ)
        avg_sentence_length = sentence_features.get("average_length", 15)
        technical_density = vocab_features.get("technical_density", 0)

        readability_score = max(0, min(100, 100 - (avg_sentence_length - 15) * 2 - technical_density))

        if readability_score >= 80:
            level = "å¾ˆå®¹æ˜“é˜…è¯»"
        elif readability_score >= 60:
            level = "è¾ƒå®¹æ˜“é˜…è¯»"
        elif readability_score >= 40:
            level = "ä¸­ç­‰éš¾åº¦"
        elif readability_score >= 20:
            level = "è¾ƒéš¾é˜…è¯»"
        else:
            level = "å¾ˆéš¾é˜…è¯»"

        return {
            "readability_score": round(readability_score, 1),
            "readability_level": level,
            "factors": {
                "sentence_complexity": "é«˜" if avg_sentence_length > 25 else "ä¸­" if avg_sentence_length > 15 else "ä½",
                "vocabulary_difficulty": "é«˜" if technical_density > 20 else "ä¸­" if technical_density > 10 else "ä½"
            }
        }

    def _analyze_tone_details(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯­è°ƒè¯¦æƒ…"""
        emotional_features = features.get("emotional_tone", {})

        # åˆ†æè¯­è°ƒå€¾å‘
        positive_words = ["å¥½", "ä¼˜ç§€", "æˆåŠŸ", "æå‡", "æ”¹å–„", "åˆ›æ–°", "å‘å±•", "è¿›æ­¥"]
        negative_words = ["é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜", "ä¸è¶³", "ç¼ºé™·", "å¤±è´¥", "ä¸‹é™", "å‡å°‘"]
        neutral_words = ["åˆ†æ", "ç ”ç©¶", "æ¢è®¨", "è€ƒè™‘", "å»ºè®®", "æ–¹æ¡ˆ", "è®¡åˆ’", "å®æ–½"]

        positive_count = sum(content.count(word) for word in positive_words)
        negative_count = sum(content.count(word) for word in negative_words)
        neutral_count = sum(content.count(word) for word in neutral_words)

        total_tone_words = positive_count + negative_count + neutral_count

        if total_tone_words > 0:
            tone_distribution = {
                "positive_ratio": round(positive_count / total_tone_words * 100, 1),
                "negative_ratio": round(negative_count / total_tone_words * 100, 1),
                "neutral_ratio": round(neutral_count / total_tone_words * 100, 1)
            }
        else:
            tone_distribution = {"positive_ratio": 33.3, "negative_ratio": 33.3, "neutral_ratio": 33.3}

        return {
            "tone_distribution": tone_distribution,
            "dominant_tone": max(tone_distribution.items(), key=lambda x: x[1])[0].replace("_ratio", ""),
            "emotional_intensity": emotional_features.get("intensity_score", 0),
            "tone_consistency": "é«˜" if max(tone_distribution.values()) > 60 else "ä¸­" if max(tone_distribution.values()) > 40 else "ä½"
        }

    def _analyze_structure_details(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç»“æ„è¯¦æƒ…"""
        org_features = features.get("text_organization", {})

        return {
            "paragraph_structure": {
                "paragraph_count": org_features.get("paragraph_count", 0),
                "average_length": org_features.get("average_paragraph_length", 0),
                "length_consistency": "é«˜" if org_features.get("average_paragraph_length", 0) > 100 else "ä¸­"
            },
            "logical_flow": {
                "connector_usage": org_features.get("connector_density", 0),
                "enumeration_usage": org_features.get("enumeration_usage", 0),
                "summary_usage": org_features.get("summary_usage", 0)
            },
            "organization_score": min(100, (org_features.get("connector_density", 0) * 2 +
                                           org_features.get("enumeration_usage", 0) * 5 +
                                           org_features.get("summary_usage", 0) * 10))
        }

    def _analyze_vocabulary_details(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯æ±‡è¯¦æƒ…"""
        vocab_features = features.get("vocabulary_choice", {})

        return {
            "vocabulary_richness": {
                "unique_words": len(set(re.findall(r'[\u4e00-\u9fff]+', content))),
                "total_words": len(re.findall(r'[\u4e00-\u9fff]+', content)),
                "diversity_ratio": round(len(set(re.findall(r'[\u4e00-\u9fff]+', content))) /
                                       max(len(re.findall(r'[\u4e00-\u9fff]+', content)), 1) * 100, 1)
            },
            "word_complexity": {
                "technical_density": vocab_features.get("technical_density", 0),
                "formality_score": vocab_features.get("formality_score", 0),
                "modifier_usage": vocab_features.get("modifier_usage", 0)
            },
            "action_orientation": {
                "action_verb_ratio": vocab_features.get("action_verb_ratio", 0),
                "passive_voice_usage": self._count_passive_voice(content)
            }
        }

    def _analyze_style_consistency(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé£æ ¼ä¸€è‡´æ€§"""
        # åˆ†æ®µåˆ†æé£æ ¼ä¸€è‡´æ€§
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

        if len(paragraphs) < 2:
            return {"consistency_score": 100, "variation_level": "æ— æ³•è¯„ä¼°"}

        # ç®€åŒ–çš„ä¸€è‡´æ€§åˆ†æ
        consistency_factors = []

        # å¥é•¿ä¸€è‡´æ€§
        sentence_lengths = []
        for para in paragraphs:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', para)
            avg_len = sum(len(s) for s in sentences if s.strip()) / max(len([s for s in sentences if s.strip()]), 1)
            sentence_lengths.append(avg_len)

        if sentence_lengths:
            length_variance = max(sentence_lengths) - min(sentence_lengths)
            consistency_factors.append(max(0, 100 - length_variance * 2))

        consistency_score = sum(consistency_factors) / max(len(consistency_factors), 1)

        return {
            "consistency_score": round(consistency_score, 1),
            "variation_level": "ä½" if consistency_score > 80 else "ä¸­" if consistency_score > 60 else "é«˜",
            "factors_analyzed": ["å¥é•¿ä¸€è‡´æ€§", "è¯æ±‡ä½¿ç”¨", "è¯­è°ƒå˜åŒ–"]
        }

    def _count_passive_voice(self, content: str) -> float:
        """ç»Ÿè®¡è¢«åŠ¨è¯­æ€ä½¿ç”¨é¢‘ç‡"""
        passive_patterns = [r'è¢«[\u4e00-\u9fff]+', r'å—åˆ°[\u4e00-\u9fff]+', r'å¾—åˆ°[\u4e00-\u9fff]+']
        passive_count = sum(len(re.findall(pattern, content)) for pattern in passive_patterns)
        total_chars = len(content)
        return round(passive_count / total_chars * 1000, 2) if total_chars > 0 else 0

    def _compare_formality(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”æ­£å¼æ€§"""
        vocab_features = features.get("vocabulary_choice", {})
        habit_features = features.get("language_habits", {})

        formality_score = vocab_features.get("formality_score", 0)
        formal_structure = habit_features.get("formal_structure_usage", 0)
        colloquial_level = habit_features.get("colloquial_level", 0)

        overall_formality = (formality_score + formal_structure - colloquial_level) / 3

        return {
            "formality_level": "é«˜" if overall_formality > 20 else "ä¸­" if overall_formality > 10 else "ä½",
            "formal_score": round(overall_formality, 1),
            "indicators": {
                "formal_vocabulary": formality_score,
                "formal_structures": formal_structure,
                "colloquial_elements": colloquial_level
            }
        }

    def _compare_technicality(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”æŠ€æœ¯æ€§"""
        vocab_features = features.get("vocabulary_choice", {})
        professional_features = features.get("professionalism", {})

        technical_density = vocab_features.get("technical_density", 0)
        professional_score = professional_features.get("professional_score", 0)

        return {
            "technicality_level": "é«˜" if technical_density > 20 else "ä¸­" if technical_density > 10 else "ä½",
            "technical_score": round(technical_density, 1),
            "professional_score": round(professional_score, 1),
            "balance": "æŠ€æœ¯æ€§å¼º" if technical_density > professional_score else "é€šç”¨æ€§å¼º"
        }

    def _compare_objectivity(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”å®¢è§‚æ€§"""
        emotional_features = features.get("emotional_tone", {})
        expression_features = features.get("expression_style", {})

        emotional_intensity = emotional_features.get("intensity_score", 0)
        assertive_score = expression_features.get("assertive_score", 0)

        objectivity_score = max(0, 100 - emotional_intensity * 10 - assertive_score)

        return {
            "objectivity_level": "é«˜" if objectivity_score > 70 else "ä¸­" if objectivity_score > 40 else "ä½",
            "objectivity_score": round(objectivity_score, 1),
            "subjectivity_indicators": {
                "emotional_intensity": emotional_intensity,
                "assertive_tone": assertive_score
            }
        }

    def _compare_conciseness(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”ç®€æ´æ€§"""
        sentence_features = features.get("sentence_structure", {})
        vocab_features = features.get("vocabulary_choice", {})

        avg_length = sentence_features.get("average_length", 15)
        modifier_usage = vocab_features.get("modifier_usage", 0)

        conciseness_score = max(0, 100 - (avg_length - 15) * 2 - modifier_usage)

        return {
            "conciseness_level": "é«˜" if conciseness_score > 70 else "ä¸­" if conciseness_score > 40 else "ä½",
            "conciseness_score": round(conciseness_score, 1),
            "verbosity_indicators": {
                "sentence_length": avg_length,
                "modifier_density": modifier_usage
            }
        }

    def _generate_enhanced_style_prompt(self, features: Dict[str, Any], style_type: str, detailed_analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¢å¼ºçš„æ–‡é£æç¤ºè¯"""
        prompt_parts = []

        # åŸºç¡€é£æ ¼æè¿°
        style_info = self.style_types.get(style_type, {})
        style_name = style_info.get("name", "é€šç”¨é£æ ¼")
        characteristics = style_info.get("characteristics", [])

        prompt_parts.append(f"è¯·é‡‡ç”¨{style_name}è¿›è¡Œå†™ä½œï¼Œå…·ä½“ç‰¹å¾åŒ…æ‹¬ï¼š{', '.join(characteristics)}ã€‚")

        # å¥å¼è¦æ±‚
        sentence_features = features.get("sentence_structure", {})
        avg_length = sentence_features.get("average_length", 15)

        if avg_length > 25:
            prompt_parts.append("ä½¿ç”¨è¾ƒé•¿çš„å¤åˆå¥ï¼Œæ³¨é‡è¡¨è¾¾çš„å®Œæ•´æ€§å’Œé€»è¾‘æ€§ã€‚")
        elif avg_length < 15:
            prompt_parts.append("ä½¿ç”¨ç®€æ´æ˜äº†çš„çŸ­å¥ï¼Œçªå‡ºé‡ç‚¹ï¼Œä¾¿äºç†è§£ã€‚")
        else:
            prompt_parts.append("å¥å¼é•¿çŸ­é€‚ä¸­ï¼Œå…¼é¡¾è¡¨è¾¾å®Œæ•´æ€§å’Œå¯è¯»æ€§ã€‚")

        # è¯æ±‡è¦æ±‚
        vocab_features = features.get("vocabulary_choice", {})
        formality_score = vocab_features.get("formality_score", 0)
        technical_density = vocab_features.get("technical_density", 0)

        if formality_score > 20:
            prompt_parts.append("ä½¿ç”¨æ­£å¼ã€è§„èŒƒçš„è¯æ±‡ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾ã€‚")
        elif formality_score < 10:
            prompt_parts.append("å¯ä»¥ä½¿ç”¨ç›¸å¯¹è½»æ¾ã€è‡ªç„¶çš„è¡¨è¾¾æ–¹å¼ã€‚")

        if technical_density > 15:
            prompt_parts.append("é€‚å½“ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½“ç°ä¸“ä¸šæ€§ã€‚")

        # è¯­è°ƒè¦æ±‚
        tone_analysis = detailed_analysis.get("tone_analysis", {})
        dominant_tone = tone_analysis.get("dominant_tone", "neutral")

        if dominant_tone == "positive":
            prompt_parts.append("ä¿æŒç§¯ææ­£é¢çš„è¯­è°ƒï¼Œçªå‡ºä¼˜åŠ¿å’Œæˆæœã€‚")
        elif dominant_tone == "negative":
            prompt_parts.append("å®¢è§‚åˆ†æé—®é¢˜ï¼Œæå‡ºå»ºè®¾æ€§æ„è§ã€‚")
        else:
            prompt_parts.append("ä¿æŒå®¢è§‚ä¸­æ€§çš„è¯­è°ƒï¼Œæ³¨é‡äº‹å®é™ˆè¿°ã€‚")

        # ç»“æ„è¦æ±‚
        org_features = features.get("text_organization", {})
        connector_density = org_features.get("connector_density", 0)

        if connector_density > 10:
            prompt_parts.append("æ³¨é‡é€»è¾‘è¿æ¥ï¼Œä½¿ç”¨é€‚å½“çš„è¿‡æ¸¡è¯å’Œè¿æ¥è¯ã€‚")

        # å¯è¯»æ€§è¦æ±‚
        readability = detailed_analysis.get("readability_analysis", {})
        readability_level = readability.get("readability_level", "ä¸­ç­‰éš¾åº¦")

        prompt_parts.append(f"ç¡®ä¿æ–‡æœ¬{readability_level}ï¼Œé€‚åˆç›®æ ‡è¯»è€…ç¾¤ä½“ã€‚")

        return " ".join(prompt_parts)
    
    def _identify_style_type(self, content: str, features: Dict[str, Any]) -> Tuple[str, float]:
        """è¯†åˆ«æ–‡é£ç±»å‹"""
        scores = {}

        # å®‰å…¨è·å–ç‰¹å¾å€¼çš„è¾…åŠ©å‡½æ•°
        def safe_get(feature_dict: Dict, key: str, default: float = 0.0) -> float:
            return feature_dict.get(key, default) if feature_dict else default

        for style_id, style_info in self.style_types.items():
            try:
                score = 0.0

                # åŸºäºç‰¹å¾æ¨¡å¼åŒ¹é… (æƒé‡: 0.2)
                pattern_score = 0.0
                for pattern in style_info.get("typical_patterns", []):
                    if pattern in content:
                        pattern_score += 0.04  # æ¯ä¸ªæ¨¡å¼0.04åˆ†ï¼Œæœ€å¤š5ä¸ªæ¨¡å¼
                score += min(pattern_score, 0.2)

                # åŸºäºç‰¹å¾åˆ†æç»“æœ (æƒé‡: 0.8)
                sentence_features = features.get("sentence_structure", {})
                vocab_features = features.get("vocabulary_choice", {})
                expression_features = features.get("expression_style", {})
                org_features = features.get("text_organization", {})
                habit_features = features.get("language_habits", {})
                emotional_features = features.get("emotional_tone", {})
                professional_features = features.get("professionalism", {})

                if style_id == "formal_official":
                    # æ­£å¼å…¬æ–‡é£æ ¼
                    score += min(safe_get(vocab_features, "formality_score") * 0.02, 0.2)  # æ­£å¼åº¦
                    score += min(safe_get(org_features, "connector_density") * 0.01, 0.1)  # è¿æ¥è¯å¯†åº¦
                    score += min(safe_get(professional_features, "authority_indicators") * 0.02, 0.2)  # æƒå¨æ€§
                    score += min((1 - safe_get(habit_features, "colloquial_level") * 0.01), 0.1)  # éå£è¯­åŒ–
                    score += min(safe_get(expression_features, "passive_active_ratio") * 0.2, 0.2)  # è¢«åŠ¨è¯­æ€

                elif style_id == "business_professional":
                    # å•†åŠ¡ä¸“ä¸šé£æ ¼
                    score += min(safe_get(vocab_features, "action_verb_ratio") * 0.02, 0.2)  # åŠ¨ä½œåŠ¨è¯
                    score += min((1 - safe_get(expression_features, "passive_active_ratio")) * 0.3, 0.3)  # ä¸»åŠ¨è¯­æ€
                    score += min(safe_get(professional_features, "precision_level") * 0.02, 0.1)  # ç²¾ç¡®æ€§
                    score += min(safe_get(emotional_features, "neutral_ratio") * 0.2, 0.2)  # ä¸­æ€§è¯­è°ƒ

                elif style_id == "academic_research":
                    # å­¦æœ¯ç ”ç©¶é£æ ¼
                    score += min(safe_get(sentence_features, "complex_ratio") * 0.4, 0.4)  # å¤æ‚å¥æ¯”ä¾‹
                    score += min(safe_get(vocab_features, "technical_density") * 0.05, 0.2)  # æŠ€æœ¯å¯†åº¦
                    score += min(safe_get(professional_features, "authority_indicators") * 0.02, 0.2)  # æƒå¨æ€§

                elif style_id == "narrative_descriptive":
                    # å™è¿°æè¿°é£æ ¼
                    score += min(safe_get(vocab_features, "modifier_usage") * 0.02, 0.2)  # ä¿®é¥°è¯ä½¿ç”¨
                    score += min(safe_get(habit_features, "colloquial_level") * 0.02, 0.2)  # å£è¯­åŒ–ç¨‹åº¦
                    score += min(safe_get(emotional_features, "intensity") * 0.02, 0.2)  # æƒ…æ„Ÿå¼ºåº¦
                    score += min((safe_get(emotional_features, "positive_ratio") +
                                safe_get(emotional_features, "negative_ratio")) * 0.2, 0.2)  # æƒ…æ„Ÿè‰²å½©

                elif style_id == "concise_practical":
                    # ç®€æ´å®ç”¨é£æ ¼
                    avg_length = safe_get(sentence_features, "average_length", 20)
                    if avg_length < 20:
                        score += 0.3  # çŸ­å¥åå¥½
                    else:
                        score += max(0, 0.3 - (avg_length - 20) * 0.01)

                    score += min((1 - safe_get(habit_features, "de_structure_density") * 0.001) * 0.2, 0.2)
                    score += min((1 - safe_get(vocab_features, "modifier_usage") * 0.01) * 0.2, 0.2)
                    score += min(safe_get(vocab_features, "action_verb_ratio") * 0.01, 0.1)

                scores[style_id] = min(max(score, 0.0), 1.0)  # ç¡®ä¿åˆ†æ•°åœ¨0-1ä¹‹é—´

            except Exception as e:
                print(f"è®¡ç®—é£æ ¼ {style_id} åˆ†æ•°æ—¶å‡ºé”™: {str(e)}")
                scores[style_id] = 0.0

        # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„æ–‡é£ç±»å‹
        if not scores:
            return "business_professional", 0.5

        best_style = max(scores.items(), key=lambda x: x[1])

        # å¦‚æœæœ€é«˜åˆ†å¤ªä½ï¼Œè¿”å›é»˜è®¤é£æ ¼
        if best_style[1] < 0.3:
            return "business_professional", best_style[1]

        return best_style[0], best_style[1]
    
    def _generate_style_prompt(self, features: Dict[str, Any], style_type: str) -> str:
        """ç”Ÿæˆæ–‡é£æç¤ºè¯"""
        try:
            style_info = self.style_types.get(style_type, {})
            style_name = style_info.get("name", "é€šç”¨é£æ ¼")
            characteristics = style_info.get("characteristics", [])

            # åŸºç¡€æ–‡é£æè¿°
            prompt_parts = [
                f"è¯·æŒ‰ç…§{style_name}è¿›è¡Œå†…å®¹ç”Ÿæˆï¼Œå…·ä½“è¦æ±‚å¦‚ä¸‹ï¼š",
                "",
                "ã€æ–‡é£ç‰¹å¾ã€‘"
            ]

            for char in characteristics:
                prompt_parts.append(f"- {char}")

            # æ ¹æ®å…·ä½“ç‰¹å¾æ•°æ®ç”Ÿæˆä¸ªæ€§åŒ–è¦æ±‚
            prompt_parts.extend(self._generate_sentence_requirements(features))
            prompt_parts.extend(self._generate_vocabulary_requirements(features))
            prompt_parts.extend(self._generate_expression_requirements(features))
            prompt_parts.extend(self._generate_organization_requirements(features))
            prompt_parts.extend(self._generate_tone_requirements(features))

            prompt_parts.append("")
            prompt_parts.append("ã€ç‰¹åˆ«æ³¨æ„ã€‘")
            prompt_parts.append("- é¿å…AIç”Ÿæˆç—•è¿¹ï¼Œè®©å†…å®¹è‡ªç„¶æµç•…")
            prompt_parts.append("- ä¿æŒä¸åŸæ–‡æ¡£é£æ ¼çš„ä¸€è‡´æ€§")
            prompt_parts.append("- ç¡®ä¿å†…å®¹å‡†ç¡®ã€é€»è¾‘æ¸…æ™°")

            # æ·»åŠ è´Ÿé¢ç¤ºä¾‹å’Œé¿å…äº‹é¡¹
            prompt_parts.extend(self._generate_avoidance_guidelines(features, style_type))

            return "\n".join(prompt_parts)

        except Exception as e:
            # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸºç¡€æç¤ºè¯
            return f"è¯·æŒ‰ç…§{style_info.get('name', 'ä¸“ä¸š')}é£æ ¼è¿›è¡Œå†…å®¹ç”Ÿæˆï¼Œä¿æŒè¯­è¨€è§„èŒƒã€é€»è¾‘æ¸…æ™°ã€è¡¨è¾¾å‡†ç¡®ã€‚"

    def _generate_sentence_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå¥å¼è¦æ±‚"""
        requirements = ["", "ã€å¥å¼è¦æ±‚ã€‘"]

        sentence_features = features.get("sentence_structure", {})
        avg_length = sentence_features.get("average_length", 15)
        complex_ratio = sentence_features.get("complex_ratio", 0.5)

        # å¥é•¿è¦æ±‚
        if avg_length > 30:
            requirements.append("- ä¿æŒé•¿å¥çš„ä½¿ç”¨ï¼Œæ³¨é‡è¡¨è¾¾çš„å®Œæ•´æ€§å’Œé€»è¾‘å±‚æ¬¡")
            requirements.append("- é€‚å½“ä½¿ç”¨åˆ†å·å’Œç ´æŠ˜å·æ¥ç»„ç»‡å¤æ‚å¥å¼")
        elif avg_length > 20:
            requirements.append("- é€‚å½“ä½¿ç”¨é•¿å¥ï¼Œä¿æŒè¡¨è¾¾çš„å®Œæ•´æ€§å’Œé€»è¾‘æ€§")
            requirements.append("- é•¿çŸ­å¥ç»“åˆï¼Œé¿å…å¥å¼è¿‡äºå•è°ƒ")
        elif avg_length < 12:
            requirements.append("- å¤šä½¿ç”¨çŸ­å¥ï¼Œä¿æŒè¡¨è¾¾çš„ç®€æ´æ˜äº†")
            requirements.append("- é¿å…å†—é•¿å¤æ‚çš„å¥å¼ç»“æ„")
        else:
            requirements.append("- é•¿çŸ­å¥ç»“åˆï¼Œä¿æŒèŠ‚å¥æ„Ÿå’Œå¯è¯»æ€§")

        # å¤æ‚åº¦è¦æ±‚
        if complex_ratio > 0.7:
            requirements.append("- ä¿æŒå¥å¼çš„å¤æ‚æ€§å’Œå±‚æ¬¡æ„Ÿ")
        elif complex_ratio < 0.3:
            requirements.append("- ä½¿ç”¨ç®€å•ç›´æ¥çš„å¥å¼ç»“æ„")

        return requirements

    def _generate_vocabulary_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¯æ±‡è¦æ±‚"""
        requirements = ["", "ã€è¯æ±‡è¦æ±‚ã€‘"]

        vocab_features = features.get("vocabulary_choice", {})
        formality = vocab_features.get("formality_score", 0)
        technical_density = vocab_features.get("technical_density", 0)
        modifier_usage = vocab_features.get("modifier_usage", 0)
        action_verb_ratio = vocab_features.get("action_verb_ratio", 0)

        # æ­£å¼åº¦è¦æ±‚
        if formality > 15:
            requirements.append("- ä½¿ç”¨æ­£å¼ã€è§„èŒƒçš„ä¹¦é¢è¯­è¯æ±‡")
            requirements.append("- é€‚å½“ä½¿ç”¨ä¸“ä¸šæœ¯è¯­å’Œå­¦æœ¯è¡¨è¾¾")
        elif formality > 8:
            requirements.append("- ä½¿ç”¨æ ‡å‡†çš„ä¹¦é¢è¯­è¡¨è¾¾")
            requirements.append("- é¿å…è¿‡äºå£è¯­åŒ–çš„è¯æ±‡")
        else:
            requirements.append("- ä½¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯æ±‡")
            requirements.append("- é¿å…è¿‡äºæ­£å¼æˆ–ç”Ÿåƒ»çš„è¡¨è¾¾")

        # æŠ€æœ¯æ€§è¦æ±‚
        if technical_density > 5:
            requirements.append("- ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®ä½¿ç”¨")

        # ä¿®é¥°è¯è¦æ±‚
        if modifier_usage > 10:
            requirements.append("- é€‚å½“ä½¿ç”¨å½¢å®¹è¯å’Œå‰¯è¯è¿›è¡Œä¿®é¥°")
        elif modifier_usage < 3:
            requirements.append("- å‡å°‘ä¸å¿…è¦çš„ä¿®é¥°è¯ï¼Œä¿æŒè¡¨è¾¾ç®€æ´")

        # åŠ¨è¯ä½¿ç”¨è¦æ±‚
        if action_verb_ratio > 20:
            requirements.append("- å¤šä½¿ç”¨åŠ¨ä½œæ€§å¼ºçš„åŠ¨è¯")
            requirements.append("- è®©è¡¨è¾¾æ›´åŠ ç”ŸåŠ¨æœ‰åŠ›")

        return requirements

    def _generate_expression_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¡¨è¾¾æ–¹å¼è¦æ±‚"""
        requirements = ["", "ã€è¡¨è¾¾æ–¹å¼ã€‘"]

        expression_features = features.get("expression_style", {})
        passive_ratio = expression_features.get("passive_active_ratio", 0)
        person_usage = expression_features.get("person_usage", {})

        # è¯­æ€è¦æ±‚
        if passive_ratio > 0.6:
            requirements.append("- é€‚å½“ä½¿ç”¨è¢«åŠ¨è¯­æ€ï¼Œä½“ç°å®¢è§‚æ€§å’Œæ­£å¼æ€§")
        elif passive_ratio > 0.3:
            requirements.append("- ä¸»è¢«åŠ¨è¯­æ€ç»“åˆä½¿ç”¨ï¼Œä¿æŒè¡¨è¾¾çš„çµæ´»æ€§")
        else:
            requirements.append("- ä¼˜å…ˆä½¿ç”¨ä¸»åŠ¨è¯­æ€ï¼Œè®©è¡¨è¾¾æ›´ç›´æ¥æœ‰åŠ›")

        # äººç§°ä½¿ç”¨è¦æ±‚
        first_person = person_usage.get("first_person", 0)
        if first_person > 5:
            requirements.append("- å¯ä»¥é€‚å½“ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼Œä½“ç°ä¸»è§‚æ€åº¦")
        elif first_person == 0:
            requirements.append("- é¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼Œä¿æŒå®¢è§‚ä¸­ç«‹")

        return requirements

    def _generate_organization_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆç»„ç»‡ç»“æ„è¦æ±‚"""
        requirements = ["", "ã€ç»„ç»‡ç»“æ„ã€‘"]

        org_features = features.get("text_organization", {})
        connector_density = org_features.get("connector_density", 0)
        summary_usage = org_features.get("summary_usage", 0)

        # è¿æ¥è¯è¦æ±‚
        if connector_density > 8:
            requirements.append("- ä½¿ç”¨ä¸°å¯Œçš„é€»è¾‘è¿æ¥è¯")
            requirements.append("- ä¿æŒæ®µè½é—´çš„é€»è¾‘å…³ç³»æ¸…æ™°")
        elif connector_density > 3:
            requirements.append("- é€‚å½“ä½¿ç”¨é€»è¾‘è¿æ¥è¯")
            requirements.append("- æ³¨æ„æ®µè½é—´çš„è¿‡æ¸¡è‡ªç„¶")
        else:
            requirements.append("- å‡å°‘æœºæ¢°åŒ–çš„è¿‡æ¸¡è¯ä½¿ç”¨")
            requirements.append("- é€šè¿‡å†…å®¹é€»è¾‘è‡ªç„¶è¿‡æ¸¡")

        # æ€»ç»“æ€§è¡¨è¾¾
        if summary_usage > 2:
            requirements.append("- é€‚å½“ä½¿ç”¨æ€»ç»“æ€§è¡¨è¾¾")
            requirements.append("- æ³¨é‡å†…å®¹çš„å½’çº³å’Œæç‚¼")

        return requirements

    def _generate_tone_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¯­è°ƒè¦æ±‚"""
        requirements = ["", "ã€è¯­è°ƒé£æ ¼ã€‘"]

        emotional_features = features.get("emotional_tone", {})
        professional_features = features.get("professionalism", {})

        positive_ratio = emotional_features.get("positive_ratio", 0)
        negative_ratio = emotional_features.get("negative_ratio", 0)
        intensity = emotional_features.get("intensity", 0)
        authority_indicators = professional_features.get("authority_indicators", 0)

        # æƒ…æ„Ÿè‰²å½©è¦æ±‚
        if positive_ratio > 0.6:
            requirements.append("- ä¿æŒç§¯ææ­£é¢çš„è¯­è°ƒ")
        elif negative_ratio > 0.4:
            requirements.append("- å¯ä»¥é€‚å½“è¡¨è¾¾å…³åˆ‡å’Œé—®é¢˜æ„è¯†")
        else:
            requirements.append("- ä¿æŒä¸­æ€§å®¢è§‚çš„è¯­è°ƒ")

        # å¼ºåº¦è¦æ±‚
        if intensity > 15:
            requirements.append("- é€‚å½“ä½¿ç”¨å¼ºè°ƒè¯æ±‡ï¼Œä½“ç°é‡è¦æ€§")
        elif intensity < 5:
            requirements.append("- ä¿æŒå¹³å’Œçš„è¡¨è¾¾å¼ºåº¦")

        # æƒå¨æ€§è¦æ±‚
        if authority_indicators > 10:
            requirements.append("- ä½“ç°ä¸“ä¸šæƒå¨æ€§ï¼Œä½¿ç”¨å‡†ç¡®çš„æ•°æ®å’Œå¼•ç”¨")

        return requirements

    def _generate_avoidance_guidelines(self, features: Dict[str, Any], style_type: str) -> List[str]:
        """ç”Ÿæˆé¿å…äº‹é¡¹æŒ‡å¯¼"""
        guidelines = ["", "ã€é¿å…äº‹é¡¹ã€‘"]

        # æ ¹æ®é£æ ¼ç±»å‹æ·»åŠ ç‰¹å®šé¿å…äº‹é¡¹
        if style_type == "business_professional":
            guidelines.extend([
                "- é¿å…è¿‡äºæ„Ÿæ€§æˆ–ä¸»è§‚çš„è¡¨è¾¾",
                "- é¿å…ä½¿ç”¨ç½‘ç»œæµè¡Œè¯­æˆ–ä¿šè¯­",
                "- é¿å…å†—é•¿çš„ä¿®é¥°å’Œåä¸½çš„è¾è—»"
            ])
        elif style_type == "academic_research":
            guidelines.extend([
                "- é¿å…å£è¯­åŒ–è¡¨è¾¾å’Œéæ­£å¼ç”¨è¯",
                "- é¿å…ä¸»è§‚è‡†æ–­ï¼Œç¡®ä¿è®ºè¯ä¸¥è°¨",
                "- é¿å…è¿‡äºç»å¯¹çš„è¡¨è¿°"
            ])
        elif style_type == "concise_practical":
            guidelines.extend([
                "- é¿å…å†—ä½™å’Œé‡å¤è¡¨è¾¾",
                "- é¿å…è¿‡äºå¤æ‚çš„å¥å¼ç»“æ„",
                "- é¿å…ä¸å¿…è¦çš„ä¿®é¥°è¯æ±‡"
            ])

        # é€šç”¨é¿å…äº‹é¡¹
        guidelines.extend([
            "- é¿å…æ˜æ˜¾çš„AIç”Ÿæˆç—•è¿¹",
            "- é¿å…é€»è¾‘ä¸æ¸…æˆ–å‰åçŸ›ç›¾",
            "- é¿å…è¯­æ³•é”™è¯¯å’Œç”¨è¯ä¸å½“"
        ])

        return guidelines
    
    def _generate_template_id(self, document_name: str, features: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡é£æ¨¡æ¿ID"""
        content = f"{document_name}_{json.dumps(features, sort_keys=True)}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]
    
    def save_style_template(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿å­˜æ–‡é£æ¨¡æ¿"""
        try:
            template_id = analysis_result.get("template_id")
            # è‡ªåŠ¨è¡¥å…¨template_id
            if not template_id:
                template_id = self._generate_template_id(
                    analysis_result.get("document_name", "æœªå‘½åæ–‡æ¡£"),
                    analysis_result.get("style_features", {})
                )
                analysis_result["template_id"] = template_id
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            template_file = os.path.join(self.storage_path, f"{template_id}.json")
            with open(template_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°æ¨¡æ¿ç´¢å¼•
            self._update_style_template_index(template_id, analysis_result)
            
            return {
                "success": True,
                "template_id": template_id,
                "template_name": analysis_result.get("document_name", "æœªå‘½åæ¨¡æ¿"),
                "style_type": analysis_result.get("style_type", "æœªçŸ¥é£æ ¼"),
                "saved_path": template_file
            }
            
        except Exception as e:
            return {"error": f"ä¿å­˜æ–‡é£æ¨¡æ¿å¤±è´¥: {str(e)}"}
    
    def load_style_template(self, template_id: str) -> Dict[str, Any]:
        """åŠ è½½æ–‡é£æ¨¡æ¿"""
        try:
            template_file = os.path.join(self.storage_path, f"{template_id}.json")
            if not os.path.exists(template_file):
                return {"error": f"æ–‡é£æ¨¡æ¿ä¸å­˜åœ¨: {template_id}"}
            
            with open(template_file, 'r', encoding='utf-8') as f:
                template_data = json.load(f)
            
            return template_data
            
        except Exception as e:
            return {"error": f"åŠ è½½æ–‡é£æ¨¡æ¿å¤±è´¥: {str(e)}"}
    
    def list_style_templates(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ–‡é£æ¨¡æ¿"""
        try:
            index_file = os.path.join(self.storage_path, "style_template_index.json")
            if not os.path.exists(index_file):
                return []
            
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            return index_data.get("templates", [])
            
        except Exception as e:
            print(f"åŠ è½½æ–‡é£æ¨¡æ¿ç´¢å¼•å¤±è´¥: {str(e)}")
            return []
    
    def _update_style_template_index(self, template_id: str, template_data: Dict[str, Any]):
        """æ›´æ–°æ–‡é£æ¨¡æ¿ç´¢å¼•"""
        index_file = os.path.join(self.storage_path, "style_template_index.json")
        
        # è¯»å–ç°æœ‰ç´¢å¼•
        if os.path.exists(index_file):
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
        else:
            index_data = {"templates": []}
        
        # æ›´æ–°æˆ–æ·»åŠ æ¨¡æ¿ä¿¡æ¯
        template_info = {
            "template_id": template_id,
            "name": template_data.get("document_name", "æœªå‘½åæ¨¡æ¿"),
            "style_type": template_data.get("style_type", "æœªçŸ¥é£æ ¼"),
            "style_name": self.style_types.get(template_data.get("style_type", ""), {}).get("name", "æœªçŸ¥é£æ ¼"),
            "confidence_score": template_data.get("confidence_score", 0.0),
            "created_time": template_data.get("analysis_time", datetime.now().isoformat()),
            "description": f"æ–‡é£æ¨¡æ¿ï¼š{template_data.get('document_name', 'æœªå‘½åæ–‡æ¡£')}"
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_index = -1
        for i, template in enumerate(index_data["templates"]):
            if template["template_id"] == template_id:
                existing_index = i
                break
        
        if existing_index >= 0:
            index_data["templates"][existing_index] = template_info
        else:
            index_data["templates"].append(template_info)
        
        # ä¿å­˜ç´¢å¼•
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)

    def validate_style_application(self, original_content: str, generated_content: str,
                                 target_style: str, style_features: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é£æ ¼åº”ç”¨æ•ˆæœ"""
        try:
            # åˆ†æç”Ÿæˆå†…å®¹çš„é£æ ¼ç‰¹å¾
            generated_features = self._analyze_style_features(generated_content)

            # è¯†åˆ«ç”Ÿæˆå†…å®¹çš„é£æ ¼ç±»å‹
            identified_style, confidence = self._identify_style_type(generated_content, generated_features)

            # è®¡ç®—é£æ ¼ä¸€è‡´æ€§åˆ†æ•°
            consistency_score = self._calculate_style_consistency(
                style_features, generated_features, target_style
            )

            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            quality_metrics = self._calculate_quality_metrics(
                original_content, generated_content, generated_features
            )

            # ç”Ÿæˆæ”¹è¿›å»ºè®®
            improvement_suggestions = self._generate_improvement_suggestions(
                target_style, generated_features, consistency_score
            )

            return {
                "success": True,
                "target_style": target_style,
                "identified_style": identified_style,
                "style_confidence": confidence,
                "consistency_score": consistency_score,
                "quality_metrics": quality_metrics,
                "generated_features": generated_features,
                "improvement_suggestions": improvement_suggestions,
                "validation_time": datetime.now().isoformat()
            }

        except Exception as e:
            return {"error": f"é£æ ¼éªŒè¯å¤±è´¥: {str(e)}"}

    def _calculate_style_consistency(self, target_features: Dict[str, Any],
                                   generated_features: Dict[str, Any],
                                   target_style: str) -> float:
        """è®¡ç®—é£æ ¼ä¸€è‡´æ€§åˆ†æ•°"""
        try:
            total_score = 0.0
            total_weight = 0.0

            # å¥å¼ç»“æ„ä¸€è‡´æ€§ (æƒé‡: 0.25)
            sentence_score = self._compare_sentence_features(
                target_features.get("sentence_structure", {}),
                generated_features.get("sentence_structure", {})
            )
            total_score += sentence_score * 0.25
            total_weight += 0.25

            # è¯æ±‡é€‰æ‹©ä¸€è‡´æ€§ (æƒé‡: 0.3)
            vocab_score = self._compare_vocabulary_features(
                target_features.get("vocabulary_choice", {}),
                generated_features.get("vocabulary_choice", {})
            )
            total_score += vocab_score * 0.3
            total_weight += 0.3

            # è¡¨è¾¾æ–¹å¼ä¸€è‡´æ€§ (æƒé‡: 0.2)
            expression_score = self._compare_expression_features(
                target_features.get("expression_style", {}),
                generated_features.get("expression_style", {})
            )
            total_score += expression_score * 0.2
            total_weight += 0.2

            # ç»„ç»‡ç»“æ„ä¸€è‡´æ€§ (æƒé‡: 0.15)
            org_score = self._compare_organization_features(
                target_features.get("text_organization", {}),
                generated_features.get("text_organization", {})
            )
            total_score += org_score * 0.15
            total_weight += 0.15

            # è¯­è¨€ä¹ æƒ¯ä¸€è‡´æ€§ (æƒé‡: 0.1)
            habit_score = self._compare_habit_features(
                target_features.get("language_habits", {}),
                generated_features.get("language_habits", {})
            )
            total_score += habit_score * 0.1
            total_weight += 0.1

            return total_score / total_weight if total_weight > 0 else 0.0

        except Exception as e:
            print(f"è®¡ç®—é£æ ¼ä¸€è‡´æ€§æ—¶å‡ºé”™: {str(e)}")
            return 0.0

    def _compare_sentence_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒå¥å¼ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # å¹³å‡å¥é•¿æ¯”è¾ƒ
        target_length = target.get("average_length", 15)
        generated_length = generated.get("average_length", 15)
        length_diff = abs(target_length - generated_length) / max(target_length, 1)
        score += max(0, 1 - length_diff)
        comparisons += 1

        # å¤æ‚å¥æ¯”ä¾‹æ¯”è¾ƒ
        target_complex = target.get("complex_ratio", 0.5)
        generated_complex = generated.get("complex_ratio", 0.5)
        complex_diff = abs(target_complex - generated_complex)
        score += max(0, 1 - complex_diff * 2)
        comparisons += 1

        return score / comparisons if comparisons > 0 else 0.0

    def _compare_vocabulary_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒè¯æ±‡ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # æ­£å¼åº¦æ¯”è¾ƒ
        target_formality = target.get("formality_score", 0)
        generated_formality = generated.get("formality_score", 0)
        if target_formality > 0:
            formality_ratio = min(generated_formality / target_formality, target_formality / generated_formality)
            score += formality_ratio
            comparisons += 1

        # åŠ¨ä½œåŠ¨è¯æ¯”ä¾‹æ¯”è¾ƒ
        target_action = target.get("action_verb_ratio", 0)
        generated_action = generated.get("action_verb_ratio", 0)
        if target_action > 0:
            action_ratio = min(generated_action / target_action, target_action / generated_action)
            score += action_ratio
            comparisons += 1

        return score / comparisons if comparisons > 0 else 0.5

    def _compare_expression_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒè¡¨è¾¾æ–¹å¼ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # è¢«åŠ¨è¯­æ€æ¯”ä¾‹æ¯”è¾ƒ
        target_passive = target.get("passive_active_ratio", 0)
        generated_passive = generated.get("passive_active_ratio", 0)
        passive_diff = abs(target_passive - generated_passive)
        score += max(0, 1 - passive_diff * 2)
        comparisons += 1

        return score / comparisons if comparisons > 0 else 0.0

    def _compare_organization_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒç»„ç»‡ç»“æ„ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # è¿æ¥è¯å¯†åº¦æ¯”è¾ƒ
        target_connector = target.get("connector_density", 0)
        generated_connector = generated.get("connector_density", 0)
        if target_connector > 0:
            connector_ratio = min(generated_connector / target_connector, target_connector / generated_connector)
            score += connector_ratio
            comparisons += 1

        return score / comparisons if comparisons > 0 else 0.5

    def _compare_habit_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒè¯­è¨€ä¹ æƒ¯ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # å£è¯­åŒ–ç¨‹åº¦æ¯”è¾ƒ
        target_colloquial = target.get("colloquial_level", 0)
        generated_colloquial = generated.get("colloquial_level", 0)
        colloquial_diff = abs(target_colloquial - generated_colloquial)
        score += max(0, 1 - colloquial_diff * 0.1)
        comparisons += 1

        return score / comparisons if comparisons > 0 else 0.0

    def _calculate_quality_metrics(self, original_content: str, generated_content: str,
                                 generated_features: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        try:
            metrics = {}

            # é•¿åº¦æ¯”è¾ƒ
            original_length = len(original_content)
            generated_length = len(generated_content)
            metrics["length_ratio"] = generated_length / original_length if original_length > 0 else 0

            # å¥å­æ•°é‡æ¯”è¾ƒ
            original_sentences = generated_features.get("sentence_structure", {}).get("total_sentences", 0)
            metrics["sentence_count"] = original_sentences

            # å¯è¯»æ€§è¯„ä¼°ï¼ˆåŸºäºå¹³å‡å¥é•¿ï¼‰
            avg_length = generated_features.get("sentence_structure", {}).get("average_length", 15)
            if 15 <= avg_length <= 25:
                metrics["readability_score"] = 1.0
            elif 10 <= avg_length < 15 or 25 < avg_length <= 35:
                metrics["readability_score"] = 0.8
            else:
                metrics["readability_score"] = 0.6

            # è¯æ±‡ä¸°å¯Œåº¦ï¼ˆåŸºäºä¿®é¥°è¯ä½¿ç”¨ï¼‰
            modifier_usage = generated_features.get("vocabulary_choice", {}).get("modifier_usage", 0)
            metrics["vocabulary_richness"] = min(modifier_usage / 10, 1.0)

            # ä¸“ä¸šæ€§è¯„ä¼°
            professionalism = generated_features.get("professionalism", {})
            authority_score = professionalism.get("authority_indicators", 0)
            precision_score = professionalism.get("precision_level", 0)
            metrics["professionalism_score"] = min((authority_score + precision_score) / 20, 1.0)

            return metrics

        except Exception as e:
            print(f"è®¡ç®—è´¨é‡æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
            return {"error": str(e)}

    def _generate_improvement_suggestions(self, target_style: str, generated_features: Dict[str, Any],
                                        consistency_score: float) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        try:
            # åŸºäºä¸€è‡´æ€§åˆ†æ•°ç»™å‡ºæ€»ä½“å»ºè®®
            if consistency_score < 0.6:
                suggestions.append("æ•´ä½“é£æ ¼ä¸€è‡´æ€§è¾ƒä½ï¼Œå»ºè®®é‡æ–°è°ƒæ•´ç”Ÿæˆç­–ç•¥")
            elif consistency_score < 0.8:
                suggestions.append("é£æ ¼åŸºæœ¬ç¬¦åˆè¦æ±‚ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
            else:
                suggestions.append("é£æ ¼ä¸€è‡´æ€§è‰¯å¥½ï¼Œç¬¦åˆç›®æ ‡è¦æ±‚")

            # åŸºäºå…·ä½“ç‰¹å¾ç»™å‡ºå»ºè®®
            sentence_features = generated_features.get("sentence_structure", {})
            vocab_features = generated_features.get("vocabulary_choice", {})
            expression_features = generated_features.get("expression_style", {})

            # å¥å¼å»ºè®®
            avg_length = sentence_features.get("average_length", 15)
            if target_style == "concise_practical" and avg_length > 20:
                suggestions.append("å¥å­åé•¿ï¼Œå»ºè®®ä½¿ç”¨æ›´ç®€æ´çš„è¡¨è¾¾")
            elif target_style == "academic_research" and avg_length < 20:
                suggestions.append("å¥å­åçŸ­ï¼Œå»ºè®®å¢åŠ å¥å¼çš„å¤æ‚æ€§å’Œå®Œæ•´æ€§")

            # è¯æ±‡å»ºè®®
            formality = vocab_features.get("formality_score", 0)
            if target_style == "formal_official" and formality < 10:
                suggestions.append("æ­£å¼åº¦ä¸å¤Ÿï¼Œå»ºè®®ä½¿ç”¨æ›´è§„èŒƒçš„ä¹¦é¢è¯­")
            elif target_style == "narrative_descriptive" and formality > 15:
                suggestions.append("è¿‡äºæ­£å¼ï¼Œå»ºè®®ä½¿ç”¨æ›´ç”ŸåŠ¨è‡ªç„¶çš„è¡¨è¾¾")

            # è¯­æ€å»ºè®®
            passive_ratio = expression_features.get("passive_active_ratio", 0)
            if target_style == "business_professional" and passive_ratio > 0.4:
                suggestions.append("è¢«åŠ¨è¯­æ€ä½¿ç”¨è¿‡å¤šï¼Œå»ºè®®å¤šç”¨ä¸»åŠ¨è¯­æ€")

            # å¦‚æœæ²¡æœ‰å…·ä½“å»ºè®®ï¼Œç»™å‡ºé€šç”¨å»ºè®®
            if len(suggestions) <= 1:
                suggestions.append("ç»§ç»­ä¿æŒå½“å‰çš„å†™ä½œé£æ ¼")
                suggestions.append("æ³¨æ„è¯­è¨€çš„å‡†ç¡®æ€§å’Œé€»è¾‘æ€§")

        except Exception as e:
            suggestions = [f"ç”Ÿæˆæ”¹è¿›å»ºè®®æ—¶å‡ºé”™: {str(e)}"]

        return suggestions

    def export_styled_document(self, session_id: str) -> Dict[str, Any]:
        """
        å¯¼å‡ºåº”ç”¨äº†æ–‡é£å˜åŒ–çš„æ–‡æ¡£ï¼ˆå¢å¼ºï¼šä¿ç•™ç»“æ„ã€è‡ªåŠ¨å°é¢ã€ç›®å½•ã€æ‰¹æ³¨ã€é«˜äº®diffã€åŸæ ¼å¼ã€å…ƒæ•°æ®ã€å˜æ›´æŠ¥å‘Šï¼‰
        """
        try:
            # 1. è¯»å–ä¼šè¯æ•°æ®
            session_file = os.path.join(self.semantic_behavior_dir, "profiles", f"{session_id}.json")
            if not os.path.exists(session_file):
                return {"error": "ä¼šè¯ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"}
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            original_content = session_data.get("original_content", "")
            suggested_changes = session_data.get("suggested_changes", [])
            doc_name = session_data.get("document_name", f"session_{session_id}")
            template_id = session_data.get("style_template_id", "template")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # 2. åº”ç”¨å˜æ›´ï¼Œä¿ç•™ç»“æ„ï¼ŒæŒ‰diffé¡ºåºåˆæˆ
            from docx import Document
            from docx.shared import Pt, Inches, RGBColor
            from docx.enum.text import WD_ALIGN_PARAGRAPH
            from docx.oxml.ns import qn
            from docx.enum.style import WD_STYLE_TYPE
            from docx.oxml import OxmlElement
            import re
            doc = Document()
            # 2.1 å°é¢
            doc.add_section()
            cover = doc.add_paragraph()
            run = cover.add_run(f"æ–‡é£ç»Ÿä¸€å¯¼å‡ºæ–‡æ¡£\n\nåŸæ–‡ä»¶: {doc_name}\næ¨¡æ¿: {template_id}\nå¯¼å‡ºæ—¶é—´: {timestamp}")
            run.font.size = Pt(20)
            run.bold = True
            cover.alignment = WD_ALIGN_PARAGRAPH.CENTER
            doc.add_page_break()
            # 2.2 ç›®å½•
            toc = doc.add_paragraph("ç›®å½•", style='Heading 1')
            doc.add_paragraph("ï¼ˆè¯·åœ¨Wordä¸­æ›´æ–°ç›®å½•åŸŸä»¥æ˜¾ç¤ºç« èŠ‚ï¼‰")
            doc.add_page_break()
            # 2.3 æ­£æ–‡ï¼ˆä¿ç•™åŸç»“æ„ï¼Œåº”ç”¨å˜æ›´å¹¶é«˜äº®diffï¼‰
            paragraphs = original_content.split('\n')
            for para_text in paragraphs:
                if not para_text.strip():
                    doc.add_paragraph()
                    continue
                para = doc.add_paragraph()
                applied = False
                for change in suggested_changes:
                    if change.get("status") == "accepted" and change.get("original_text") in para_text:
                        before, match, after = para_text.partition(change["original_text"])
                        if before:
                            para.add_run(before)
                        run = para.add_run(change["suggested_text"])
                        # ç®€åŒ–é«˜äº®è®¾ç½®ï¼Œé¿å…å¯¼å…¥é—®é¢˜
                        run.font.color.rgb = RGBColor(255, 255, 0)  # é»„è‰²æ–‡å­—
                        comment = OxmlElement('w:commentRangeStart')
                        comment.set(qn('w:id'), '0')
                        para._p.append(comment)
                        run = para.add_run(f"ï¼ˆé£æ ¼å˜æ›´ï¼š{change.get('change_type','')}ï¼Œç½®ä¿¡åº¦{change.get('confidence',0):.2f}ï¼‰")
                        run.font.size = Pt(8)
                        run.font.color.rgb = RGBColor(255,0,0)
                        comment_end = OxmlElement('w:commentRangeEnd')
                        comment_end.set(qn('w:id'), '0')
                        para._p.append(comment_end)
                        if after:
                            para.add_run(after)
                        applied = True
                        break
                if not applied:
                    para.add_run(para_text)
            # 2.4 ç»Ÿä¸€æ ·å¼
            for section in doc.sections:
                section.top_margin = Inches(1)
                section.bottom_margin = Inches(1)
                section.left_margin = Inches(1.25)
                section.right_margin = Inches(1.25)
            # ç®€åŒ–æ ·å¼è®¾ç½®ï¼Œé¿å…å¤æ‚çš„å­—ä½“æ“ä½œ
            for style in doc.styles:
                if style.type == WD_STYLE_TYPE.PARAGRAPH:
                    style.font.name = 'å®‹ä½“'
                    style._element.rPr.rFonts.set(qn('w:eastAsia'), 'å®‹ä½“')
                    style.font.size = Pt(12)
            # 2.5 å…ƒæ•°æ®åµŒå…¥
            core_props = doc.core_properties
            core_props.title = f"æ–‡é£ç»Ÿä¸€å¯¼å‡º-{doc_name}"
            core_props.subject = f"é£æ ¼æ¨¡æ¿ID: {template_id}"
            core_props.author = "æ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹"
            core_props.comments = f"å¯¼å‡ºæ—¶é—´: {timestamp}ï¼›å˜æ›´æ•°: {len([c for c in suggested_changes if c.get('status') == 'accepted'])}"
            core_props.keywords = f"style_template_id:{template_id};export_time:{timestamp}"
            # 2.6 æ–‡æ¡£æœ«å°¾æ·»åŠ é£æ ¼è°ƒæ•´æŠ¥å‘Š
            doc.add_page_break()
            report = doc.add_paragraph("é£æ ¼è°ƒæ•´æŠ¥å‘Š", style='Heading 1')
            report.alignment = WD_ALIGN_PARAGRAPH.LEFT
            doc.add_paragraph(f"åŸæ–‡ä»¶å: {doc_name}")
            doc.add_paragraph(f"é£æ ¼æ¨¡æ¿ID: {template_id}")
            doc.add_paragraph(f"å¯¼å‡ºæ—¶é—´: {timestamp}")
            doc.add_paragraph(f"æ€»å˜æ›´æ•°: {len([c for c in suggested_changes if c.get('status') == 'accepted'])}")
            for idx, change in enumerate(suggested_changes, 1):
                if change.get("status") == "accepted":
                    para = doc.add_paragraph(f"[{idx}] ç±»å‹: {change.get('change_type','')} | ç½®ä¿¡åº¦: {change.get('confidence',0):.2f}", style='List Number')
                    para.add_run(f"\nåŸæ–‡: {change.get('original_text','')}")
                    para.add_run(f"\nå»ºè®®: {change.get('suggested_text','')}")
            # 2.7 æ–‡ä»¶å‘½å
            filename = f"{doc_name}_{template_id}_{timestamp}.docx"
            import tempfile
            temp_dir = tempfile.gettempdir()
            output_path = os.path.join(temp_dir, filename)
            doc.save(output_path)
            with open(output_path, 'rb') as f:
                docx_content = f.read()
            try:
                os.remove(output_path)
            except:
                pass
            return {
                "success": True,
                "docx_content": docx_content,
                "filename": filename,
                "content_length": len(paragraphs),
                "changes_applied": len([c for c in suggested_changes if c.get("status") == "accepted"])
            }
        except Exception as e:
            return {"error": f"å¯¼å‡ºæ–‡é£è°ƒæ•´æ–‡æ¡£å¤±è´¥: {str(e)}"}

    def generate_style_preview(self, analysis_result: dict, style_template_id: str) -> dict:
        """
        æ ¹æ®åˆ†æç»“æœå’Œé£æ ¼æ¨¡æ¿IDï¼Œå¯¹ç›®æ ‡æ–‡æ¡£è¿›è¡Œé£æ ¼è¿ç§»é¢„è§ˆï¼Œå¹¶è¿”å›è¿ç§»ç»“æœã€diffå’Œä¸€è‡´æ€§è¯„åˆ†ã€‚
        
        Args:
            analysis_result: æ–‡æ¡£åˆ†æç»“æœ
            style_template_id: é£æ ¼æ¨¡æ¿ID
            
        Returns:
            é£æ ¼é¢„è§ˆç»“æœï¼ŒåŒ…å«é¢„è§ˆæ–‡æœ¬ã€å·®å¼‚ã€ä¸€è‡´æ€§è¯„åˆ†ç­‰
        """
        try:
            # 1. è·å–ç›®æ ‡æ–‡æ¡£å†…å®¹
            document_content = analysis_result.get("document_content") or analysis_result.get("text") or ""
            document_name = analysis_result.get("document_name", "æœªå‘½åæ–‡æ¡£")

            # 2. åŠ è½½é£æ ¼æ¨¡æ¿ï¼ˆå¢å¼ºç‰ˆï¼Œå¸¦å›é€€æœºåˆ¶ï¼‰
            template = self._load_template_with_fallback(style_template_id, analysis_result)
            if not template:
                return {"error": "æ— æ³•åŠ è½½é£æ ¼æ¨¡æ¿ä¸”æ— æ³•åˆ›å»ºé»˜è®¤æ¨¡æ¿", "success": False}

            target_style_type = template.get("style_type") or template.get("styleType") or "business_professional"

            # 3. é€‰æ‹©å¢å¼ºå¤„ç†å™¨æˆ–è§„åˆ™å¼•æ“è¿›è¡Œé£æ ¼è¿ç§»
            if self.use_enhanced_features and self.enhanced_processor and hasattr(self.enhanced_processor, "transfer_style"):
                # ä½¿ç”¨å¢å¼ºé£æ ¼è¿ç§»
                transfer_result = self.enhanced_processor.transfer_style(
                    document_content=document_content,
                    target_style_type=target_style_type,
                    template=template,
                    original_analysis=analysis_result
                )
            else:
                # åŸºç¡€è§„åˆ™è¿ç§»ï¼ˆå¯è‡ªå®šä¹‰æ›´å¤æ‚é€»è¾‘ï¼‰
                transfer_result = {
                    "rewritten_text": document_content,  # è¿™é‡Œå¯è°ƒç”¨åŸºç¡€è¿ç§»æ–¹æ³•
                    "style_changes": [],
                    "success": True,
                    "note": "æœªå¯ç”¨å¢å¼ºé£æ ¼è¿ç§»ï¼Œè¿”å›åŸæ–‡"
                }

            # 4. è®¡ç®—é£æ ¼ä¸€è‡´æ€§åˆ†æ•°
            consistency_score = 0.0
            if "rewritten_text" in transfer_result:
                generated_features = self._analyze_style_features(transfer_result["rewritten_text"])
                target_features = template.get("style_features", {})
                consistency_score = self._calculate_style_consistency(target_features, generated_features, target_style_type)

            # 5. ç”Ÿæˆdiffï¼ˆå¯é€‰ï¼šå¯¹æ¯”è¿ç§»å‰åæ–‡æœ¬å·®å¼‚ï¼‰
            diff = []
            if "rewritten_text" in transfer_result and transfer_result["rewritten_text"] != document_content:
                import difflib
                diff = list(difflib.unified_diff(
                    document_content.splitlines(), 
                    transfer_result["rewritten_text"].splitlines(),
                    lineterm=""
                ))

            # 6. è¿”å›ç»“æ„åŒ–ç»“æœ
            return {
                "success": True,
                "preview_text": transfer_result.get("rewritten_text", document_content),
                "diff": diff,
                "consistency_score": consistency_score,
                "style_changes": transfer_result.get("style_changes", []),
                "note": transfer_result.get("note", ""),
                "template_id": style_template_id,
                "target_style_type": target_style_type
            }
        except Exception as e:
            return {"error": f"é£æ ¼è¿ç§»é¢„è§ˆå¤±è´¥: {str(e)}", "success": False}

    def _load_template_with_fallback(self, template_id: str, analysis_result: dict = None) -> Optional[dict]:
        """
        å¸¦å›é€€æœºåˆ¶çš„æ¨¡æ¿åŠ è½½
        
        Args:
            template_id: æ¨¡æ¿ID
            analysis_result: åˆ†æç»“æœï¼ˆç”¨äºåˆ›å»ºé»˜è®¤æ¨¡æ¿ï¼‰
            
        Returns:
            æ¨¡æ¿æ•°æ®æˆ–None
        """
        # 1. å°è¯•åŠ è½½æŒ‡å®šæ¨¡æ¿
        template = self.load_style_template(template_id)
        if template and "error" not in template:
            return template
        
        # 2. å¦‚æœæ¨¡æ¿ä¸å­˜åœ¨ä¸”æœ‰åˆ†æç»“æœï¼Œå°è¯•ä¿å­˜åˆ†æç»“æœä½œä¸ºæ¨¡æ¿
        if analysis_result:
            print(f"æ¨¡æ¿ {template_id} ä¸å­˜åœ¨ï¼Œå°è¯•ä¿å­˜åˆ†æç»“æœä½œä¸ºæ¨¡æ¿")
            save_result = self.save_style_template(analysis_result)
            if save_result.get("success"):
                print(f"æˆåŠŸä¿å­˜æ¨¡æ¿: {save_result.get('template_id')}")
                return analysis_result  # ä½¿ç”¨åˆ†æç»“æœä½œä¸ºæ¨¡æ¿
        
        # 3. å°è¯•æŸ¥æ‰¾ç›¸ä¼¼æ¨¡æ¿
        similar_template = self._find_similar_template(template_id)
        if similar_template:
            print(f"ä½¿ç”¨ç›¸ä¼¼æ¨¡æ¿: {similar_template.get('template_id', 'unknown')}")
            return similar_template
        
        # 4. ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        default_template = self._get_default_template()
        if default_template:
            print("ä½¿ç”¨é»˜è®¤æ¨¡æ¿")
            return default_template
        
        # 5. åˆ›å»ºåŸºç¡€æ¨¡æ¿
        basic_template = self._create_basic_template()
        if basic_template:
            print("åˆ›å»ºåŸºç¡€æ¨¡æ¿")
            return basic_template
        
        return None

    def _find_similar_template(self, template_id: str) -> Optional[dict]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼æ¨¡æ¿
        
        Args:
            template_id: ç›®æ ‡æ¨¡æ¿ID
            
        Returns:
            ç›¸ä¼¼æ¨¡æ¿æˆ–None
        """
        try:
            templates = self.list_style_templates()
            if not templates:
                return None
            
            # ç®€å•çš„ç›¸ä¼¼æ€§æŸ¥æ‰¾ï¼ˆåŸºäºIDå‰ç¼€ï¼‰
            template_prefix = template_id[:8] if len(template_id) >= 8 else template_id
            
            for template in templates:
                current_id = template.get("template_id", "")
                if current_id.startswith(template_prefix):
                    return self.load_style_template(current_id)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼IDï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡æ¿
            if templates:
                first_template_id = templates[0].get("template_id")
                if first_template_id:
                    return self.load_style_template(first_template_id)
            
            return None
            
        except Exception as e:
            print(f"æŸ¥æ‰¾ç›¸ä¼¼æ¨¡æ¿å¤±è´¥: {str(e)}")
            return None

    def _get_default_template(self) -> Optional[dict]:
        """
        è·å–é»˜è®¤æ¨¡æ¿
        
        Returns:
            é»˜è®¤æ¨¡æ¿æˆ–None
        """
        try:
            # åˆ›å»ºé»˜è®¤çš„å•†åŠ¡ä¸“ä¸šé£æ ¼æ¨¡æ¿
            default_template = {
                "template_id": "default_business_professional",
                "document_name": "é»˜è®¤å•†åŠ¡ä¸“ä¸šæ¨¡æ¿",
                "style_type": "business_professional",
                "style_features": {
                    "formality": 0.8,
                    "technicality": 0.6,
                    "objectivity": 0.7,
                    "conciseness": 0.5,
                    "professionalism": 0.8
                },
                "style_prompt": """
è¯·ä½¿ç”¨å•†åŠ¡ä¸“ä¸šçš„å†™ä½œé£æ ¼ï¼Œè¦æ±‚ï¼š
1. è¯­è¨€æ­£å¼ã€å®¢è§‚ï¼Œé¿å…ä¸»è§‚è‰²å½©
2. ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½†ä¿æŒæ˜“æ‡‚
3. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¸¥å¯†
4. é¿å…å£è¯­åŒ–è¡¨è¾¾
5. ä¿æŒç®€æ´æ˜äº†
                """.strip(),
                "confidence_score": 0.9,
                "created_time": datetime.now().isoformat()
            }
            
            return default_template
            
        except Exception as e:
            print(f"è·å–é»˜è®¤æ¨¡æ¿å¤±è´¥: {str(e)}")
            return None

    def _create_basic_template(self) -> Optional[dict]:
        """
        åˆ›å»ºåŸºç¡€æ¨¡æ¿
        
        Returns:
            åŸºç¡€æ¨¡æ¿æˆ–None
        """
        try:
            basic_template = {
                "template_id": "basic_template",
                "document_name": "åŸºç¡€æ¨¡æ¿",
                "style_type": "general",
                "style_features": {
                    "formality": 0.5,
                    "technicality": 0.3,
                    "objectivity": 0.6,
                    "conciseness": 0.5,
                    "professionalism": 0.5
                },
                "style_prompt": """
è¯·ä½¿ç”¨é€šç”¨çš„å†™ä½œé£æ ¼ï¼Œè¦æ±‚ï¼š
1. è¯­è¨€æ¸…æ™°æ˜“æ‡‚
2. ç»“æ„åˆç†
3. è¡¨è¾¾å‡†ç¡®
4. é¿å…è¿‡äºå¤æ‚çš„å¥å¼
                """.strip(),
                "confidence_score": 0.7,
                "created_time": datetime.now().isoformat()
            }
            
            return basic_template
            
        except Exception as e:
            print(f"åˆ›å»ºåŸºç¡€æ¨¡æ¿å¤±è´¥: {str(e)}")
            return None

    def handle_batch_style_changes(self, session_id: str, action: str = "accept_all") -> dict:
        """
        æ‰¹é‡å¤„ç†é£æ ¼å˜åŒ–ï¼ˆå¦‚å…¨éƒ¨æ¥å—/å…¨éƒ¨æ‹’ç»ï¼‰- çœŸå®å®ç°
        """
        try:
            # 1. ä»sessionæ–‡ä»¶ä¸­è¯»å–çœŸå®çš„åŸå§‹æ–‡æ¡£å†…å®¹å’Œé£æ ¼æ¨¡æ¿
            session_file = os.path.join(self.semantic_behavior_dir, "profiles", f"{session_id}.json")
            if not os.path.exists(session_file):
                return {
                    "success": False,
                    "error": f"ä¼šè¯æ–‡ä»¶ä¸å­˜åœ¨: {session_id}"
                }
            
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            
            # è·å–çœŸå®çš„åŸå§‹æ–‡æ¡£å†…å®¹
            original_content = session_data.get("original_content", "")
            document_name = session_data.get("document_name", f"session_{session_id}")
            style_template_id = session_data.get("style_template_id", "")
            
            if not original_content:
                return {
                    "success": False,
                    "error": "sessionæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ–‡æ¡£å†…å®¹"
                }
            
            if not style_template_id:
                return {
                    "success": False,
                    "error": "sessionæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°é£æ ¼æ¨¡æ¿ID"
                }
            
            # 2. åŠ è½½é£æ ¼æ¨¡æ¿
            template = self.load_style_template(style_template_id)
            if not template:
                template = {
                    "style_type": "business_professional",
                    "style_features": {
                        "formality": 0.8,
                        "technicality": 0.6,
                        "objectivity": 0.7,
                        "conciseness": 0.5
                    }
                }
            
            # 3. æ ¹æ®actionæ‰§è¡ŒçœŸå®çš„é£æ ¼è¿ç§»
            if action == "accept_all":
                # ä½¿ç”¨LLMè¿›è¡ŒçœŸå®çš„é£æ ¼è¿ç§»
                migrated_content = self._perform_real_style_migration(
                    original_content, template, style_template_id
                )
                
                # ç”ŸæˆçœŸå®çš„å˜æ›´è®°å½•
                suggested_changes = self._generate_real_changes(
                    original_content, migrated_content, template
                )
                
                # æ ‡è®°æ‰€æœ‰å˜æ›´ä¸ºå·²æ¥å—
                for change in suggested_changes:
                    change["status"] = "accepted"
                    
            elif action == "reject_all":
                # æ‹’ç»æ‰€æœ‰å˜æ›´ï¼Œä¿æŒåŸæ–‡
                migrated_content = original_content
                suggested_changes = []
                
            else:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}"
                }
            
            # 4. æ›´æ–°sessionæ•°æ®
            session_data.update({
                "action": action,
                "migrated_content": migrated_content,
                "suggested_changes": suggested_changes,
                "target_style": template.get("style_type", "business_professional"),
                "last_updated": datetime.now().isoformat()
            })
            
            # ä¿å­˜æ›´æ–°åçš„sessionæ–‡ä»¶
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
            
            print(f"ä¼šè¯æ•°æ®å·²ä¿å­˜: {session_file}")
            
            return {
                "success": True,
                "session_id": session_id,
                "action": action,
                "message": f"æ‰¹é‡{action}é£æ ¼å˜åŒ–å·²å¤„ç†",
                "changes_count": len(suggested_changes),
                "accepted_count": len([c for c in suggested_changes if c.get("status") == "accepted"]),
                "migrated_content_length": len(migrated_content)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"æ‰¹é‡å¤„ç†é£æ ¼å˜åŒ–å¤±è´¥: {str(e)}"
            }
    
    def _perform_real_style_migration(self, original_content: str, template: dict, template_id: str) -> str:
        """
        æ‰§è¡ŒçœŸå®çš„é£æ ¼è¿ç§»
        """
        try:
            # 1. æ„å»ºé£æ ¼è¿ç§»æç¤ºè¯
            target_style = template.get("style_type", "business_professional")
            style_features = template.get("style_features", {})
            
            prompt = self._build_style_migration_prompt(
                original_content, target_style, style_features
            )
            
            # 2. è°ƒç”¨LLMè¿›è¡Œé£æ ¼è¿ç§»
            if self.llm_client:
                response = self.llm_client.generate_text(prompt, max_tokens=2000)
                if response and "content" in response:
                    migrated_content = response["content"].strip()
                    # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
                    if migrated_content.startswith("```"):
                        lines = migrated_content.split('\n')
                        if len(lines) > 2:
                            migrated_content = '\n'.join(lines[1:-1])
                    return migrated_content
            
            # 3. å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŸºç¡€è¿ç§»
            return self._rule_based_style_migration(original_content, target_style, style_features)
            
        except Exception as e:
            print(f"é£æ ¼è¿ç§»å¤±è´¥: {e}")
            return original_content
    
    def _build_style_migration_prompt(self, content: str, target_style: str, style_features: dict) -> str:
        """
        æ„å»ºé£æ ¼è¿ç§»çš„LLMæç¤ºè¯
        """
        formality = style_features.get("formality", 0.5)
        technicality = style_features.get("technicality", 0.5)
        objectivity = style_features.get("objectivity", 0.5)
        conciseness = style_features.get("conciseness", 0.5)
        
        prompt = f"""
è¯·å°†ä»¥ä¸‹æ–‡æ¡£çš„é£æ ¼è°ƒæ•´ä¸º{target_style}é£æ ¼ï¼Œè¦æ±‚ï¼š

é£æ ¼ç‰¹å¾ï¼š
- æ­£å¼ç¨‹åº¦ï¼š{formality:.1f}ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šæ­£å¼ï¼‰
- æŠ€æœ¯æ€§ï¼š{technicality:.1f}ï¼ˆ0-1ï¼Œè¶Šé«˜è¶ŠæŠ€æœ¯åŒ–ï¼‰
- å®¢è§‚æ€§ï¼š{objectivity:.1f}ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå®¢è§‚ï¼‰
- ç®€æ´æ€§ï¼š{conciseness:.1f}ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šç®€æ´ï¼‰

è°ƒæ•´è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„æ ¸å¿ƒä¿¡æ¯å’Œé€»è¾‘ç»“æ„
2. è°ƒæ•´è¯æ±‡é€‰æ‹©ï¼Œä½¿å…¶ç¬¦åˆç›®æ ‡é£æ ¼
3. ä¼˜åŒ–å¥å¼ç»“æ„ï¼Œæé«˜è¡¨è¾¾çš„ä¸“ä¸šæ€§
4. ç¡®ä¿è¯­è¨€çš„ä¸€è‡´æ€§å’Œè¿è´¯æ€§

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¿”å›è°ƒæ•´åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ ‡è®°ã€‚
"""
        return prompt
    
    def _rule_based_style_migration(self, content: str, target_style: str, style_features: dict) -> str:
        """
        åŸºäºè§„åˆ™çš„é£æ ¼è¿ç§»ï¼ˆLLMä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰
        """
        migrated_content = content
        
        # æ ¹æ®ç›®æ ‡é£æ ¼åº”ç”¨ä¸åŒçš„è§„åˆ™
        if target_style == "business_professional":
            # å•†åŠ¡ä¸“ä¸šé£æ ¼è°ƒæ•´
            replacements = {
                "æˆ‘è§‰å¾—": "æˆ‘è®¤ä¸º",
                "æŒºå¥½çš„": "è¾ƒä¸ºç†æƒ³",
                "åº”è¯¥å¯ä»¥": "èƒ½å¤Ÿ",
                "è§£å†³é—®é¢˜": "è§£å†³ç›¸å…³é—®é¢˜",
                "ç”¨äº†": "é‡‡ç”¨äº†",
                "ç®—äº†ä¸€ä¸‹": "è¿›è¡Œäº†åˆ†æ",
                "æ€»çš„æ¥è¯´": "ç»¼ä¸Šæ‰€è¿°",
                "ä¸é”™": "è‰¯å¥½",
                "åº”è¯¥èƒ½ç”¨": "å…·å¤‡å¯è¡Œæ€§"
            }
            
            for old, new in replacements.items():
                migrated_content = migrated_content.replace(old, new)
        
        elif target_style == "academic":
            # å­¦æœ¯é£æ ¼è°ƒæ•´
            replacements = {
                "æˆ‘è§‰å¾—": "ç ”ç©¶è¡¨æ˜",
                "æŒºå¥½çš„": "å…·æœ‰ç§¯ææ•ˆæœ",
                "åº”è¯¥å¯ä»¥": "èƒ½å¤Ÿæœ‰æ•ˆ",
                "è§£å†³é—®é¢˜": "è§£å†³ç›¸å…³é—®é¢˜",
                "ç”¨äº†": "é‡‡ç”¨äº†",
                "ç®—äº†ä¸€ä¸‹": "è¿›è¡Œäº†ç»Ÿè®¡åˆ†æ",
                "æ€»çš„æ¥è¯´": "ç»¼ä¸Šæ‰€è¿°",
                "ä¸é”™": "è¡¨ç°è‰¯å¥½",
                "åº”è¯¥èƒ½ç”¨": "å…·å¤‡åº”ç”¨ä»·å€¼"
            }
            
            for old, new in replacements.items():
                migrated_content = migrated_content.replace(old, new)
        
        return migrated_content
    
    def _generate_real_changes(self, original_content: str, migrated_content: str, template: dict) -> list:
        """
        ç”ŸæˆçœŸå®çš„å˜æ›´è®°å½•
        """
        changes = []
        
        # ä½¿ç”¨difflibç”Ÿæˆå·®å¼‚
        import difflib
        
        # ç®€å•çš„è¯çº§åˆ«å·®å¼‚æ£€æµ‹
        original_words = original_content.split()
        migrated_words = migrated_content.split()
        
        matcher = difflib.SequenceMatcher(None, original_words, migrated_words)
        
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'replace':
                original_text = ' '.join(original_words[i1:i2])
                suggested_text = ' '.join(migrated_words[j1:j2])
                
                if original_text.strip() and suggested_text.strip():
                    change = {
                        "original_text": original_text,
                        "suggested_text": suggested_text,
                        "status": "accepted",
                        "change_type": self._classify_change_type(original_text, suggested_text),
                        "confidence": 0.85,
                        "position": i1
                    }
                    changes.append(change)
        
        return changes
    
    def _classify_change_type(self, original: str, suggested: str) -> str:
        """
        åˆ†ç±»å˜æ›´ç±»å‹
        """
        if len(original) < len(suggested):
            return "vocabulary_improvement"
        elif len(original) > len(suggested):
            return "conciseness_improvement"
        else:
            return "style_alignment"

    def handle_style_change(self, session_id: str, change_id: str, action: str) -> dict:
        """
        å¤„ç†å•ä¸ªé£æ ¼å˜åŒ–ï¼ˆæ¥å—/æ‹’ç»ï¼‰
        
        Args:
            session_id: ä¼šè¯ID
            change_id: å˜åŒ–ID
            action: æ“ä½œç±»å‹ ('accept' æˆ– 'reject')
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        try:
            # 1. éªŒè¯å‚æ•°
            if action not in ['accept', 'reject']:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}ï¼Œå¿…é¡»æ˜¯ 'accept' æˆ– 'reject'"
                }
            
            # 2. ä»sessionæ–‡ä»¶ä¸­è¯»å–æ•°æ®
            session_file = os.path.join(self.semantic_behavior_dir, "profiles", f"{session_id}.json")
            if not os.path.exists(session_file):
                return {
                    "success": False,
                    "error": f"ä¼šè¯æ–‡ä»¶ä¸å­˜åœ¨: {session_id}"
                }
            
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            
            # 3. è·å–åŸå§‹å†…å®¹å’Œå»ºè®®çš„å˜åŒ–
            original_content = session_data.get("original_content", "")
            suggested_changes = session_data.get("suggested_changes", [])
            
            if not original_content:
                return {
                    "success": False,
                    "error": "sessionæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ–‡æ¡£å†…å®¹"
                }
            
            # 4. æŸ¥æ‰¾æŒ‡å®šçš„å˜åŒ–
            target_change = None
            for change in suggested_changes:
                if change.get("change_id") == change_id:
                    target_change = change
                    break
            
            if not target_change:
                return {
                    "success": False,
                    "error": f"æœªæ‰¾åˆ°æŒ‡å®šçš„å˜åŒ–: {change_id}"
                }
            
            # 5. æ›´æ–°å˜åŒ–çŠ¶æ€
            target_change["status"] = action
            target_change["action_time"] = datetime.now().isoformat()
            
            # 6. ç”Ÿæˆæ›´æ–°åçš„é¢„è§ˆå†…å®¹
            updated_preview = self._generate_updated_preview(original_content, suggested_changes)
            
            # 7. æ›´æ–°sessionæ•°æ®
            session_data.update({
                "suggested_changes": suggested_changes,
                "updated_preview": updated_preview,
                "last_updated": datetime.now().isoformat()
            })
            
            # 8. ä¿å­˜æ›´æ–°åçš„sessionæ–‡ä»¶
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
            
            print(f"å•ä¸ªé£æ ¼å˜åŒ–å·²å¤„ç†: {change_id} -> {action}")
            
            return {
                "success": True,
                "change_id": change_id,
                "action": action,
                "message": f"å˜åŒ– {change_id} å·²{action}",
                "updated_preview": {
                    "content": updated_preview,
                    "accepted_changes": len([c for c in suggested_changes if c.get("status") == "accepted"]),
                    "rejected_changes": len([c for c in suggested_changes if c.get("status") == "rejected"]),
                    "pending_changes": len([c for c in suggested_changes if c.get("status") == "pending"])
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"å¤„ç†å•ä¸ªé£æ ¼å˜åŒ–å¤±è´¥: {str(e)}"
            }
    
    def _generate_updated_preview(self, original_content: str, suggested_changes: list) -> str:
        """
        æ ¹æ®å·²æ¥å—çš„å˜åŒ–ç”Ÿæˆæ›´æ–°åçš„é¢„è§ˆå†…å®¹
        
        Args:
            original_content: åŸå§‹å†…å®¹
            suggested_changes: å»ºè®®çš„å˜åŒ–åˆ—è¡¨
            
        Returns:
            str: æ›´æ–°åçš„é¢„è§ˆå†…å®¹
        """
        try:
            # æŒ‰ä½ç½®æ’åºå˜åŒ–ï¼Œç¡®ä¿æŒ‰é¡ºåºåº”ç”¨
            accepted_changes = [c for c in suggested_changes if c.get("status") == "accepted"]
            accepted_changes.sort(key=lambda x: x.get("position", {}).get("start", 0))
            
            # ä»åå¾€å‰åº”ç”¨å˜åŒ–ï¼Œé¿å…ä½ç½®åç§»
            updated_content = original_content
            offset = 0
            
            for change in accepted_changes:
                position = change.get("position", {})
                start = position.get("start", 0) + offset
                end = position.get("end", 0) + offset
                suggested_text = change.get("suggested_text", "")
                
                # åº”ç”¨å˜åŒ–
                if start < len(updated_content) and end <= len(updated_content):
                    updated_content = updated_content[:start] + suggested_text + updated_content[end:]
                    # æ›´æ–°åç§»é‡
                    offset += len(suggested_text) - (end - start)
            
            return updated_content
            
        except Exception as e:
            print(f"ç”Ÿæˆæ›´æ–°é¢„è§ˆå¤±è´¥: {e}")
            return original_content

    def apply_style_changes(self, session_id: str, changes: List[Dict[str, Any]]) -> dict:
        """
        åº”ç”¨é£æ ¼å˜åŒ–åˆ°æ–‡æ¡£
        
        Args:
            session_id: ä¼šè¯ID
            changes: è¦åº”ç”¨çš„å˜åŒ–åˆ—è¡¨
            
        Returns:
            Dict: åº”ç”¨ç»“æœ
        """
        try:
            # 1. éªŒè¯å‚æ•°
            if not changes:
                return {
                    "success": False,
                    "error": "æ²¡æœ‰æä¾›è¦åº”ç”¨çš„å˜åŒ–"
                }
            
            # 2. ä»sessionæ–‡ä»¶ä¸­è¯»å–æ•°æ®
            session_file = os.path.join(self.semantic_behavior_dir, "profiles", f"{session_id}.json")
            if not os.path.exists(session_file):
                return {
                    "success": False,
                    "error": f"ä¼šè¯æ–‡ä»¶ä¸å­˜åœ¨: {session_id}"
                }
            
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            
            # 3. è·å–åŸå§‹æ–‡æ¡£å†…å®¹
            original_content = session_data.get("original_content", "")
            if not original_content:
                return {
                    "success": False,
                    "error": "ä¼šè¯ä¸­æ²¡æœ‰åŸå§‹æ–‡æ¡£å†…å®¹"
                }
            
            # 4. åº”ç”¨å˜åŒ–
            updated_content = original_content
            applied_changes = []
            failed_changes = []
            
            for change in changes:
                change_id = change.get("id")
                change_type = change.get("type")
                change_data = change.get("data", {})
                
                try:
                    if change_type == "text_replacement":
                        result = self._apply_text_replacement(updated_content, change_data)
                        if result["success"]:
                            updated_content = result["updated_content"]
                            applied_changes.append({
                                "id": change_id,
                                "type": change_type,
                                "status": "applied"
                            })
                        else:
                            failed_changes.append({
                                "id": change_id,
                                "type": change_type,
                                "error": result["error"]
                            })
                    
                    elif change_type == "format_change":
                        result = self._apply_format_change(updated_content, change_data)
                        if result["success"]:
                            updated_content = result["updated_content"]
                            applied_changes.append({
                                "id": change_id,
                                "type": change_type,
                                "status": "applied"
                            })
                        else:
                            failed_changes.append({
                                "id": change_id,
                                "type": change_type,
                                "error": result["error"]
                            })
                    
                    elif change_type == "style_adjustment":
                        result = self._apply_style_adjustment(updated_content, change_data)
                        if result["success"]:
                            updated_content = result["updated_content"]
                            applied_changes.append({
                                "id": change_id,
                                "type": change_type,
                                "status": "applied"
                            })
                        else:
                            failed_changes.append({
                                "id": change_id,
                                "type": change_type,
                                "error": result["error"]
                            })
                    
                    else:
                        failed_changes.append({
                            "id": change_id,
                            "type": change_type,
                            "error": f"ä¸æ”¯æŒçš„å˜åŒ–ç±»å‹: {change_type}"
                        })
                
                except Exception as e:
                    failed_changes.append({
                        "id": change_id,
                        "type": change_type,
                        "error": f"åº”ç”¨å˜åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
                    })
            
            # 5. æ›´æ–°sessionæ•°æ®
            session_data["updated_content"] = updated_content
            session_data["applied_changes"] = applied_changes
            session_data["failed_changes"] = failed_changes
            session_data["last_updated"] = datetime.now().isoformat()
            
            # 6. ä¿å­˜æ›´æ–°åçš„session
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
            
            # 7. ç”Ÿæˆåº”ç”¨æŠ¥å‘Š
            application_report = self._generate_application_report(applied_changes, failed_changes, changes)
            
            return {
                "success": True,
                "session_id": session_id,
                "applied_changes_count": len(applied_changes),
                "failed_changes_count": len(failed_changes),
                "total_changes_count": len(changes),
                "application_report": application_report,
                "updated_content_preview": updated_content[:500] + "..." if len(updated_content) > 500 else updated_content,
                "applied_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"åº”ç”¨é£æ ¼å˜åŒ–å¤±è´¥: {str(e)}"
            }
    
    def _apply_text_replacement(self, content: str, change_data: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨æ–‡æœ¬æ›¿æ¢"""
        try:
            old_text = change_data.get("old_text", "")
            new_text = change_data.get("new_text", "")
            
            if not old_text:
                return {
                    "success": False,
                    "error": "ç¼ºå°‘è¦æ›¿æ¢çš„æ–‡æœ¬"
                }
            
            if old_text not in content:
                return {
                    "success": False,
                    "error": f"åœ¨æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°æ–‡æœ¬: {old_text[:50]}..."
                }
            
            updated_content = content.replace(old_text, new_text)
            
            return {
                "success": True,
                "updated_content": updated_content,
                "replacement_count": content.count(old_text)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"æ–‡æœ¬æ›¿æ¢å¤±è´¥: {str(e)}"
            }
    
    def _apply_format_change(self, content: str, change_data: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨æ ¼å¼å˜åŒ–"""
        try:
            format_type = change_data.get("format_type")
            target_text = change_data.get("target_text", "")
            new_format = change_data.get("new_format", {})
            
            if not target_text or not format_type:
                return {
                    "success": False,
                    "error": "ç¼ºå°‘æ ¼å¼å˜åŒ–çš„ç›®æ ‡æ–‡æœ¬æˆ–æ ¼å¼ç±»å‹"
                }
            
            if target_text not in content:
                return {
                    "success": False,
                    "error": f"åœ¨æ–‡æ¡£ä¸­æ‰¾ä¸åˆ°ç›®æ ‡æ–‡æœ¬: {target_text[:50]}..."
                }
            
            # æ ¹æ®æ ¼å¼ç±»å‹åº”ç”¨ä¸åŒçš„æ ¼å¼
            if format_type == "bold":
                updated_content = content.replace(target_text, f"**{target_text}**")
            elif format_type == "italic":
                updated_content = content.replace(target_text, f"*{target_text}*")
            elif format_type == "underline":
                updated_content = content.replace(target_text, f"__{target_text}__")
            elif format_type == "highlight":
                updated_content = content.replace(target_text, f"=={target_text}==")
            else:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹: {format_type}"
                }
            
            return {
                "success": True,
                "updated_content": updated_content,
                "format_type": format_type
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"æ ¼å¼å˜åŒ–å¤±è´¥: {str(e)}"
            }
    
    def _apply_style_adjustment(self, content: str, change_data: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨é£æ ¼è°ƒæ•´"""
        try:
            adjustment_type = change_data.get("adjustment_type")
            target_section = change_data.get("target_section", "")
            adjustment_data = change_data.get("adjustment_data", {})
            
            if not adjustment_type:
                return {
                    "success": False,
                    "error": "ç¼ºå°‘é£æ ¼è°ƒæ•´ç±»å‹"
                }
            
            # æ ¹æ®è°ƒæ•´ç±»å‹åº”ç”¨ä¸åŒçš„é£æ ¼è°ƒæ•´
            if adjustment_type == "tone_adjustment":
                # è°ƒæ•´è¯­æ°”
                tone = adjustment_data.get("tone", "neutral")
                updated_content = self._adjust_tone(content, tone)
            elif adjustment_type == "complexity_adjustment":
                # è°ƒæ•´å¤æ‚åº¦
                complexity = adjustment_data.get("complexity", "medium")
                updated_content = self._adjust_complexity(content, complexity)
            elif adjustment_type == "formality_adjustment":
                # è°ƒæ•´æ­£å¼ç¨‹åº¦
                formality = adjustment_data.get("formality", "neutral")
                updated_content = self._adjust_formality(content, formality)
            else:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„é£æ ¼è°ƒæ•´ç±»å‹: {adjustment_type}"
                }
            
            return {
                "success": True,
                "updated_content": updated_content,
                "adjustment_type": adjustment_type
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"é£æ ¼è°ƒæ•´å¤±è´¥: {str(e)}"
            }
    
    def _adjust_tone(self, content: str, tone: str) -> str:
        """è°ƒæ•´è¯­æ°”"""
        # ç®€åŒ–çš„è¯­æ°”è°ƒæ•´å®ç°
        if tone == "formal":
            # å¢åŠ æ­£å¼æ€§
            content = content.replace("æˆ‘ä»¬", "æœ¬æœºæ„")
            content = content.replace("ä½ ä»¬", "è´µæ–¹")
        elif tone == "casual":
            # å¢åŠ éšæ„æ€§
            content = content.replace("æœ¬æœºæ„", "æˆ‘ä»¬")
            content = content.replace("è´µæ–¹", "ä½ ä»¬")
        
        return content
    
    def _adjust_complexity(self, content: str, complexity: str) -> str:
        """è°ƒæ•´å¤æ‚åº¦"""
        # ç®€åŒ–çš„å¤æ‚åº¦è°ƒæ•´å®ç°
        if complexity == "simple":
            # ç®€åŒ–è¡¨è¾¾
            content = content.replace("å› æ­¤", "æ‰€ä»¥")
            content = content.replace("ç„¶è€Œ", "ä½†æ˜¯")
        elif complexity == "complex":
            # å¢åŠ å¤æ‚åº¦
            content = content.replace("æ‰€ä»¥", "å› æ­¤")
            content = content.replace("ä½†æ˜¯", "ç„¶è€Œ")
        
        return content
    
    def _adjust_formality(self, content: str, formality: str) -> str:
        """è°ƒæ•´æ­£å¼ç¨‹åº¦"""
        # ç®€åŒ–çš„æ­£å¼ç¨‹åº¦è°ƒæ•´å®ç°
        if formality == "formal":
            # å¢åŠ æ­£å¼æ€§
            content = content.replace("è¿™ä¸ª", "è¯¥")
            content = content.replace("é‚£ä¸ª", "è¯¥")
        elif formality == "informal":
            # å‡å°‘æ­£å¼æ€§
            content = content.replace("è¯¥", "è¿™ä¸ª")
        
        return content
    
    def _generate_application_report(self, applied_changes: List[Dict[str, Any]], 
                                   failed_changes: List[Dict[str, Any]], 
                                   total_changes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆåº”ç”¨æŠ¥å‘Š"""
        try:
            # ç»Ÿè®¡ä¿¡æ¯
            total_count = len(total_changes)
            applied_count = len(applied_changes)
            failed_count = len(failed_changes)
            success_rate = applied_count / total_count if total_count > 0 else 0
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            type_stats = {}
            for change in total_changes:
                change_type = change.get("type", "unknown")
                if change_type not in type_stats:
                    type_stats[change_type] = {"total": 0, "applied": 0, "failed": 0}
                type_stats[change_type]["total"] += 1
            
            for change in applied_changes:
                change_type = change.get("type", "unknown")
                if change_type in type_stats:
                    type_stats[change_type]["applied"] += 1
            
            for change in failed_changes:
                change_type = change.get("type", "unknown")
                if change_type in type_stats:
                    type_stats[change_type]["failed"] += 1
            
            return {
                "summary": {
                    "total_changes": total_count,
                    "applied_changes": applied_count,
                    "failed_changes": failed_count,
                    "success_rate": success_rate
                },
                "type_statistics": type_stats,
                "applied_changes": applied_changes,
                "failed_changes": failed_changes,
                "recommendations": self._generate_application_recommendations(applied_changes, failed_changes)
            }
            
        except Exception as e:
            return {"error": f"ç”Ÿæˆåº”ç”¨æŠ¥å‘Šå¤±è´¥: {str(e)}"}
    
    def _generate_application_recommendations(self, applied_changes: List[Dict[str, Any]], 
                                            failed_changes: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆåº”ç”¨å»ºè®®"""
        recommendations = []
        
        if failed_changes:
            recommendations.append(f"æœ‰ {len(failed_changes)} ä¸ªå˜åŒ–åº”ç”¨å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥åŸå› ")
        
        if applied_changes:
            recommendations.append(f"æˆåŠŸåº”ç”¨äº† {len(applied_changes)} ä¸ªå˜åŒ–")
        
        # æ£€æŸ¥ç‰¹å®šç±»å‹çš„å¤±è´¥
        text_replacement_failures = [c for c in failed_changes if c.get("type") == "text_replacement"]
        if text_replacement_failures:
            recommendations.append("æ–‡æœ¬æ›¿æ¢å¤±è´¥è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥ç›®æ ‡æ–‡æœ¬æ˜¯å¦å­˜åœ¨")
        
        format_failures = [c for c in failed_changes if c.get("type") == "format_change"]
        if format_failures:
            recommendations.append("æ ¼å¼å˜åŒ–å¤±è´¥è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥æ ¼å¼ç±»å‹æ˜¯å¦æ”¯æŒ")
        
        if not recommendations:
            recommendations.append("æ‰€æœ‰å˜åŒ–éƒ½å·²æˆåŠŸåº”ç”¨")
        
        return recommendations

    def handle_batch_style_changes(self, session_id: str, changes: List[Dict[str, Any]]) -> dict:
        """
        æ‰¹é‡å¤„ç†é£æ ¼å˜åŒ–ï¼ˆå¦‚å…¨éƒ¨æ¥å—/å…¨éƒ¨æ‹’ç»ï¼‰- çœŸå®å®ç°
        """
        try:
            # 1. ä»sessionæ–‡ä»¶ä¸­è¯»å–çœŸå®çš„åŸå§‹æ–‡æ¡£å†…å®¹å’Œé£æ ¼æ¨¡æ¿
            session_file = os.path.join(self.semantic_behavior_dir, "profiles", f"{session_id}.json")
            if not os.path.exists(session_file):
                return {
                    "success": False,
                    "error": f"ä¼šè¯æ–‡ä»¶ä¸å­˜åœ¨: {session_id}"
                }
            
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            
            # è·å–çœŸå®çš„åŸå§‹æ–‡æ¡£å†…å®¹
            original_content = session_data.get("original_content", "")
            document_name = session_data.get("document_name", f"session_{session_id}")
            style_template_id = session_data.get("style_template_id", "")
            
            if not original_content:
                return {
                    "success": False,
                    "error": "sessionæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ–‡æ¡£å†…å®¹"
                }
            
            if not style_template_id:
                return {
                    "success": False,
                    "error": "sessionæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°é£æ ¼æ¨¡æ¿ID"
                }
            
            # 2. åŠ è½½é£æ ¼æ¨¡æ¿
            template = self.load_style_template(style_template_id)
            if not template:
                template = {
                    "style_type": "business_professional",
                    "style_features": {
                        "formality": 0.8,
                        "technicality": 0.6,
                        "objectivity": 0.7,
                        "conciseness": 0.5
                    }
                }
            
            # 3. æ ¹æ®actionæ‰§è¡ŒçœŸå®çš„é£æ ¼è¿ç§»
            if action == "accept_all":
                # ä½¿ç”¨LLMè¿›è¡ŒçœŸå®çš„é£æ ¼è¿ç§»
                migrated_content = self._perform_real_style_migration(
                    original_content, template, style_template_id
                )
                
                # ç”ŸæˆçœŸå®çš„å˜æ›´è®°å½•
                suggested_changes = self._generate_real_changes(
                    original_content, migrated_content, template
                )
                
                # æ ‡è®°æ‰€æœ‰å˜æ›´ä¸ºå·²æ¥å—
                for change in suggested_changes:
                    change["status"] = "accepted"
                    
            elif action == "reject_all":
                # æ‹’ç»æ‰€æœ‰å˜æ›´ï¼Œä¿æŒåŸæ–‡
                migrated_content = original_content
                suggested_changes = []
                
            else:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}"
                }
            
            # 4. æ›´æ–°sessionæ•°æ®
            session_data.update({
                "action": action,
                "migrated_content": migrated_content,
                "suggested_changes": suggested_changes,
                "target_style": template.get("style_type", "business_professional"),
                "last_updated": datetime.now().isoformat()
            })
            
            # ä¿å­˜æ›´æ–°åçš„sessionæ–‡ä»¶
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
            
            print(f"ä¼šè¯æ•°æ®å·²ä¿å­˜: {session_file}")
            
            return {
                "success": True,
                "session_id": session_id,
                "action": action,
                "message": f"æ‰¹é‡{action}é£æ ¼å˜åŒ–å·²å¤„ç†",
                "changes_count": len(suggested_changes),
                "accepted_count": len([c for c in suggested_changes if c.get("status") == "accepted"]),
                "migrated_content_length": len(migrated_content)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"æ‰¹é‡å¤„ç†é£æ ¼å˜åŒ–å¤±è´¥: {str(e)}"
            }
    
    def _perform_real_style_migration(self, original_content: str, template: dict, template_id: str) -> str:
        """
        æ‰§è¡ŒçœŸå®çš„é£æ ¼è¿ç§»
        """
        try:
            # 1. æ„å»ºé£æ ¼è¿ç§»æç¤ºè¯
            target_style = template.get("style_type", "business_professional")
            style_features = template.get("style_features", {})
            
            prompt = self._build_style_migration_prompt(
                original_content, target_style, style_features
            )
            
            # 2. è°ƒç”¨LLMè¿›è¡Œé£æ ¼è¿ç§»
            if self.llm_client:
                response = self.llm_client.generate_text(prompt, max_tokens=2000)
                if response and "content" in response:
                    migrated_content = response["content"].strip()
                    # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
                    if migrated_content.startswith("```"):
                        lines = migrated_content.split('\n')
                        if len(lines) > 2:
                            migrated_content = '\n'.join(lines[1:-1])
                    return migrated_content
            
            # 3. å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŸºç¡€è¿ç§»
            return self._rule_based_style_migration(original_content, target_style, style_features)
            
        except Exception as e:
            print(f"é£æ ¼è¿ç§»å¤±è´¥: {e}")
            return original_content
    
    def _build_style_migration_prompt(self, content: str, target_style: str, style_features: dict) -> str:
        """
        æ„å»ºé£æ ¼è¿ç§»çš„LLMæç¤ºè¯
        """
        formality = style_features.get("formality", 0.5)
        technicality = style_features.get("technicality", 0.5)
        objectivity = style_features.get("objectivity", 0.5)
        conciseness = style_features.get("conciseness", 0.5)
        
        prompt = f"""
è¯·å°†ä»¥ä¸‹æ–‡æ¡£çš„é£æ ¼è°ƒæ•´ä¸º{target_style}é£æ ¼ï¼Œè¦æ±‚ï¼š

é£æ ¼ç‰¹å¾ï¼š
- æ­£å¼ç¨‹åº¦ï¼š{formality:.1f}ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šæ­£å¼ï¼‰
- æŠ€æœ¯æ€§ï¼š{technicality:.1f}ï¼ˆ0-1ï¼Œè¶Šé«˜è¶ŠæŠ€æœ¯åŒ–ï¼‰
- å®¢è§‚æ€§ï¼š{objectivity:.1f}ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå®¢è§‚ï¼‰
- ç®€æ´æ€§ï¼š{conciseness:.1f}ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šç®€æ´ï¼‰

è°ƒæ•´è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„æ ¸å¿ƒä¿¡æ¯å’Œé€»è¾‘ç»“æ„
2. è°ƒæ•´è¯æ±‡é€‰æ‹©ï¼Œä½¿å…¶ç¬¦åˆç›®æ ‡é£æ ¼
3. ä¼˜åŒ–å¥å¼ç»“æ„ï¼Œæé«˜è¡¨è¾¾çš„ä¸“ä¸šæ€§
4. ç¡®ä¿è¯­è¨€çš„ä¸€è‡´æ€§å’Œè¿è´¯æ€§

åŸæ–‡ï¼š
{content}

è¯·ç›´æ¥è¿”å›è°ƒæ•´åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ ‡è®°ã€‚
"""
        return prompt
    
    def _rule_based_style_migration(self, content: str, target_style: str, style_features: dict) -> str:
        """
        åŸºäºè§„åˆ™çš„é£æ ¼è¿ç§»ï¼ˆLLMä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰
        """
        migrated_content = content
        
        # æ ¹æ®ç›®æ ‡é£æ ¼åº”ç”¨ä¸åŒçš„è§„åˆ™
        if target_style == "business_professional":
            # å•†åŠ¡ä¸“ä¸šé£æ ¼è°ƒæ•´
            replacements = {
                "æˆ‘è§‰å¾—": "æˆ‘è®¤ä¸º",
                "æŒºå¥½çš„": "è¾ƒä¸ºç†æƒ³",
                "åº”è¯¥å¯ä»¥": "èƒ½å¤Ÿ",
                "è§£å†³é—®é¢˜": "è§£å†³ç›¸å…³é—®é¢˜",
                "ç”¨äº†": "é‡‡ç”¨äº†",
                "ç®—äº†ä¸€ä¸‹": "è¿›è¡Œäº†åˆ†æ",
                "æ€»çš„æ¥è¯´": "ç»¼ä¸Šæ‰€è¿°",
                "ä¸é”™": "è‰¯å¥½",
                "åº”è¯¥èƒ½ç”¨": "å…·å¤‡å¯è¡Œæ€§"
            }
            
            for old, new in replacements.items():
                migrated_content = migrated_content.replace(old, new)
        
        elif target_style == "academic":
            # å­¦æœ¯é£æ ¼è°ƒæ•´
            replacements = {
                "æˆ‘è§‰å¾—": "ç ”ç©¶è¡¨æ˜",
                "æŒºå¥½çš„": "å…·æœ‰ç§¯ææ•ˆæœ",
                "åº”è¯¥å¯ä»¥": "èƒ½å¤Ÿæœ‰æ•ˆ",
                "è§£å†³é—®é¢˜": "è§£å†³ç›¸å…³é—®é¢˜",
                "ç”¨äº†": "é‡‡ç”¨äº†",
                "ç®—äº†ä¸€ä¸‹": "è¿›è¡Œäº†ç»Ÿè®¡åˆ†æ",
                "æ€»çš„æ¥è¯´": "ç»¼ä¸Šæ‰€è¿°",
                "ä¸é”™": "è¡¨ç°è‰¯å¥½",
                "åº”è¯¥èƒ½ç”¨": "å…·å¤‡åº”ç”¨ä»·å€¼"
            }
            
            for old, new in replacements.items():
                migrated_content = migrated_content.replace(old, new)
        
        return migrated_content
    
    def _generate_real_changes(self, original_content: str, migrated_content: str, template: dict) -> list:
        """
        ç”ŸæˆçœŸå®çš„å˜æ›´è®°å½•
        """
        changes = []
        
        # ä½¿ç”¨difflibç”Ÿæˆå·®å¼‚
        import difflib
        
        # ç®€å•çš„è¯çº§åˆ«å·®å¼‚æ£€æµ‹
        original_words = original_content.split()
        migrated_words = migrated_content.split()
        
        matcher = difflib.SequenceMatcher(None, original_words, migrated_words)
        
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'replace':
                original_text = ' '.join(original_words[i1:i2])
                suggested_text = ' '.join(migrated_words[j1:j2])
                
                if original_text.strip() and suggested_text.strip():
                    change = {
                        "original_text": original_text,
                        "suggested_text": suggested_text,
                        "status": "accepted",
                        "change_type": self._classify_change_type(original_text, suggested_text),
                        "confidence": 0.85,
                        "position": i1
                    }
                    changes.append(change)
        
        return changes
    
    def _classify_change_type(self, original: str, suggested: str) -> str:
        """
        åˆ†ç±»å˜æ›´ç±»å‹
        """
        if len(original) < len(suggested):
            return "vocabulary_improvement"
        elif len(original) > len(suggested):
            return "conciseness_improvement"
        else:
            return "style_alignment"

    def handle_style_change(self, session_id: str, change_id: str, action: str) -> dict:
        """
        å¤„ç†å•ä¸ªé£æ ¼å˜åŒ–ï¼ˆæ¥å—/æ‹’ç»ï¼‰
        
        Args:
            session_id: ä¼šè¯ID
            change_id: å˜åŒ–ID
            action: æ“ä½œç±»å‹ ('accept' æˆ– 'reject')
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        try:
            # 1. éªŒè¯å‚æ•°
            if action not in ['accept', 'reject']:
                return {
                    "success": False,
                    "error": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}ï¼Œå¿…é¡»æ˜¯ 'accept' æˆ– 'reject'"
                }
            
            # 2. ä»sessionæ–‡ä»¶ä¸­è¯»å–æ•°æ®
            session_file = os.path.join(self.semantic_behavior_dir, "profiles", f"{session_id}.json")
            if not os.path.exists(session_file):
                return {
                    "success": False,
                    "error": f"ä¼šè¯æ–‡ä»¶ä¸å­˜åœ¨: {session_id}"
                }
            
            with open(session_file, 'r', encoding='utf-8') as f:
                session_data = json.load(f)
            
            # 3. è·å–åŸå§‹å†…å®¹å’Œå»ºè®®çš„å˜åŒ–
            original_content = session_data.get("original_content", "")
            suggested_changes = session_data.get("suggested_changes", [])
            
            if not original_content:
                return {
                    "success": False,
                    "error": "sessionæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ–‡æ¡£å†…å®¹"
                }
            
            # 4. æŸ¥æ‰¾æŒ‡å®šçš„å˜åŒ–
            target_change = None
            for change in suggested_changes:
                if change.get("change_id") == change_id:
                    target_change = change
                    break
            
            if not target_change:
                return {
                    "success": False,
                    "error": f"æœªæ‰¾åˆ°æŒ‡å®šçš„å˜åŒ–: {change_id}"
                }
            
            # 5. æ›´æ–°å˜åŒ–çŠ¶æ€
            target_change["status"] = action
            target_change["action_time"] = datetime.now().isoformat()
            
            # 6. ç”Ÿæˆæ›´æ–°åçš„é¢„è§ˆå†…å®¹
            updated_preview = self._generate_updated_preview(original_content, suggested_changes)
            
            # 7. æ›´æ–°sessionæ•°æ®
            session_data.update({
                "suggested_changes": suggested_changes,
                "updated_preview": updated_preview,
                "last_updated": datetime.now().isoformat()
            })
            
            # 8. ä¿å­˜æ›´æ–°åçš„sessionæ–‡ä»¶
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
            
            print(f"å•ä¸ªé£æ ¼å˜åŒ–å·²å¤„ç†: {change_id} -> {action}")
            
            return {
                "success": True,
                "change_id": change_id,
                "action": action,
                "message": f"å˜åŒ– {change_id} å·²{action}",
                "updated_preview": {
                    "content": updated_preview,
                    "accepted_changes": len([c for c in suggested_changes if c.get("status") == "accepted"]),
                    "rejected_changes": len([c for c in suggested_changes if c.get("status") == "rejected"]),
                    "pending_changes": len([c for c in suggested_changes if c.get("status") == "pending"])
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"å¤„ç†å•ä¸ªé£æ ¼å˜åŒ–å¤±è´¥: {str(e)}"
            }
    
    def _generate_updated_preview(self, original_content: str, suggested_changes: list) -> str:
        """
        æ ¹æ®å·²æ¥å—çš„å˜åŒ–ç”Ÿæˆæ›´æ–°åçš„é¢„è§ˆå†…å®¹
        
        Args:
            original_content: åŸå§‹å†…å®¹
            suggested_changes: å»ºè®®çš„å˜åŒ–åˆ—è¡¨
            
        Returns:
            str: æ›´æ–°åçš„é¢„è§ˆå†…å®¹
        """
        try:
            # æŒ‰ä½ç½®æ’åºå˜åŒ–ï¼Œç¡®ä¿æŒ‰é¡ºåºåº”ç”¨
            accepted_changes = [c for c in suggested_changes if c.get("status") == "accepted"]
            accepted_changes.sort(key=lambda x: x.get("position", {}).get("start", 0))
            
            # ä»åå¾€å‰åº”ç”¨å˜åŒ–ï¼Œé¿å…ä½ç½®åç§»
            updated_content = original_content
            offset = 0
            
            for change in accepted_changes:
                position = change.get("position", {})
                start = position.get("start", 0) + offset
                end = position.get("end", 0) + offset
                suggested_text = change.get("suggested_text", "")
                
                # åº”ç”¨å˜åŒ–
                if start < len(updated_content) and end <= len(updated_content):
                    updated_content = updated_content[:start] + suggested_text + updated_content[end:]
                    # æ›´æ–°åç§»é‡
                    offset += len(suggested_text) - (end - start)
            
            return updated_content
            
        except Exception as e:
            print(f"ç”Ÿæˆæ›´æ–°é¢„è§ˆå¤±è´¥: {e}")
            return original_content
