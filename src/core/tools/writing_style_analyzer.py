import re
import json
import os
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime
import hashlib

# å¯¼å…¥å¢å¼ºçš„æ–‡é£åˆ†æç»„ä»¶
try:
    from .comprehensive_style_processor import ComprehensiveStyleProcessor
    from .enhanced_style_extractor import EnhancedStyleExtractor
    from .style_alignment_engine import StyleAlignmentEngine
    ENHANCED_FEATURES_AVAILABLE = True
except ImportError:
    ENHANCED_FEATURES_AVAILABLE = False
    print("Warning: Enhanced style analysis features not available. Using basic functionality.")

class WritingStyleAnalyzer:
    """
    æ–‡é£åˆ†æå™¨
    åˆ†ææ–‡æ¡£çš„å†™ä½œé£æ ¼ç‰¹å¾ï¼Œç”Ÿæˆæ–‡é£æ¨¡æ¿ï¼Œæ”¯æŒæ–‡é£å¯¹é½åŠŸèƒ½
    ç°å·²é›†æˆå¢å¼ºçš„æ–‡é£åˆ†æåŠŸèƒ½
    """

    def __init__(self, storage_path: str = "src/core/knowledge_base/writing_style_templates", llm_client=None):
        self.tool_name = "æ–‡é£åˆ†æå™¨"
        self.description = "åˆ†ææ–‡æ¡£å†™ä½œé£æ ¼ï¼Œç”Ÿæˆæ–‡é£æ¨¡æ¿ï¼Œæ”¯æŒæ–‡é£å¯¹é½å’Œæ¶¦è‰²åŠŸèƒ½"
        self.storage_path = storage_path
        self.llm_client = llm_client

        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        os.makedirs(storage_path, exist_ok=True)

        # åˆå§‹åŒ–å¢å¼ºåŠŸèƒ½ç»„ä»¶
        if ENHANCED_FEATURES_AVAILABLE and llm_client:
            self.enhanced_processor = ComprehensiveStyleProcessor(
                llm_client=llm_client,
                storage_path=os.path.join(storage_path, "enhanced_analysis")
            )
            self.use_enhanced_features = True
            print("âœ… å¢å¼ºæ–‡é£åˆ†æåŠŸèƒ½å·²å¯ç”¨")
        else:
            self.enhanced_processor = None
            self.use_enhanced_features = False
            print("âš ï¸ ä½¿ç”¨åŸºç¡€æ–‡é£åˆ†æåŠŸèƒ½")
        
        # æ–‡é£ç‰¹å¾åˆ†æç»´åº¦
        self.style_dimensions = {
            "sentence_structure": {
                "name": "å¥å¼ç»“æ„",
                "features": ["å¹³å‡å¥é•¿", "é•¿çŸ­å¥æ¯”ä¾‹", "å¤åˆå¥ä½¿ç”¨", "å¹¶åˆ—å¥ä½¿ç”¨"]
            },
            "vocabulary_choice": {
                "name": "è¯æ±‡é€‰æ‹©", 
                "features": ["æ­£å¼ç¨‹åº¦", "ä¸“ä¸šæœ¯è¯­", "ä¿®é¥°è¯ä½¿ç”¨", "åŠ¨è¯ç±»å‹åå¥½"]
            },
            "expression_style": {
                "name": "è¡¨è¾¾æ–¹å¼",
                "features": ["ä¸»è¢«åŠ¨è¯­æ€", "äººç§°ä½¿ç”¨", "è¯­æ°”å¼ºåº¦", "æƒ…æ„Ÿè‰²å½©"]
            },
            "text_organization": {
                "name": "æ–‡æœ¬ç»„ç»‡",
                "features": ["æ®µè½ç»“æ„", "é€»è¾‘è¿æ¥", "è¿‡æ¸¡æ–¹å¼", "æ€»ç»“ä¹ æƒ¯"]
            },
            "language_habits": {
                "name": "è¯­è¨€ä¹ æƒ¯",
                "features": ["å£è¯­åŒ–ç¨‹åº¦", "ä¹¦é¢è¯­è§„èŒƒ", "åœ°åŸŸç‰¹è‰²", "è¡Œä¸šç‰¹è‰²"]
            }
        }
        
        # å¸¸è§æ–‡é£ç±»å‹
        self.style_types = {
            "formal_official": {
                "name": "æ­£å¼å…¬æ–‡é£æ ¼",
                "characteristics": ["ä¸¥è°¨è§„èŒƒ", "ç”¨è¯å‡†ç¡®", "é€»è¾‘æ¸…æ™°", "æ ¼å¼æ ‡å‡†"],
                "typical_patterns": ["æ ¹æ®", "æŒ‰ç…§", "ç°å°†", "ç‰¹æ­¤", "åŠ¡å¿…"]
            },
            "business_professional": {
                "name": "å•†åŠ¡ä¸“ä¸šé£æ ¼", 
                "characteristics": ["ç®€æ´æ˜äº†", "é‡ç‚¹çªå‡º", "æ•°æ®å¯¼å‘", "ç»“æœå¯¼å‘"],
                "typical_patterns": ["æå‡", "ä¼˜åŒ–", "å®ç°", "è¾¾æˆ", "æ¨è¿›"]
            },
            "academic_research": {
                "name": "å­¦æœ¯ç ”ç©¶é£æ ¼",
                "characteristics": ["å®¢è§‚ä¸¥è°¨", "é€»è¾‘ä¸¥å¯†", "è®ºè¯å……åˆ†", "å¼•ç”¨è§„èŒƒ"],
                "typical_patterns": ["ç ”ç©¶è¡¨æ˜", "åˆ†æå‘ç°", "ç»¼åˆè€ƒè™‘", "æ·±å…¥æ¢è®¨"]
            },
            "narrative_descriptive": {
                "name": "å™è¿°æè¿°é£æ ¼",
                "characteristics": ["ç”ŸåŠ¨å½¢è±¡", "ç»†èŠ‚ä¸°å¯Œ", "æƒ…æ„ŸçœŸå®", "æ•…äº‹æ€§å¼º"],
                "typical_patterns": ["ç”ŸåŠ¨åœ°", "è¯¦ç»†åœ°", "æ·±åˆ»åœ°", "çœŸå®åœ°"]
            },
            "concise_practical": {
                "name": "ç®€æ´å®ç”¨é£æ ¼",
                "characteristics": ["è¨€ç®€æ„èµ…", "ç›´æ¥æœ‰æ•ˆ", "æ“ä½œæ€§å¼º", "æ˜“äºç†è§£"],
                "typical_patterns": ["ç›´æ¥", "ç«‹å³", "é©¬ä¸Š", "ç®€å•", "å¿«é€Ÿ"]
            }
        }
    
    def analyze_writing_style(self, document_content: str, document_name: str = None,
                             use_enhanced: bool = None) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£çš„å†™ä½œé£æ ¼

        Args:
            document_content: æ–‡æ¡£å†…å®¹
            document_name: æ–‡æ¡£åç§°
            use_enhanced: æ˜¯å¦ä½¿ç”¨å¢å¼ºåŠŸèƒ½ï¼ˆNoneæ—¶è‡ªåŠ¨åˆ¤æ–­ï¼‰

        Returns:
            æ–‡é£åˆ†æç»“æœ
        """
        # å†³å®šæ˜¯å¦ä½¿ç”¨å¢å¼ºåŠŸèƒ½
        if use_enhanced is None:
            use_enhanced = self.use_enhanced_features

        if use_enhanced and self.enhanced_processor:
            return self._analyze_with_enhanced_features(document_content, document_name)
        else:
            return self._analyze_with_basic_features(document_content, document_name)

    def _analyze_with_enhanced_features(self, document_content: str, document_name: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨å¢å¼ºåŠŸèƒ½è¿›è¡Œæ–‡é£åˆ†æ"""
        try:
            print(f"ğŸ” ä½¿ç”¨å¢å¼ºåŠŸèƒ½åˆ†ææ–‡æ¡£: {document_name or 'æœªå‘½åæ–‡æ¡£'}")

            # ä½¿ç”¨ç»¼åˆæ–‡é£å¤„ç†å™¨è¿›è¡Œåˆ†æ
            enhanced_result = self.enhanced_processor.extract_comprehensive_style_features(
                document_content, document_name, include_advanced_analysis=True
            )

            # è½¬æ¢ä¸ºå…¼å®¹çš„æ ¼å¼
            analysis_result = {
                "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
                "analysis_time": datetime.now().isoformat(),
                "analysis_method": "enhanced",
                "document_stats": self._get_document_statistics(document_content),
                "enhanced_analysis": enhanced_result,
                "style_features": self._extract_style_features_from_enhanced(enhanced_result),
                "style_type": self._determine_style_type_from_enhanced(enhanced_result),
                "confidence_score": self._calculate_confidence_from_enhanced(enhanced_result),
                "style_prompt": self._generate_style_prompt_from_enhanced(enhanced_result),
                "template_id": self._generate_template_id(document_name, {}),
                "detailed_analysis": enhanced_result.get("advanced_features", {}),
                "writing_recommendations": self._generate_recommendations_from_enhanced(enhanced_result),
                "style_comparison": {}
            }

            print("âœ… å¢å¼ºæ–‡é£åˆ†æå®Œæˆ")
            return analysis_result

        except Exception as e:
            print(f"âŒ å¢å¼ºåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€åˆ†æ: {str(e)}")
            return self._analyze_with_basic_features(document_content, document_name)

    def _analyze_with_basic_features(self, document_content: str, document_name: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨åŸºç¡€åŠŸèƒ½è¿›è¡Œæ–‡é£åˆ†æ"""
        try:
            analysis_result = {
                "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
                "analysis_time": datetime.now().isoformat(),
                "analysis_method": "basic",
                "document_stats": self._get_document_statistics(document_content),
                "style_features": {},
                "style_type": None,
                "confidence_score": 0.0,
                "style_prompt": "",
                "template_id": None,
                "detailed_analysis": {},
                "writing_recommendations": [],
                "style_comparison": {}
            }

            # åˆ†æå„ä¸ªç»´åº¦çš„æ–‡é£ç‰¹å¾
            style_features = self._analyze_style_features(document_content)
            analysis_result["style_features"] = style_features

            # è¯†åˆ«æ–‡é£ç±»å‹
            style_type, confidence = self._identify_style_type(document_content, style_features)
            analysis_result["style_type"] = style_type
            analysis_result["confidence_score"] = confidence

            # ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
            detailed_analysis = self._generate_detailed_analysis(document_content, style_features, style_type)
            analysis_result["detailed_analysis"] = detailed_analysis

            # ç”Ÿæˆå†™ä½œå»ºè®®
            recommendations = self._generate_writing_recommendations(style_features, style_type)
            analysis_result["writing_recommendations"] = recommendations

            # ç”Ÿæˆé£æ ¼å¯¹æ¯”
            style_comparison = self._generate_style_comparison(style_features)
            analysis_result["style_comparison"] = style_comparison

            # ç”Ÿæˆæ–‡é£æç¤ºè¯
            style_prompt = self._generate_enhanced_style_prompt(style_features, style_type, detailed_analysis)
            analysis_result["style_prompt"] = style_prompt

            # ç”Ÿæˆæ¨¡æ¿ID
            template_id = self._generate_template_id(document_name, style_features)
            analysis_result["template_id"] = template_id

            return analysis_result

        except Exception as e:
            return {"error": f"æ–‡é£åˆ†æå¤±è´¥: {str(e)}"}

    def _extract_style_features_from_enhanced(self, enhanced_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­æå–é£æ ¼ç‰¹å¾"""
        style_features = {}

        try:
            basic_features = enhanced_result.get("basic_features", {})

            # æå–é‡åŒ–ç‰¹å¾
            quant_features = basic_features.get("quantitative_features", {})
            if quant_features:
                lexical = quant_features.get("lexical_features", {})
                syntactic = quant_features.get("syntactic_features", {})

                style_features["lexical_richness"] = lexical.get("ttr", 0)
                style_features["avg_word_length"] = lexical.get("avg_word_length", 0)
                style_features["formal_density"] = lexical.get("formal_word_density", 0)
                style_features["avg_sentence_length"] = syntactic.get("avg_sentence_length", 0)
                style_features["sentence_variety"] = syntactic.get("sentence_length_std", 0)

            # æå–LLMç‰¹å¾
            llm_features = basic_features.get("llm_features", {})
            if llm_features:
                evaluations = llm_features.get("evaluations", {})
                for dimension, eval_data in evaluations.items():
                    if isinstance(eval_data, dict) and "score" in eval_data:
                        style_features[f"llm_{dimension}"] = eval_data["score"]

        except Exception as e:
            style_features["extraction_error"] = str(e)

        return style_features

    def _determine_style_type_from_enhanced(self, enhanced_result: Dict[str, Any]) -> str:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­ç¡®å®šé£æ ¼ç±»å‹"""
        try:
            advanced_features = enhanced_result.get("advanced_features", {})
            comprehensive_analysis = advanced_features.get("comprehensive_analysis", {})

            if comprehensive_analysis.get("success"):
                parsed_analysis = comprehensive_analysis.get("parsed_analysis", {})
                overall_style = parsed_analysis.get("overall_style", {})

                # ä»LLMåˆ†æä¸­æå–é£æ ¼ç±»å‹
                style_type = overall_style.get("ä¸»è¦é£æ ¼ç±»å‹", "")
                if style_type:
                    # æ˜ å°„åˆ°å†…éƒ¨é£æ ¼ç±»å‹
                    style_mapping = {
                        "æ­£å¼å…¬æ–‡": "formal_official",
                        "å•†åŠ¡ä¸“ä¸š": "business_professional",
                        "å­¦æœ¯ç ”ç©¶": "academic_research",
                        "å™è¿°æè¿°": "narrative_descriptive",
                        "ç®€æ´å®ç”¨": "concise_practical"
                    }
                    return style_mapping.get(style_type, "business_professional")

            # å›é€€åˆ°åŸºäºç‰¹å¾çš„åˆ¤æ–­
            style_features = self._extract_style_features_from_enhanced(enhanced_result)
            formal_score = style_features.get("llm_æ­£å¼ç¨‹åº¦", 3.0)

            if formal_score >= 4.0:
                return "formal_official"
            elif formal_score >= 3.5:
                return "business_professional"
            else:
                return "concise_practical"

        except Exception:
            return "business_professional"

    def _calculate_confidence_from_enhanced(self, enhanced_result: Dict[str, Any]) -> float:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­è®¡ç®—ç½®ä¿¡åº¦"""
        try:
            basic_features = enhanced_result.get("basic_features", {})

            # åŸºäºæˆåŠŸæå–çš„ç‰¹å¾æ•°é‡è®¡ç®—ç½®ä¿¡åº¦
            feature_vector = basic_features.get("feature_vector", [])
            if feature_vector:
                base_confidence = min(len(feature_vector) / 20.0, 1.0)  # å‡è®¾20ä¸ªç‰¹å¾ä¸ºæ»¡åˆ†
            else:
                base_confidence = 0.5

            # å¦‚æœæœ‰LLMåˆ†æï¼Œæé«˜ç½®ä¿¡åº¦
            llm_features = basic_features.get("llm_features", {})
            if llm_features.get("evaluations"):
                base_confidence = min(base_confidence + 0.2, 1.0)

            # å¦‚æœæœ‰é«˜çº§åˆ†æï¼Œè¿›ä¸€æ­¥æé«˜ç½®ä¿¡åº¦
            advanced_features = enhanced_result.get("advanced_features", {})
            if advanced_features:
                base_confidence = min(base_confidence + 0.1, 1.0)

            return round(base_confidence, 3)

        except Exception:
            return 0.7  # é»˜è®¤ç½®ä¿¡åº¦

    def _generate_style_prompt_from_enhanced(self, enhanced_result: Dict[str, Any]) -> str:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­ç”Ÿæˆé£æ ¼æç¤ºè¯"""
        try:
            style_features = self._extract_style_features_from_enhanced(enhanced_result)
            style_type = self._determine_style_type_from_enhanced(enhanced_result)

            # è·å–é£æ ¼ç±»å‹ä¿¡æ¯
            style_info = self.style_types.get(style_type, {})
            style_name = style_info.get("name", "æ ‡å‡†é£æ ¼")
            characteristics = style_info.get("characteristics", [])

            # æ„å»ºæç¤ºè¯
            prompt_parts = [f"è¯·æŒ‰ç…§{style_name}è¿›è¡Œå†™ä½œ"]

            if characteristics:
                prompt_parts.append(f"ç‰¹ç‚¹ï¼š{', '.join(characteristics)}")

            # æ·»åŠ å…·ä½“çš„é£æ ¼æŒ‡å¯¼
            if style_features.get("formal_density", 0) > 10:
                prompt_parts.append("ä½¿ç”¨æ­£å¼è¯æ±‡å’Œè¡¨è¾¾")

            if style_features.get("avg_sentence_length", 0) > 15:
                prompt_parts.append("é‡‡ç”¨è¾ƒé•¿çš„å¤åˆå¥ç»“æ„")
            elif style_features.get("avg_sentence_length", 0) < 10:
                prompt_parts.append("ä½¿ç”¨ç®€æ´æ˜äº†çš„çŸ­å¥")

            return "ï¼›".join(prompt_parts)

        except Exception:
            return "è¯·ä¿æŒåŸæœ‰çš„å†™ä½œé£æ ¼"

    def _generate_recommendations_from_enhanced(self, enhanced_result: Dict[str, Any]) -> List[str]:
        """ä»å¢å¼ºåˆ†æç»“æœä¸­ç”Ÿæˆå†™ä½œå»ºè®®"""
        recommendations = []

        try:
            style_features = self._extract_style_features_from_enhanced(enhanced_result)

            # åŸºäºç‰¹å¾ç»™å‡ºå»ºè®®
            if style_features.get("lexical_richness", 0) < 0.5:
                recommendations.append("å»ºè®®å¢åŠ è¯æ±‡å¤šæ ·æ€§ï¼Œé¿å…é‡å¤ä½¿ç”¨ç›¸åŒè¯æ±‡")

            if style_features.get("sentence_variety", 0) < 3:
                recommendations.append("å»ºè®®å¢åŠ å¥å¼å˜åŒ–ï¼Œä½¿ç”¨é•¿çŸ­å¥ç»“åˆçš„æ–¹å¼")

            if style_features.get("formal_density", 0) < 5:
                recommendations.append("å¦‚éœ€æé«˜æ­£å¼ç¨‹åº¦ï¼Œå¯å¢åŠ æ­£å¼è¯æ±‡çš„ä½¿ç”¨")

            # ä»LLMåˆ†æä¸­æå–å»ºè®®
            advanced_features = enhanced_result.get("advanced_features", {})
            comprehensive_analysis = advanced_features.get("comprehensive_analysis", {})

            if comprehensive_analysis.get("success"):
                parsed_analysis = comprehensive_analysis.get("parsed_analysis", {})
                style_summary = parsed_analysis.get("style_summary", {})

                improvement_suggestions = style_summary.get("æ”¹è¿›å»ºè®®", "")
                if improvement_suggestions and improvement_suggestions != "æ— ":
                    recommendations.append(improvement_suggestions)

            return recommendations[:5]  # æœ€å¤šè¿”å›5æ¡å»ºè®®

        except Exception:
            return ["å»ºè®®ä¿æŒå½“å‰å†™ä½œé£æ ¼çš„ä¸€è‡´æ€§"]

    def analyze_with_semantic_behavior(self, document_content: str, document_name: str = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•è¿›è¡Œæ–‡é£åˆ†æ

        Args:
            document_content: æ–‡æ¡£å†…å®¹
            document_name: æ–‡æ¡£åç§°

        Returns:
            è¯­ä¹‰è¡Œä¸ºåˆ†æç»“æœ
        """
        if not self.use_enhanced_features or not self.enhanced_processor:
            return {"error": "å¢å¼ºåŠŸèƒ½æœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰è¡Œä¸ºåˆ†æ"}

        try:
            print(f"ğŸ§  å¼€å§‹è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ: {document_name or 'æœªå‘½åæ–‡æ¡£'}")

            # ä½¿ç”¨ç»¼åˆå¤„ç†å™¨çš„è¯­ä¹‰åˆ†æåŠŸèƒ½
            semantic_result = self.enhanced_processor.analyze_semantic_behavior(
                document_content, document_name, "comprehensive"
            )

            if semantic_result.get("success"):
                # è½¬æ¢ä¸ºå…¼å®¹çš„æ ¼å¼
                analysis_result = {
                    "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
                    "analysis_time": datetime.now().isoformat(),
                    "analysis_method": "semantic_behavior",
                    "semantic_analysis": semantic_result,
                    "style_features": self._extract_semantic_style_features(semantic_result),
                    "style_type": self._determine_semantic_style_type(semantic_result),
                    "confidence_score": self._calculate_semantic_confidence(semantic_result),
                    "style_prompt": self._generate_semantic_style_prompt(semantic_result),
                    "template_id": self._generate_template_id(document_name, {}),
                    "detailed_analysis": semantic_result.get("comprehensive_insights", {}),
                    "writing_recommendations": self._generate_semantic_recommendations(semantic_result),
                    "style_comparison": {}
                }

                print("âœ… è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå®Œæˆ")
                return analysis_result
            else:
                return {"error": f"è¯­ä¹‰åˆ†æå¤±è´¥: {semantic_result.get('error', 'æœªçŸ¥é”™è¯¯')}"}

        except Exception as e:
            return {"error": f"è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå¤±è´¥: {str(e)}"}

    def _extract_semantic_style_features(self, semantic_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»è¯­ä¹‰åˆ†æç»“æœä¸­æå–é£æ ¼ç‰¹å¾"""
        style_features = {}

        try:
            # ä»æœ€ç»ˆç”»åƒä¸­æå–ç‰¹å¾
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                style_scores = final_profile.get("style_scores", {})

                # æ˜ å°„åˆ°ä¼ ç»Ÿç‰¹å¾åç§°
                style_features.update({
                    "conceptual_organization": style_scores.get("conceptual_organization", 3.0),
                    "semantic_coherence": style_scores.get("semantic_coherence", 3.0),
                    "creative_association": style_scores.get("creative_association", 3.0),
                    "emotional_expression": style_scores.get("emotional_expression", 3.0),
                    "cognitive_complexity": style_scores.get("cognitive_complexity", 3.0),
                    "thematic_focus": style_scores.get("thematic_focus", 3.0)
                })

                # æ·»åŠ ç‰¹å¾å‘é‡é•¿åº¦
                feature_vector = final_profile.get("feature_vector", [])
                style_features["feature_vector_length"] = len(feature_vector)
                style_features["feature_vector_norm"] = final_profile.get("comparative_metrics", {}).get("feature_vector_norm", 0.0)

        except Exception as e:
            style_features["extraction_error"] = str(e)

        return style_features

    def _determine_semantic_style_type(self, semantic_result: Dict[str, Any]) -> str:
        """ä»è¯­ä¹‰åˆ†æç»“æœä¸­ç¡®å®šé£æ ¼ç±»å‹"""
        try:
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                classification = final_profile.get("style_classification", {})
                primary_style = classification.get("primary_style", "")

                # æ˜ å°„åˆ°å†…éƒ¨é£æ ¼ç±»å‹
                style_mapping = {
                    "ç³»ç»Ÿæ€§æ€ç»´å‹": "academic_research",
                    "é€»è¾‘è¿è´¯å‹": "business_professional",
                    "åˆ›æ–°è”æƒ³å‹": "creative_narrative",
                    "æƒ…æ„Ÿè¡¨è¾¾å‹": "narrative_descriptive",
                    "å¤æ‚æ€ç»´å‹": "academic_research",
                    "ä¸“æ³¨èšç„¦å‹": "formal_official"
                }

                return style_mapping.get(primary_style, "business_professional")

            return "business_professional"

        except Exception:
            return "business_professional"

    def _calculate_semantic_confidence(self, semantic_result: Dict[str, Any]) -> float:
        """è®¡ç®—è¯­ä¹‰åˆ†æçš„ç½®ä¿¡åº¦"""
        try:
            # åŸºäºåˆ†ææˆåŠŸçš„é˜¶æ®µæ•°é‡
            analysis_summary = semantic_result.get("semantic_analysis", {}).get("analysis_summary", {})
            stages_completed = analysis_summary.get("stages_completed", 0)
            max_stages = 4

            base_confidence = stages_completed / max_stages

            # å¦‚æœæœ‰æœ€ç»ˆç”»åƒï¼Œæé«˜ç½®ä¿¡åº¦
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                profile_confidence = final_profile.get("comparative_metrics", {}).get("style_score_average", 3.0) / 5.0
                base_confidence = (base_confidence + profile_confidence) / 2

            return min(1.0, max(0.0, base_confidence))

        except Exception:
            return 0.7

    def _generate_semantic_style_prompt(self, semantic_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯­ä¹‰é£æ ¼æç¤ºè¯"""
        try:
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                classification = final_profile.get("style_classification", {})
                primary_style = classification.get("primary_style", "ç»¼åˆå‹")
                characteristics = classification.get("style_characteristics", [])

                prompt_parts = [f"è¯·æŒ‰ç…§{primary_style}è¿›è¡Œå†™ä½œ"]

                if characteristics:
                    prompt_parts.append(f"ç‰¹ç‚¹ï¼š{', '.join(characteristics)}")

                # æ·»åŠ å…·ä½“çš„è¯­ä¹‰æŒ‡å¯¼
                style_scores = final_profile.get("style_scores", {})
                if style_scores.get("conceptual_organization", 0) > 4.0:
                    prompt_parts.append("æ³¨é‡æ¦‚å¿µçš„ç³»ç»Ÿæ€§ç»„ç»‡")
                if style_scores.get("creative_association", 0) > 4.0:
                    prompt_parts.append("å‘æŒ¥åˆ›æ–°è”æƒ³èƒ½åŠ›")
                if style_scores.get("emotional_expression", 0) > 4.0:
                    prompt_parts.append("å¢å¼ºæƒ…æ„Ÿè¡¨è¾¾åŠ›")

                return "ï¼›".join(prompt_parts)

            return "è¯·ä¿æŒè¯­ä¹‰è¿è´¯å’Œé€»è¾‘æ¸…æ™°çš„å†™ä½œé£æ ¼"

        except Exception:
            return "è¯·ä¿æŒåŸæœ‰çš„å†™ä½œé£æ ¼"

    def _generate_semantic_recommendations(self, semantic_result: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¯­ä¹‰åˆ†æå»ºè®®"""
        recommendations = []

        try:
            # ä»ç»¼åˆæ´å¯Ÿä¸­æå–å»ºè®®
            comprehensive_insights = semantic_result.get("comprehensive_insights", {})
            actionable_recommendations = comprehensive_insights.get("actionable_recommendations", [])
            recommendations.extend(actionable_recommendations[:3])

            # ä»æœ€ç»ˆç”»åƒä¸­æå–æ”¹è¿›å»ºè®®
            final_profile = semantic_result.get("semantic_analysis", {}).get("final_profile", {})
            if final_profile.get("success"):
                profile_summary = final_profile.get("profile_summary", {})
                improvements = profile_summary.get("potential_improvements", [])
                for improvement in improvements[:2]:
                    recommendations.append(f"å»ºè®®æå‡{improvement}")

            # å¦‚æœæ²¡æœ‰å…·ä½“å»ºè®®ï¼Œæä¾›é€šç”¨å»ºè®®
            if not recommendations:
                recommendations = [
                    "å»ºè®®ä¿æŒæ¦‚å¿µç»„ç»‡çš„ç³»ç»Ÿæ€§",
                    "æ³¨æ„è¯­ä¹‰è¿è´¯æ€§å’Œé€»è¾‘æ€§",
                    "é€‚å½“å¢åŠ åˆ›æ–°æ€§è¡¨è¾¾"
                ]

            return recommendations[:5]

        except Exception:
            return ["å»ºè®®ä¿æŒå½“å‰çš„è¯­ä¹‰é£æ ¼ç‰¹å¾"]

    def compare_semantic_styles(self, document1_content: str, document2_content: str,
                              doc1_name: str = None, doc2_name: str = None) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£çš„è¯­ä¹‰é£æ ¼

        Args:
            document1_content: ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹
            document2_content: ç¬¬äºŒä¸ªæ–‡æ¡£å†…å®¹
            doc1_name: ç¬¬ä¸€ä¸ªæ–‡æ¡£åç§°
            doc2_name: ç¬¬äºŒä¸ªæ–‡æ¡£åç§°

        Returns:
            è¯­ä¹‰é£æ ¼æ¯”è¾ƒç»“æœ
        """
        if not self.use_enhanced_features or not self.enhanced_processor:
            return {"error": "å¢å¼ºåŠŸèƒ½æœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰é£æ ¼æ¯”è¾ƒ"}

        comparison_result = {
            "comparison_time": datetime.now().isoformat(),
            "document1_name": doc1_name or "æ–‡æ¡£1",
            "document2_name": doc2_name or "æ–‡æ¡£2",
            "document1_analysis": {},
            "document2_analysis": {},
            "semantic_comparison": {},
            "style_compatibility": "unknown",
            "comparison_summary": {},
            "success": False
        }

        try:
            print(f"ğŸ” å¼€å§‹è¯­ä¹‰é£æ ¼æ¯”è¾ƒ: {doc1_name or 'æ–‡æ¡£1'} vs {doc2_name or 'æ–‡æ¡£2'}")

            # åˆ†æç¬¬ä¸€ä¸ªæ–‡æ¡£
            doc1_analysis = self.analyze_with_semantic_behavior(document1_content, doc1_name)
            comparison_result["document1_analysis"] = doc1_analysis

            # åˆ†æç¬¬äºŒä¸ªæ–‡æ¡£
            doc2_analysis = self.analyze_with_semantic_behavior(document2_content, doc2_name)
            comparison_result["document2_analysis"] = doc2_analysis

            # å¦‚æœä¸¤ä¸ªåˆ†æéƒ½æˆåŠŸï¼Œè¿›è¡Œæ¯”è¾ƒ
            if (not doc1_analysis.get("error") and not doc2_analysis.get("error") and
                self.enhanced_processor.semantic_analysis_enabled):

                # ä½¿ç”¨ç»¼åˆå¤„ç†å™¨çš„è¯­ä¹‰æ¯”è¾ƒåŠŸèƒ½
                semantic_comparison = self.enhanced_processor.compare_semantic_profiles(
                    document1_content, document2_content, doc1_name, doc2_name
                )
                comparison_result["semantic_comparison"] = semantic_comparison

                # ç”Ÿæˆå…¼å®¹æ€§è¯„ä¼°
                if semantic_comparison.get("success"):
                    profile_comparison = semantic_comparison.get("profile_comparison", {})
                    similarity_score = profile_comparison.get("similarity_score", 0.0)

                    if similarity_score > 0.8:
                        comparison_result["style_compatibility"] = "é«˜åº¦å…¼å®¹"
                    elif similarity_score > 0.6:
                        comparison_result["style_compatibility"] = "è¾ƒä¸ºå…¼å®¹"
                    elif similarity_score > 0.4:
                        comparison_result["style_compatibility"] = "éƒ¨åˆ†å…¼å®¹"
                    else:
                        comparison_result["style_compatibility"] = "å·®å¼‚è¾ƒå¤§"

                # ç”Ÿæˆæ¯”è¾ƒæ‘˜è¦
                comparison_result["comparison_summary"] = self._generate_semantic_comparison_summary(
                    doc1_analysis, doc2_analysis, semantic_comparison
                )

                comparison_result["success"] = True
                print("âœ… è¯­ä¹‰é£æ ¼æ¯”è¾ƒå®Œæˆ")
            else:
                comparison_result["error"] = "æ–‡æ¡£åˆ†æå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰æ¯”è¾ƒ"
                print("âŒ è¯­ä¹‰é£æ ¼æ¯”è¾ƒå¤±è´¥")

        except Exception as e:
            comparison_result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰é£æ ¼æ¯”è¾ƒå¤±è´¥: {str(e)}")

        return comparison_result

    def _generate_semantic_comparison_summary(self, doc1_analysis: Dict[str, Any],
                                            doc2_analysis: Dict[str, Any],
                                            semantic_comparison: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯­ä¹‰æ¯”è¾ƒæ‘˜è¦"""
        summary = {
            "overall_similarity": 0.0,
            "style_differences": [],
            "common_characteristics": [],
            "recommendation": ""
        }

        try:
            # æ•´ä½“ç›¸ä¼¼åº¦
            if semantic_comparison.get("success"):
                profile_comparison = semantic_comparison.get("profile_comparison", {})
                summary["overall_similarity"] = profile_comparison.get("similarity_score", 0.0)

                # ç»´åº¦å·®å¼‚
                dimension_diffs = profile_comparison.get("dimension_differences", {})
                differences = []
                similarities = []

                for dimension, diff_data in dimension_diffs.items():
                    difference = diff_data.get("difference", 0)
                    if difference > 1.0:  # å·®å¼‚è¾ƒå¤§
                        differences.append(f"{dimension}å·®å¼‚è¾ƒå¤§")
                    elif difference < 0.5:  # ç›¸ä¼¼åº¦è¾ƒé«˜
                        similarities.append(f"{dimension}è¾ƒä¸ºç›¸ä¼¼")

                summary["style_differences"] = differences[:3]
                summary["common_characteristics"] = similarities[:3]

            # ç”Ÿæˆå»ºè®®
            similarity_score = summary["overall_similarity"]
            if similarity_score > 0.7:
                summary["recommendation"] = "ä¸¤ä¸ªæ–‡æ¡£é£æ ¼ç›¸è¿‘ï¼Œå¯ä»¥è¿›è¡Œé£æ ¼å¯¹é½"
            elif similarity_score > 0.4:
                summary["recommendation"] = "ä¸¤ä¸ªæ–‡æ¡£é£æ ¼æœ‰ä¸€å®šå·®å¼‚ï¼Œå»ºè®®é‡ç‚¹è°ƒæ•´å·®å¼‚è¾ƒå¤§çš„ç»´åº¦"
            else:
                summary["recommendation"] = "ä¸¤ä¸ªæ–‡æ¡£é£æ ¼å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦å…¨é¢çš„é£æ ¼è¿ç§»"

        except Exception as e:
            summary["error"] = str(e)

        return summary

    def _get_document_statistics(self, content: str) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        lines = content.split('\n')
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        words = re.findall(r'[\u4e00-\u9fff]+', content)

        return {
            "total_characters": len(content),
            "total_lines": len(lines),
            "total_sentences": len([s for s in sentences if s.strip()]),
            "total_paragraphs": len(paragraphs),
            "total_words": len(words),
            "average_sentence_length": round(len(content) / max(len([s for s in sentences if s.strip()]), 1), 1),
            "average_paragraph_length": round(len(content) / max(len(paragraphs), 1), 1),
            "reading_time_minutes": round(len(words) / 200, 1)  # å‡è®¾æ¯åˆ†é’Ÿ200å­—
        }

    def _generate_detailed_analysis(self, content: str, features: Dict[str, Any], style_type: str) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        return {
            "readability_analysis": self._analyze_readability(content, features),
            "tone_analysis": self._analyze_tone_details(content, features),
            "structure_analysis": self._analyze_structure_details(content, features),
            "vocabulary_analysis": self._analyze_vocabulary_details(content, features),
            "style_consistency": self._analyze_style_consistency(content, features)
        }

    def _generate_writing_recommendations(self, features: Dict[str, Any], style_type: str) -> List[str]:
        """ç”Ÿæˆå†™ä½œå»ºè®®"""
        recommendations = []

        # åŸºäºå¥å¼ç»“æ„çš„å»ºè®®
        sentence_features = features.get("sentence_structure", {})
        avg_length = sentence_features.get("average_length", 15)

        if avg_length > 30:
            recommendations.append("å¥å­åé•¿ï¼Œå»ºè®®é€‚å½“æ‹†åˆ†ä¸ºçŸ­å¥ä»¥æé«˜å¯è¯»æ€§")
        elif avg_length < 10:
            recommendations.append("å¥å­åçŸ­ï¼Œå¯ä»¥é€‚å½“å¢åŠ å¥å­çš„å®Œæ•´æ€§å’Œè¡¨è¾¾åŠ›")

        # åŸºäºè¯æ±‡é€‰æ‹©çš„å»ºè®®
        vocab_features = features.get("vocabulary_choice", {})
        modifier_usage = vocab_features.get("modifier_usage", 0)

        if modifier_usage > 50:
            recommendations.append("ä¿®é¥°è¯ä½¿ç”¨è¾ƒå¤šï¼Œå»ºè®®ç²¾ç®€è¡¨è¾¾ï¼Œçªå‡ºé‡ç‚¹")
        elif modifier_usage < 10:
            recommendations.append("å¯ä»¥é€‚å½“å¢åŠ ä¿®é¥°è¯ï¼Œä¸°å¯Œè¡¨è¾¾å±‚æ¬¡")

        # åŸºäºæ–‡é£ç±»å‹çš„å»ºè®®
        if style_type == "business_professional":
            recommendations.append("ä¿æŒä¸“ä¸šæ€§ï¼Œæ³¨æ„ç”¨è¯å‡†ç¡®æ€§å’Œé€»è¾‘æ¸…æ™°")
        elif style_type == "academic_research":
            recommendations.append("å¢å¼ºè®ºè¯ä¸¥å¯†æ€§ï¼Œæ³¨æ„å¼•ç”¨å’Œæ•°æ®æ”¯æ’‘")
        elif style_type == "concise_practical":
            recommendations.append("ç»§ç»­ä¿æŒç®€æ´æ˜äº†ï¼Œçªå‡ºå®ç”¨æ€§")

        return recommendations

    def _generate_style_comparison(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé£æ ¼å¯¹æ¯”åˆ†æ"""
        return {
            "formal_vs_informal": self._compare_formality(features),
            "technical_vs_general": self._compare_technicality(features),
            "objective_vs_subjective": self._compare_objectivity(features),
            "concise_vs_elaborate": self._compare_conciseness(features)
        }

    def _analyze_style_features(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡é£ç‰¹å¾"""
        if not content or not content.strip():
            return self._get_empty_features()

        features = {}

        try:
            # å¥å¼ç»“æ„åˆ†æ
            features["sentence_structure"] = self._analyze_sentence_structure(content)

            # è¯æ±‡é€‰æ‹©åˆ†æ
            features["vocabulary_choice"] = self._analyze_vocabulary_choice(content)

            # è¡¨è¾¾æ–¹å¼åˆ†æ
            features["expression_style"] = self._analyze_expression_style(content)

            # æ–‡æœ¬ç»„ç»‡åˆ†æ
            features["text_organization"] = self._analyze_text_organization(content)

            # è¯­è¨€ä¹ æƒ¯åˆ†æ
            features["language_habits"] = self._analyze_language_habits(content)

            # æ–°å¢ï¼šæƒ…æ„Ÿè‰²å½©åˆ†æ
            features["emotional_tone"] = self._analyze_emotional_tone(content)

            # æ–°å¢ï¼šä¸“ä¸šæ€§åˆ†æ
            features["professionalism"] = self._analyze_professionalism(content)

            # æ–°å¢ï¼šä¿®è¾æ‰‹æ³•åˆ†æ
            features["rhetorical_devices"] = self._analyze_rhetorical_devices(content)

        except Exception as e:
            print(f"æ–‡é£ç‰¹å¾åˆ†æå‡ºé”™: {str(e)}")
            features = self._get_empty_features()

        return features

    def _get_empty_features(self) -> Dict[str, Any]:
        """è·å–ç©ºçš„ç‰¹å¾ç»“æ„"""
        return {
            "sentence_structure": {"average_length": 0, "long_short_ratio": 0, "complex_ratio": 0, "total_sentences": 0},
            "vocabulary_choice": {"formality_score": 0, "technical_density": 0, "modifier_usage": 0, "action_verb_ratio": 0},
            "expression_style": {"passive_active_ratio": 0, "person_usage": {"first_person": 0, "second_person": 0, "third_person": 0}, "tone_strength": 0},
            "text_organization": {"paragraph_count": 0, "average_paragraph_length": 0, "connector_density": 0, "summary_usage": 0},
            "language_habits": {"colloquial_level": 0, "formal_structure_usage": 0, "de_structure_density": 0},
            "emotional_tone": {"positive_ratio": 0, "negative_ratio": 0, "neutral_ratio": 1.0, "intensity": 0},
            "professionalism": {"domain_specificity": 0, "authority_indicators": 0, "precision_level": 0},
            "rhetorical_devices": {"metaphor_usage": 0, "parallel_structure": 0, "question_usage": 0}
        }
    
    def _analyze_sentence_structure(self, content: str) -> Dict[str, Any]:
        """åˆ†æå¥å¼ç»“æ„"""
        # æ¸…ç†å†…å®¹ï¼Œç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content.strip())

        # æ›´ç²¾ç¡®çš„å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\.!?;]', content)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]

        if not sentences:
            return {
                "average_length": 0,
                "long_short_ratio": 0,
                "complex_ratio": 0,
                "total_sentences": 0
            }

        # è®¡ç®—å¹³å‡å¥é•¿
        total_chars = sum(len(s) for s in sentences)
        average_length = total_chars / len(sentences)

        # é•¿çŸ­å¥æ¯”ä¾‹ï¼ˆé•¿å¥å®šä¹‰ä¸ºè¶…è¿‡25å­—ï¼‰
        long_sentences = [s for s in sentences if len(s) > 25]
        short_sentences = [s for s in sentences if len(s) <= 15]
        long_short_ratio = len(long_sentences) / len(sentences) if sentences else 0

        # å¤åˆå¥æ¯”ä¾‹ï¼ˆåŒ…å«é€—å·ã€åˆ†å·ã€è¿è¯ç­‰çš„å¥å­ï¼‰
        complex_patterns = [r'ï¼Œ', r'ï¼›', r'ï¼š', r'è€Œä¸”', r'ä½†æ˜¯', r'ç„¶è€Œ', r'å› ä¸º', r'æ‰€ä»¥', r'å¦‚æœ', r'è™½ç„¶']
        complex_sentences = []
        for sentence in sentences:
            if any(re.search(pattern, sentence) for pattern in complex_patterns):
                complex_sentences.append(sentence)
        complex_ratio = len(complex_sentences) / len(sentences) if sentences else 0

        return {
            "average_length": round(average_length, 1),
            "long_short_ratio": round(long_short_ratio, 3),
            "complex_ratio": round(complex_ratio, 3),
            "total_sentences": len(sentences),
            "long_sentences": len(long_sentences),
            "short_sentences": len(short_sentences)
        }
    
    def _analyze_vocabulary_choice(self, content: str) -> Dict[str, Any]:
        """åˆ†æè¯æ±‡é€‰æ‹©"""
        # æ¸…ç†å†…å®¹
        content_clean = re.sub(r'\s+', '', content)
        total_chars = len(content_clean)

        if total_chars == 0:
            return {
                "formality_score": 0,
                "technical_density": 0,
                "modifier_usage": 0,
                "action_verb_ratio": 0
            }

        # æ­£å¼è¯æ±‡æ¨¡å¼
        formal_words = ["æ ¹æ®", "æŒ‰ç…§", "ä¾æ®", "é‰´äº", "åŸºäº", "å…³äº", "é’ˆå¯¹", "ä¸ºäº†", "é€šè¿‡", "é‡‡ç”¨",
                       "è¿›è¡Œ", "å®æ–½", "å¼€å±•", "è½å®", "ç¡®ä¿", "ä¿è¯", "ç»´æŠ¤", "ä¿ƒè¿›", "æ¨åŠ¨", "åŠ å¼º"]
        formal_count = sum(content.count(word) for word in formal_words)

        # ä¸“ä¸šæœ¯è¯­æ¨¡å¼
        technical_patterns = [
            r'[\u4e00-\u9fff]+ç³»ç»Ÿ', r'[\u4e00-\u9fff]+æŠ€æœ¯', r'[\u4e00-\u9fff]+æ–¹æ¡ˆ',
            r'[\u4e00-\u9fff]+æ ‡å‡†', r'[\u4e00-\u9fff]+è§„èŒƒ', r'[\u4e00-\u9fff]+å¹³å°',
            r'[\u4e00-\u9fff]+æ¨¡å¼', r'[\u4e00-\u9fff]+æœºåˆ¶', r'[\u4e00-\u9fff]+æµç¨‹'
        ]
        technical_count = sum(len(re.findall(pattern, content)) for pattern in technical_patterns)

        # ä¿®é¥°è¯ä½¿ç”¨
        modifiers = ["å¾ˆ", "éå¸¸", "ç‰¹åˆ«", "æå…¶", "ç›¸å½“", "æ¯”è¾ƒ", "è¾ƒä¸º", "ååˆ†", "æ›´åŠ ", "è¿›ä¸€æ­¥"]
        modifier_count = sum(content.count(word) for word in modifiers)

        # åŠ¨è¯ç±»å‹ï¼ˆåŠ¨ä½œåŠ¨è¯ï¼‰
        action_verbs = ["å®æ–½", "æ‰§è¡Œ", "å¼€å±•", "æ¨è¿›", "è½å®", "å®Œæˆ", "è¾¾æˆ", "å®ç°", "æå‡", "ä¼˜åŒ–",
                       "æ”¹è¿›", "å»ºè®¾", "æ„å»º", "å‘å±•", "åˆ›æ–°", "çªç ´", "è§£å†³", "å¤„ç†", "ç®¡ç†", "è¿è¥"]
        action_count = sum(content.count(word) for word in action_verbs)

        # è®¡ç®—å¯†åº¦ï¼ˆæ¯åƒå­—ï¼‰
        multiplier = 1000 / total_chars if total_chars > 0 else 0

        return {
            "formality_score": round(formal_count * multiplier, 2),
            "technical_density": round(technical_count * multiplier, 2),
            "modifier_usage": round(modifier_count * multiplier, 2),
            "action_verb_ratio": round(action_count * multiplier, 2)
        }
    
    def _analyze_expression_style(self, content: str) -> Dict[str, Any]:
        """åˆ†æè¡¨è¾¾æ–¹å¼"""
        total_chars = len(content)

        if total_chars == 0:
            return {
                "passive_active_ratio": 0,
                "person_usage": {"first_person": 0, "second_person": 0, "third_person": 0},
                "tone_strength": 0
            }

        # è¢«åŠ¨è¯­æ€æ£€æµ‹ï¼ˆæ›´ç²¾ç¡®çš„æ¨¡å¼ï¼‰
        passive_patterns = [
            r'[\u4e00-\u9fff]+è¢«[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+å—åˆ°[\u4e00-\u9fff]+',
            r'[\u4e00-\u9fff]+å¾—åˆ°[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+è·å¾—[\u4e00-\u9fff]+',
            r'[\u4e00-\u9fff]+é­åˆ°[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+å—[\u4e00-\u9fff]+å½±å“'
        ]
        passive_count = sum(len(re.findall(pattern, content)) for pattern in passive_patterns)

        # ä¸»åŠ¨è¯­æ€æ£€æµ‹
        active_patterns = [
            r'[\u4e00-\u9fff]+è¿›è¡Œ[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+å¼€å±•[\u4e00-\u9fff]+',
            r'[\u4e00-\u9fff]+å®æ–½[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+æ¨è¿›[\u4e00-\u9fff]+',
            r'[\u4e00-\u9fff]+å®Œæˆ[\u4e00-\u9fff]+', r'[\u4e00-\u9fff]+å»ºè®¾[\u4e00-\u9fff]+'
        ]
        active_count = sum(len(re.findall(pattern, content)) for pattern in active_patterns)

        # äººç§°ä½¿ç”¨ç»Ÿè®¡
        first_person = content.count('æˆ‘') + content.count('æˆ‘ä»¬') + content.count('æœ¬äºº') + content.count('ç¬”è€…')
        second_person = content.count('ä½ ') + content.count('æ‚¨') + content.count('ä½ ä»¬') + content.count('å„ä½')
        third_person = content.count('ä»–') + content.count('å¥¹') + content.count('å®ƒ') + content.count('ä»–ä»¬') + content.count('å¥¹ä»¬')

        # è¯­æ°”å¼ºåº¦åˆ†æ
        strong_words = ['å¿…é¡»', 'åŠ¡å¿…', 'ä¸¥ç¦', 'ç»å¯¹', 'ä¸€å®š', 'åšå†³', 'ä¸¥æ ¼', 'åˆ‡å®']
        mild_words = ['å»ºè®®', 'å¸Œæœ›', 'å¯ä»¥', 'å°½é‡', 'é€‚å½“', 'é…Œæƒ…', 'å¯èƒ½', 'æˆ–è®¸']

        strong_tone = sum(content.count(word) for word in strong_words)
        mild_tone = sum(content.count(word) for word in mild_words)

        # è®¡ç®—æ¯”ä¾‹
        total_voice_patterns = passive_count + active_count
        passive_ratio = passive_count / total_voice_patterns if total_voice_patterns > 0 else 0

        return {
            "passive_active_ratio": round(passive_ratio, 3),
            "person_usage": {
                "first_person": first_person,
                "second_person": second_person,
                "third_person": third_person
            },
            "tone_strength": round((strong_tone - mild_tone) / total_chars * 1000, 2) if total_chars > 0 else 0,
            "passive_count": passive_count,
            "active_count": active_count
        }
    
    def _analyze_text_organization(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬ç»„ç»‡"""
        # æ®µè½åˆ†æï¼ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²ï¼‰
        paragraphs = re.split(r'\n\s*\n', content)
        paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 10]

        total_chars = len(content)

        if total_chars == 0:
            return {
                "paragraph_count": 0,
                "average_paragraph_length": 0,
                "connector_density": 0,
                "summary_usage": 0
            }

        # é€»è¾‘è¿æ¥è¯ï¼ˆæ›´å…¨é¢ï¼‰
        connectors = [
            "é¦–å…ˆ", "å…¶æ¬¡", "ç„¶å", "æœ€å", "å› æ­¤", "æ‰€ä»¥", "ä½†æ˜¯", "ç„¶è€Œ", "æ­¤å¤–", "å¦å¤–",
            "åŒæ—¶", "è€Œä¸”", "å¹¶ä¸”", "ä¸è¿‡", "è™½ç„¶", "å°½ç®¡", "ç”±äº", "é‰´äº", "åŸºäº", "æ ¹æ®",
            "æ€»ä¹‹", "ç»¼ä¸Š", "å¦ä¸€æ–¹é¢", "ä¸æ­¤åŒæ—¶", "ç›¸æ¯”ä¹‹ä¸‹", "æ¢è¨€ä¹‹", "ä¹Ÿå°±æ˜¯è¯´"
        ]
        connector_count = sum(content.count(word) for word in connectors)

        # æ€»ç»“æ€§è¡¨è¾¾
        summary_words = ["æ€»ä¹‹", "ç»¼ä¸Š", "æ€»çš„æ¥è¯´", "ç»¼åˆä»¥ä¸Š", "æ€»è€Œè¨€ä¹‹", "ç»¼ä¸Šæ‰€è¿°", "ç”±æ­¤å¯è§", "å¯ä»¥çœ‹å‡º"]
        summary_count = sum(content.count(word) for word in summary_words)

        # åˆ—ä¸¾æ ‡è¯†
        enumeration_patterns = [r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€', r'\d+[\.ã€]', r'[ï¼ˆ(]\d+[ï¼‰)]']
        enumeration_count = sum(len(re.findall(pattern, content)) for pattern in enumeration_patterns)

        return {
            "paragraph_count": len(paragraphs),
            "average_paragraph_length": round(sum(len(p) for p in paragraphs) / len(paragraphs), 1) if paragraphs else 0,
            "connector_density": round(connector_count / total_chars * 1000, 2),
            "summary_usage": summary_count,
            "enumeration_usage": enumeration_count
        }
    
    def _analyze_language_habits(self, content: str) -> Dict[str, Any]:
        """åˆ†æè¯­è¨€ä¹ æƒ¯"""
        total_chars = len(content)

        if total_chars == 0:
            return {
                "colloquial_level": 0,
                "formal_structure_usage": 0,
                "de_structure_density": 0
            }

        # å£è¯­åŒ–è¯æ±‡
        colloquial_words = [
            "æŒº", "è›®", "ç‰¹åˆ«", "è¶…çº§", "å¥½åƒ", "æ„Ÿè§‰", "è§‰å¾—", "åº”è¯¥", "å¯èƒ½", "å¤§æ¦‚",
            "å·®ä¸å¤š", "åŸºæœ¬ä¸Š", "ä¸€èˆ¬æ¥è¯´", "è¯´å®è¯", "è€å®è¯´", "å¦ç™½è¯´", "çœŸçš„", "ç¡®å®"
        ]
        colloquial_count = sum(content.count(word) for word in colloquial_words)

        # ä¹¦é¢è¯­ç»“æ„è¯
        formal_structures = [
            "ä¹‹", "å…¶", "æ‰€", "ä¹ƒ", "å³", "äº¦", "ä¸”", "è€Œ", "äº", "ä»¥", "ä¸º", "ä¸",
            "åŠ", "æˆ–", "è‹¥", "åˆ™", "æ•…", "å› ", "ç”±", "è‡ª", "ä»", "å‘", "è‡³", "åŠå…¶"
        ]
        formal_count = sum(content.count(word) for word in formal_structures)

        # "çš„"å­—ç»“æ„å¯†åº¦
        de_count = content.count('çš„')

        # ä¹¦é¢è¯­å¥å¼
        formal_patterns = [r'[\u4e00-\u9fff]+ä¹‹[\u4e00-\u9fff]+', r'æ‰€[\u4e00-\u9fff]+', r'å…¶[\u4e00-\u9fff]+']
        formal_pattern_count = sum(len(re.findall(pattern, content)) for pattern in formal_patterns)

        return {
            "colloquial_level": round(colloquial_count / total_chars * 1000, 2),
            "formal_structure_usage": round((formal_count + formal_pattern_count) / total_chars * 1000, 2),
            "de_structure_density": round(de_count / total_chars * 1000, 2)
        }

    def _analyze_emotional_tone(self, content: str) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿè‰²å½©"""
        total_chars = len(content)
        if total_chars == 0:
            return {"positive_ratio": 0, "negative_ratio": 0, "neutral_ratio": 1.0, "intensity": 0}

        # ç§¯æè¯æ±‡
        positive_words = ["ä¼˜ç§€", "å“è¶Š", "æˆåŠŸ", "è¿›æ­¥", "æå‡", "æ”¹å–„", "åˆ›æ–°", "çªç ´", "å‘å±•", "å¢é•¿",
                         "æ»¡æ„", "é«˜å…´", "å–œæ‚¦", "èµæ‰¬", "è¡¨å½°", "è‚¯å®š", "æ”¯æŒ", "é¼“åŠ±", "å¸Œæœ›", "ä¿¡å¿ƒ"]

        # æ¶ˆæè¯æ±‡
        negative_words = ["é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜", "ä¸è¶³", "ç¼ºé™·", "é”™è¯¯", "å¤±è´¥", "ä¸‹é™", "å‡å°‘", "æŸå¤±",
                         "æ‹…å¿ƒ", "å¿§è™‘", "æ‰¹è¯„", "è´¨ç–‘", "åå¯¹", "æ‹’ç»", "å¦å®š", "è­¦å‘Š", "é£é™©", "å±æœº"]

        # å¼ºåº¦è¯æ±‡
        intensity_words = ["éå¸¸", "æå…¶", "ç‰¹åˆ«", "ååˆ†", "ç›¸å½“", "å¾ˆ", "æœ€", "æ›´", "è¿›ä¸€æ­¥", "å¤§å¹…"]

        positive_count = sum(content.count(word) for word in positive_words)
        negative_count = sum(content.count(word) for word in negative_words)
        intensity_count = sum(content.count(word) for word in intensity_words)

        total_emotional = positive_count + negative_count
        if total_emotional == 0:
            return {"positive_ratio": 0, "negative_ratio": 0, "neutral_ratio": 1.0, "intensity": 0}

        return {
            "positive_ratio": round(positive_count / total_emotional, 2),
            "negative_ratio": round(negative_count / total_emotional, 2),
            "neutral_ratio": round(1 - (positive_count + negative_count) / total_emotional, 2),
            "intensity": round(intensity_count / total_chars * 1000, 2)
        }

    def _analyze_professionalism(self, content: str) -> Dict[str, Any]:
        """åˆ†æä¸“ä¸šæ€§ç‰¹å¾"""
        total_chars = len(content)
        if total_chars == 0:
            return {"domain_specificity": 0, "authority_indicators": 0, "precision_level": 0}

        # æƒå¨æ€§æŒ‡æ ‡è¯æ±‡
        authority_words = ["æ ¹æ®", "ä¾æ®", "æŒ‰ç…§", "ç ”ç©¶è¡¨æ˜", "æ•°æ®æ˜¾ç¤º", "ç»Ÿè®¡", "è°ƒæŸ¥", "åˆ†æ",
                          "ä¸“å®¶", "å­¦è€…", "æƒå¨", "å®˜æ–¹", "æ­£å¼", "æ³•è§„", "æ ‡å‡†", "è§„èŒƒ"]

        # ç²¾ç¡®æ€§æŒ‡æ ‡
        precision_patterns = [r'\d+%', r'\d+\.\d+', r'ç¬¬\d+', r'\d+å¹´\d+æœˆ', r'\d+ä¸‡', r'\d+äº¿']

        # é¢†åŸŸç‰¹å®šè¯æ±‡ï¼ˆç¤ºä¾‹ï¼‰
        domain_words = ["æŠ€æœ¯", "ç³»ç»Ÿ", "å¹³å°", "æ–¹æ¡ˆ", "ç­–ç•¥", "æœºåˆ¶", "æ¨¡å¼", "æ¡†æ¶", "ä½“ç³»", "æµç¨‹"]

        authority_count = sum(content.count(word) for word in authority_words)
        domain_count = sum(content.count(word) for word in domain_words)
        precision_count = sum(len(re.findall(pattern, content)) for pattern in precision_patterns)

        return {
            "domain_specificity": round(domain_count / total_chars * 1000, 2),
            "authority_indicators": round(authority_count / total_chars * 1000, 2),
            "precision_level": round(precision_count / total_chars * 1000, 2)
        }

    def _analyze_rhetorical_devices(self, content: str) -> Dict[str, Any]:
        """åˆ†æä¿®è¾æ‰‹æ³•"""
        total_chars = len(content)
        if total_chars == 0:
            return {"metaphor_usage": 0, "parallel_structure": 0, "question_usage": 0}

        # æ¯”å–»è¯æ±‡
        metaphor_words = ["å¦‚åŒ", "å¥½æ¯”", "çŠ¹å¦‚", "ä»¿ä½›", "åƒ", "ä¼¼", "å®›å¦‚", "æ°ä¼¼"]

        # æ’æ¯”ç»“æ„æ¨¡å¼
        parallel_patterns = [r'[\u4e00-\u9fff]+ï¼Œ[\u4e00-\u9fff]+ï¼Œ[\u4e00-\u9fff]+',
                           r'ä¸ä»…[\u4e00-\u9fff]+ï¼Œè€Œä¸”[\u4e00-\u9fff]+',
                           r'æ—¢[\u4e00-\u9fff]+ï¼Œåˆ[\u4e00-\u9fff]+']

        # ç–‘é—®å¥
        question_count = content.count('ï¼Ÿ') + content.count('?')

        metaphor_count = sum(content.count(word) for word in metaphor_words)
        parallel_count = sum(len(re.findall(pattern, content)) for pattern in parallel_patterns)

        return {
            "metaphor_usage": round(metaphor_count / total_chars * 1000, 2),
            "parallel_structure": round(parallel_count / total_chars * 1000, 2),
            "question_usage": round(question_count / total_chars * 1000, 2)
        }

    def _analyze_readability(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå¯è¯»æ€§"""
        sentence_features = features.get("sentence_structure", {})
        vocab_features = features.get("vocabulary_choice", {})

        # è®¡ç®—å¯è¯»æ€§åˆ†æ•° (ç®€åŒ–ç‰ˆ)
        avg_sentence_length = sentence_features.get("average_length", 15)
        technical_density = vocab_features.get("technical_density", 0)

        readability_score = max(0, min(100, 100 - (avg_sentence_length - 15) * 2 - technical_density))

        if readability_score >= 80:
            level = "å¾ˆå®¹æ˜“é˜…è¯»"
        elif readability_score >= 60:
            level = "è¾ƒå®¹æ˜“é˜…è¯»"
        elif readability_score >= 40:
            level = "ä¸­ç­‰éš¾åº¦"
        elif readability_score >= 20:
            level = "è¾ƒéš¾é˜…è¯»"
        else:
            level = "å¾ˆéš¾é˜…è¯»"

        return {
            "readability_score": round(readability_score, 1),
            "readability_level": level,
            "factors": {
                "sentence_complexity": "é«˜" if avg_sentence_length > 25 else "ä¸­" if avg_sentence_length > 15 else "ä½",
                "vocabulary_difficulty": "é«˜" if technical_density > 20 else "ä¸­" if technical_density > 10 else "ä½"
            }
        }

    def _analyze_tone_details(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯­è°ƒè¯¦æƒ…"""
        emotional_features = features.get("emotional_tone", {})

        # åˆ†æè¯­è°ƒå€¾å‘
        positive_words = ["å¥½", "ä¼˜ç§€", "æˆåŠŸ", "æå‡", "æ”¹å–„", "åˆ›æ–°", "å‘å±•", "è¿›æ­¥"]
        negative_words = ["é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜", "ä¸è¶³", "ç¼ºé™·", "å¤±è´¥", "ä¸‹é™", "å‡å°‘"]
        neutral_words = ["åˆ†æ", "ç ”ç©¶", "æ¢è®¨", "è€ƒè™‘", "å»ºè®®", "æ–¹æ¡ˆ", "è®¡åˆ’", "å®æ–½"]

        positive_count = sum(content.count(word) for word in positive_words)
        negative_count = sum(content.count(word) for word in negative_words)
        neutral_count = sum(content.count(word) for word in neutral_words)

        total_tone_words = positive_count + negative_count + neutral_count

        if total_tone_words > 0:
            tone_distribution = {
                "positive_ratio": round(positive_count / total_tone_words * 100, 1),
                "negative_ratio": round(negative_count / total_tone_words * 100, 1),
                "neutral_ratio": round(neutral_count / total_tone_words * 100, 1)
            }
        else:
            tone_distribution = {"positive_ratio": 33.3, "negative_ratio": 33.3, "neutral_ratio": 33.3}

        return {
            "tone_distribution": tone_distribution,
            "dominant_tone": max(tone_distribution.items(), key=lambda x: x[1])[0].replace("_ratio", ""),
            "emotional_intensity": emotional_features.get("intensity_score", 0),
            "tone_consistency": "é«˜" if max(tone_distribution.values()) > 60 else "ä¸­" if max(tone_distribution.values()) > 40 else "ä½"
        }

    def _analyze_structure_details(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç»“æ„è¯¦æƒ…"""
        org_features = features.get("text_organization", {})

        return {
            "paragraph_structure": {
                "paragraph_count": org_features.get("paragraph_count", 0),
                "average_length": org_features.get("average_paragraph_length", 0),
                "length_consistency": "é«˜" if org_features.get("average_paragraph_length", 0) > 100 else "ä¸­"
            },
            "logical_flow": {
                "connector_usage": org_features.get("connector_density", 0),
                "enumeration_usage": org_features.get("enumeration_usage", 0),
                "summary_usage": org_features.get("summary_usage", 0)
            },
            "organization_score": min(100, (org_features.get("connector_density", 0) * 2 +
                                           org_features.get("enumeration_usage", 0) * 5 +
                                           org_features.get("summary_usage", 0) * 10))
        }

    def _analyze_vocabulary_details(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯æ±‡è¯¦æƒ…"""
        vocab_features = features.get("vocabulary_choice", {})

        return {
            "vocabulary_richness": {
                "unique_words": len(set(re.findall(r'[\u4e00-\u9fff]+', content))),
                "total_words": len(re.findall(r'[\u4e00-\u9fff]+', content)),
                "diversity_ratio": round(len(set(re.findall(r'[\u4e00-\u9fff]+', content))) /
                                       max(len(re.findall(r'[\u4e00-\u9fff]+', content)), 1) * 100, 1)
            },
            "word_complexity": {
                "technical_density": vocab_features.get("technical_density", 0),
                "formality_score": vocab_features.get("formality_score", 0),
                "modifier_usage": vocab_features.get("modifier_usage", 0)
            },
            "action_orientation": {
                "action_verb_ratio": vocab_features.get("action_verb_ratio", 0),
                "passive_voice_usage": self._count_passive_voice(content)
            }
        }

    def _analyze_style_consistency(self, content: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé£æ ¼ä¸€è‡´æ€§"""
        # åˆ†æ®µåˆ†æé£æ ¼ä¸€è‡´æ€§
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

        if len(paragraphs) < 2:
            return {"consistency_score": 100, "variation_level": "æ— æ³•è¯„ä¼°"}

        # ç®€åŒ–çš„ä¸€è‡´æ€§åˆ†æ
        consistency_factors = []

        # å¥é•¿ä¸€è‡´æ€§
        sentence_lengths = []
        for para in paragraphs:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', para)
            avg_len = sum(len(s) for s in sentences if s.strip()) / max(len([s for s in sentences if s.strip()]), 1)
            sentence_lengths.append(avg_len)

        if sentence_lengths:
            length_variance = max(sentence_lengths) - min(sentence_lengths)
            consistency_factors.append(max(0, 100 - length_variance * 2))

        consistency_score = sum(consistency_factors) / max(len(consistency_factors), 1)

        return {
            "consistency_score": round(consistency_score, 1),
            "variation_level": "ä½" if consistency_score > 80 else "ä¸­" if consistency_score > 60 else "é«˜",
            "factors_analyzed": ["å¥é•¿ä¸€è‡´æ€§", "è¯æ±‡ä½¿ç”¨", "è¯­è°ƒå˜åŒ–"]
        }

    def _count_passive_voice(self, content: str) -> float:
        """ç»Ÿè®¡è¢«åŠ¨è¯­æ€ä½¿ç”¨é¢‘ç‡"""
        passive_patterns = [r'è¢«[\u4e00-\u9fff]+', r'å—åˆ°[\u4e00-\u9fff]+', r'å¾—åˆ°[\u4e00-\u9fff]+']
        passive_count = sum(len(re.findall(pattern, content)) for pattern in passive_patterns)
        total_chars = len(content)
        return round(passive_count / total_chars * 1000, 2) if total_chars > 0 else 0

    def _compare_formality(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”æ­£å¼æ€§"""
        vocab_features = features.get("vocabulary_choice", {})
        habit_features = features.get("language_habits", {})

        formality_score = vocab_features.get("formality_score", 0)
        formal_structure = habit_features.get("formal_structure_usage", 0)
        colloquial_level = habit_features.get("colloquial_level", 0)

        overall_formality = (formality_score + formal_structure - colloquial_level) / 3

        return {
            "formality_level": "é«˜" if overall_formality > 20 else "ä¸­" if overall_formality > 10 else "ä½",
            "formal_score": round(overall_formality, 1),
            "indicators": {
                "formal_vocabulary": formality_score,
                "formal_structures": formal_structure,
                "colloquial_elements": colloquial_level
            }
        }

    def _compare_technicality(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”æŠ€æœ¯æ€§"""
        vocab_features = features.get("vocabulary_choice", {})
        professional_features = features.get("professionalism", {})

        technical_density = vocab_features.get("technical_density", 0)
        professional_score = professional_features.get("professional_score", 0)

        return {
            "technicality_level": "é«˜" if technical_density > 20 else "ä¸­" if technical_density > 10 else "ä½",
            "technical_score": round(technical_density, 1),
            "professional_score": round(professional_score, 1),
            "balance": "æŠ€æœ¯æ€§å¼º" if technical_density > professional_score else "é€šç”¨æ€§å¼º"
        }

    def _compare_objectivity(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”å®¢è§‚æ€§"""
        emotional_features = features.get("emotional_tone", {})
        expression_features = features.get("expression_style", {})

        emotional_intensity = emotional_features.get("intensity_score", 0)
        assertive_score = expression_features.get("assertive_score", 0)

        objectivity_score = max(0, 100 - emotional_intensity * 10 - assertive_score)

        return {
            "objectivity_level": "é«˜" if objectivity_score > 70 else "ä¸­" if objectivity_score > 40 else "ä½",
            "objectivity_score": round(objectivity_score, 1),
            "subjectivity_indicators": {
                "emotional_intensity": emotional_intensity,
                "assertive_tone": assertive_score
            }
        }

    def _compare_conciseness(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”ç®€æ´æ€§"""
        sentence_features = features.get("sentence_structure", {})
        vocab_features = features.get("vocabulary_choice", {})

        avg_length = sentence_features.get("average_length", 15)
        modifier_usage = vocab_features.get("modifier_usage", 0)

        conciseness_score = max(0, 100 - (avg_length - 15) * 2 - modifier_usage)

        return {
            "conciseness_level": "é«˜" if conciseness_score > 70 else "ä¸­" if conciseness_score > 40 else "ä½",
            "conciseness_score": round(conciseness_score, 1),
            "verbosity_indicators": {
                "sentence_length": avg_length,
                "modifier_density": modifier_usage
            }
        }

    def _generate_enhanced_style_prompt(self, features: Dict[str, Any], style_type: str, detailed_analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¢å¼ºçš„æ–‡é£æç¤ºè¯"""
        prompt_parts = []

        # åŸºç¡€é£æ ¼æè¿°
        style_info = self.style_types.get(style_type, {})
        style_name = style_info.get("name", "é€šç”¨é£æ ¼")
        characteristics = style_info.get("characteristics", [])

        prompt_parts.append(f"è¯·é‡‡ç”¨{style_name}è¿›è¡Œå†™ä½œï¼Œå…·ä½“ç‰¹å¾åŒ…æ‹¬ï¼š{', '.join(characteristics)}ã€‚")

        # å¥å¼è¦æ±‚
        sentence_features = features.get("sentence_structure", {})
        avg_length = sentence_features.get("average_length", 15)

        if avg_length > 25:
            prompt_parts.append("ä½¿ç”¨è¾ƒé•¿çš„å¤åˆå¥ï¼Œæ³¨é‡è¡¨è¾¾çš„å®Œæ•´æ€§å’Œé€»è¾‘æ€§ã€‚")
        elif avg_length < 15:
            prompt_parts.append("ä½¿ç”¨ç®€æ´æ˜äº†çš„çŸ­å¥ï¼Œçªå‡ºé‡ç‚¹ï¼Œä¾¿äºç†è§£ã€‚")
        else:
            prompt_parts.append("å¥å¼é•¿çŸ­é€‚ä¸­ï¼Œå…¼é¡¾è¡¨è¾¾å®Œæ•´æ€§å’Œå¯è¯»æ€§ã€‚")

        # è¯æ±‡è¦æ±‚
        vocab_features = features.get("vocabulary_choice", {})
        formality_score = vocab_features.get("formality_score", 0)
        technical_density = vocab_features.get("technical_density", 0)

        if formality_score > 20:
            prompt_parts.append("ä½¿ç”¨æ­£å¼ã€è§„èŒƒçš„è¯æ±‡ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾ã€‚")
        elif formality_score < 10:
            prompt_parts.append("å¯ä»¥ä½¿ç”¨ç›¸å¯¹è½»æ¾ã€è‡ªç„¶çš„è¡¨è¾¾æ–¹å¼ã€‚")

        if technical_density > 15:
            prompt_parts.append("é€‚å½“ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½“ç°ä¸“ä¸šæ€§ã€‚")

        # è¯­è°ƒè¦æ±‚
        tone_analysis = detailed_analysis.get("tone_analysis", {})
        dominant_tone = tone_analysis.get("dominant_tone", "neutral")

        if dominant_tone == "positive":
            prompt_parts.append("ä¿æŒç§¯ææ­£é¢çš„è¯­è°ƒï¼Œçªå‡ºä¼˜åŠ¿å’Œæˆæœã€‚")
        elif dominant_tone == "negative":
            prompt_parts.append("å®¢è§‚åˆ†æé—®é¢˜ï¼Œæå‡ºå»ºè®¾æ€§æ„è§ã€‚")
        else:
            prompt_parts.append("ä¿æŒå®¢è§‚ä¸­æ€§çš„è¯­è°ƒï¼Œæ³¨é‡äº‹å®é™ˆè¿°ã€‚")

        # ç»“æ„è¦æ±‚
        org_features = features.get("text_organization", {})
        connector_density = org_features.get("connector_density", 0)

        if connector_density > 10:
            prompt_parts.append("æ³¨é‡é€»è¾‘è¿æ¥ï¼Œä½¿ç”¨é€‚å½“çš„è¿‡æ¸¡è¯å’Œè¿æ¥è¯ã€‚")

        # å¯è¯»æ€§è¦æ±‚
        readability = detailed_analysis.get("readability_analysis", {})
        readability_level = readability.get("readability_level", "ä¸­ç­‰éš¾åº¦")

        prompt_parts.append(f"ç¡®ä¿æ–‡æœ¬{readability_level}ï¼Œé€‚åˆç›®æ ‡è¯»è€…ç¾¤ä½“ã€‚")

        return " ".join(prompt_parts)
    
    def _identify_style_type(self, content: str, features: Dict[str, Any]) -> Tuple[str, float]:
        """è¯†åˆ«æ–‡é£ç±»å‹"""
        scores = {}

        # å®‰å…¨è·å–ç‰¹å¾å€¼çš„è¾…åŠ©å‡½æ•°
        def safe_get(feature_dict: Dict, key: str, default: float = 0.0) -> float:
            return feature_dict.get(key, default) if feature_dict else default

        for style_id, style_info in self.style_types.items():
            try:
                score = 0.0

                # åŸºäºç‰¹å¾æ¨¡å¼åŒ¹é… (æƒé‡: 0.2)
                pattern_score = 0.0
                for pattern in style_info.get("typical_patterns", []):
                    if pattern in content:
                        pattern_score += 0.04  # æ¯ä¸ªæ¨¡å¼0.04åˆ†ï¼Œæœ€å¤š5ä¸ªæ¨¡å¼
                score += min(pattern_score, 0.2)

                # åŸºäºç‰¹å¾åˆ†æç»“æœ (æƒé‡: 0.8)
                sentence_features = features.get("sentence_structure", {})
                vocab_features = features.get("vocabulary_choice", {})
                expression_features = features.get("expression_style", {})
                org_features = features.get("text_organization", {})
                habit_features = features.get("language_habits", {})
                emotional_features = features.get("emotional_tone", {})
                professional_features = features.get("professionalism", {})

                if style_id == "formal_official":
                    # æ­£å¼å…¬æ–‡é£æ ¼
                    score += min(safe_get(vocab_features, "formality_score") * 0.02, 0.2)  # æ­£å¼åº¦
                    score += min(safe_get(org_features, "connector_density") * 0.01, 0.1)  # è¿æ¥è¯å¯†åº¦
                    score += min(safe_get(professional_features, "authority_indicators") * 0.02, 0.2)  # æƒå¨æ€§
                    score += min((1 - safe_get(habit_features, "colloquial_level") * 0.01), 0.1)  # éå£è¯­åŒ–
                    score += min(safe_get(expression_features, "passive_active_ratio") * 0.2, 0.2)  # è¢«åŠ¨è¯­æ€

                elif style_id == "business_professional":
                    # å•†åŠ¡ä¸“ä¸šé£æ ¼
                    score += min(safe_get(vocab_features, "action_verb_ratio") * 0.02, 0.2)  # åŠ¨ä½œåŠ¨è¯
                    score += min((1 - safe_get(expression_features, "passive_active_ratio")) * 0.3, 0.3)  # ä¸»åŠ¨è¯­æ€
                    score += min(safe_get(professional_features, "precision_level") * 0.02, 0.1)  # ç²¾ç¡®æ€§
                    score += min(safe_get(emotional_features, "neutral_ratio") * 0.2, 0.2)  # ä¸­æ€§è¯­è°ƒ

                elif style_id == "academic_research":
                    # å­¦æœ¯ç ”ç©¶é£æ ¼
                    score += min(safe_get(sentence_features, "complex_ratio") * 0.4, 0.4)  # å¤æ‚å¥æ¯”ä¾‹
                    score += min(safe_get(vocab_features, "technical_density") * 0.05, 0.2)  # æŠ€æœ¯å¯†åº¦
                    score += min(safe_get(professional_features, "authority_indicators") * 0.02, 0.2)  # æƒå¨æ€§

                elif style_id == "narrative_descriptive":
                    # å™è¿°æè¿°é£æ ¼
                    score += min(safe_get(vocab_features, "modifier_usage") * 0.02, 0.2)  # ä¿®é¥°è¯ä½¿ç”¨
                    score += min(safe_get(habit_features, "colloquial_level") * 0.02, 0.2)  # å£è¯­åŒ–ç¨‹åº¦
                    score += min(safe_get(emotional_features, "intensity") * 0.02, 0.2)  # æƒ…æ„Ÿå¼ºåº¦
                    score += min((safe_get(emotional_features, "positive_ratio") +
                                safe_get(emotional_features, "negative_ratio")) * 0.2, 0.2)  # æƒ…æ„Ÿè‰²å½©

                elif style_id == "concise_practical":
                    # ç®€æ´å®ç”¨é£æ ¼
                    avg_length = safe_get(sentence_features, "average_length", 20)
                    if avg_length < 20:
                        score += 0.3  # çŸ­å¥åå¥½
                    else:
                        score += max(0, 0.3 - (avg_length - 20) * 0.01)

                    score += min((1 - safe_get(habit_features, "de_structure_density") * 0.001) * 0.2, 0.2)
                    score += min((1 - safe_get(vocab_features, "modifier_usage") * 0.01) * 0.2, 0.2)
                    score += min(safe_get(vocab_features, "action_verb_ratio") * 0.01, 0.1)

                scores[style_id] = min(max(score, 0.0), 1.0)  # ç¡®ä¿åˆ†æ•°åœ¨0-1ä¹‹é—´

            except Exception as e:
                print(f"è®¡ç®—é£æ ¼ {style_id} åˆ†æ•°æ—¶å‡ºé”™: {str(e)}")
                scores[style_id] = 0.0

        # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„æ–‡é£ç±»å‹
        if not scores:
            return "business_professional", 0.5

        best_style = max(scores.items(), key=lambda x: x[1])

        # å¦‚æœæœ€é«˜åˆ†å¤ªä½ï¼Œè¿”å›é»˜è®¤é£æ ¼
        if best_style[1] < 0.3:
            return "business_professional", best_style[1]

        return best_style[0], best_style[1]
    
    def _generate_style_prompt(self, features: Dict[str, Any], style_type: str) -> str:
        """ç”Ÿæˆæ–‡é£æç¤ºè¯"""
        try:
            style_info = self.style_types.get(style_type, {})
            style_name = style_info.get("name", "é€šç”¨é£æ ¼")
            characteristics = style_info.get("characteristics", [])

            # åŸºç¡€æ–‡é£æè¿°
            prompt_parts = [
                f"è¯·æŒ‰ç…§{style_name}è¿›è¡Œå†…å®¹ç”Ÿæˆï¼Œå…·ä½“è¦æ±‚å¦‚ä¸‹ï¼š",
                "",
                "ã€æ–‡é£ç‰¹å¾ã€‘"
            ]

            for char in characteristics:
                prompt_parts.append(f"- {char}")

            # æ ¹æ®å…·ä½“ç‰¹å¾æ•°æ®ç”Ÿæˆä¸ªæ€§åŒ–è¦æ±‚
            prompt_parts.extend(self._generate_sentence_requirements(features))
            prompt_parts.extend(self._generate_vocabulary_requirements(features))
            prompt_parts.extend(self._generate_expression_requirements(features))
            prompt_parts.extend(self._generate_organization_requirements(features))
            prompt_parts.extend(self._generate_tone_requirements(features))

            prompt_parts.append("")
            prompt_parts.append("ã€ç‰¹åˆ«æ³¨æ„ã€‘")
            prompt_parts.append("- é¿å…AIç”Ÿæˆç—•è¿¹ï¼Œè®©å†…å®¹è‡ªç„¶æµç•…")
            prompt_parts.append("- ä¿æŒä¸åŸæ–‡æ¡£é£æ ¼çš„ä¸€è‡´æ€§")
            prompt_parts.append("- ç¡®ä¿å†…å®¹å‡†ç¡®ã€é€»è¾‘æ¸…æ™°")

            # æ·»åŠ è´Ÿé¢ç¤ºä¾‹å’Œé¿å…äº‹é¡¹
            prompt_parts.extend(self._generate_avoidance_guidelines(features, style_type))

            return "\n".join(prompt_parts)

        except Exception as e:
            # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè¿”å›åŸºç¡€æç¤ºè¯
            return f"è¯·æŒ‰ç…§{style_info.get('name', 'ä¸“ä¸š')}é£æ ¼è¿›è¡Œå†…å®¹ç”Ÿæˆï¼Œä¿æŒè¯­è¨€è§„èŒƒã€é€»è¾‘æ¸…æ™°ã€è¡¨è¾¾å‡†ç¡®ã€‚"

    def _generate_sentence_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå¥å¼è¦æ±‚"""
        requirements = ["", "ã€å¥å¼è¦æ±‚ã€‘"]

        sentence_features = features.get("sentence_structure", {})
        avg_length = sentence_features.get("average_length", 15)
        complex_ratio = sentence_features.get("complex_ratio", 0.5)

        # å¥é•¿è¦æ±‚
        if avg_length > 30:
            requirements.append("- ä¿æŒé•¿å¥çš„ä½¿ç”¨ï¼Œæ³¨é‡è¡¨è¾¾çš„å®Œæ•´æ€§å’Œé€»è¾‘å±‚æ¬¡")
            requirements.append("- é€‚å½“ä½¿ç”¨åˆ†å·å’Œç ´æŠ˜å·æ¥ç»„ç»‡å¤æ‚å¥å¼")
        elif avg_length > 20:
            requirements.append("- é€‚å½“ä½¿ç”¨é•¿å¥ï¼Œä¿æŒè¡¨è¾¾çš„å®Œæ•´æ€§å’Œé€»è¾‘æ€§")
            requirements.append("- é•¿çŸ­å¥ç»“åˆï¼Œé¿å…å¥å¼è¿‡äºå•è°ƒ")
        elif avg_length < 12:
            requirements.append("- å¤šä½¿ç”¨çŸ­å¥ï¼Œä¿æŒè¡¨è¾¾çš„ç®€æ´æ˜äº†")
            requirements.append("- é¿å…å†—é•¿å¤æ‚çš„å¥å¼ç»“æ„")
        else:
            requirements.append("- é•¿çŸ­å¥ç»“åˆï¼Œä¿æŒèŠ‚å¥æ„Ÿå’Œå¯è¯»æ€§")

        # å¤æ‚åº¦è¦æ±‚
        if complex_ratio > 0.7:
            requirements.append("- ä¿æŒå¥å¼çš„å¤æ‚æ€§å’Œå±‚æ¬¡æ„Ÿ")
        elif complex_ratio < 0.3:
            requirements.append("- ä½¿ç”¨ç®€å•ç›´æ¥çš„å¥å¼ç»“æ„")

        return requirements

    def _generate_vocabulary_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¯æ±‡è¦æ±‚"""
        requirements = ["", "ã€è¯æ±‡è¦æ±‚ã€‘"]

        vocab_features = features.get("vocabulary_choice", {})
        formality = vocab_features.get("formality_score", 0)
        technical_density = vocab_features.get("technical_density", 0)
        modifier_usage = vocab_features.get("modifier_usage", 0)
        action_verb_ratio = vocab_features.get("action_verb_ratio", 0)

        # æ­£å¼åº¦è¦æ±‚
        if formality > 15:
            requirements.append("- ä½¿ç”¨æ­£å¼ã€è§„èŒƒçš„ä¹¦é¢è¯­è¯æ±‡")
            requirements.append("- é€‚å½“ä½¿ç”¨ä¸“ä¸šæœ¯è¯­å’Œå­¦æœ¯è¡¨è¾¾")
        elif formality > 8:
            requirements.append("- ä½¿ç”¨æ ‡å‡†çš„ä¹¦é¢è¯­è¡¨è¾¾")
            requirements.append("- é¿å…è¿‡äºå£è¯­åŒ–çš„è¯æ±‡")
        else:
            requirements.append("- ä½¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯æ±‡")
            requirements.append("- é¿å…è¿‡äºæ­£å¼æˆ–ç”Ÿåƒ»çš„è¡¨è¾¾")

        # æŠ€æœ¯æ€§è¦æ±‚
        if technical_density > 5:
            requirements.append("- ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®ä½¿ç”¨")

        # ä¿®é¥°è¯è¦æ±‚
        if modifier_usage > 10:
            requirements.append("- é€‚å½“ä½¿ç”¨å½¢å®¹è¯å’Œå‰¯è¯è¿›è¡Œä¿®é¥°")
        elif modifier_usage < 3:
            requirements.append("- å‡å°‘ä¸å¿…è¦çš„ä¿®é¥°è¯ï¼Œä¿æŒè¡¨è¾¾ç®€æ´")

        # åŠ¨è¯ä½¿ç”¨è¦æ±‚
        if action_verb_ratio > 20:
            requirements.append("- å¤šä½¿ç”¨åŠ¨ä½œæ€§å¼ºçš„åŠ¨è¯")
            requirements.append("- è®©è¡¨è¾¾æ›´åŠ ç”ŸåŠ¨æœ‰åŠ›")

        return requirements

    def _generate_expression_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¡¨è¾¾æ–¹å¼è¦æ±‚"""
        requirements = ["", "ã€è¡¨è¾¾æ–¹å¼ã€‘"]

        expression_features = features.get("expression_style", {})
        passive_ratio = expression_features.get("passive_active_ratio", 0)
        person_usage = expression_features.get("person_usage", {})

        # è¯­æ€è¦æ±‚
        if passive_ratio > 0.6:
            requirements.append("- é€‚å½“ä½¿ç”¨è¢«åŠ¨è¯­æ€ï¼Œä½“ç°å®¢è§‚æ€§å’Œæ­£å¼æ€§")
        elif passive_ratio > 0.3:
            requirements.append("- ä¸»è¢«åŠ¨è¯­æ€ç»“åˆä½¿ç”¨ï¼Œä¿æŒè¡¨è¾¾çš„çµæ´»æ€§")
        else:
            requirements.append("- ä¼˜å…ˆä½¿ç”¨ä¸»åŠ¨è¯­æ€ï¼Œè®©è¡¨è¾¾æ›´ç›´æ¥æœ‰åŠ›")

        # äººç§°ä½¿ç”¨è¦æ±‚
        first_person = person_usage.get("first_person", 0)
        if first_person > 5:
            requirements.append("- å¯ä»¥é€‚å½“ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼Œä½“ç°ä¸»è§‚æ€åº¦")
        elif first_person == 0:
            requirements.append("- é¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼Œä¿æŒå®¢è§‚ä¸­ç«‹")

        return requirements

    def _generate_organization_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆç»„ç»‡ç»“æ„è¦æ±‚"""
        requirements = ["", "ã€ç»„ç»‡ç»“æ„ã€‘"]

        org_features = features.get("text_organization", {})
        connector_density = org_features.get("connector_density", 0)
        summary_usage = org_features.get("summary_usage", 0)

        # è¿æ¥è¯è¦æ±‚
        if connector_density > 8:
            requirements.append("- ä½¿ç”¨ä¸°å¯Œçš„é€»è¾‘è¿æ¥è¯")
            requirements.append("- ä¿æŒæ®µè½é—´çš„é€»è¾‘å…³ç³»æ¸…æ™°")
        elif connector_density > 3:
            requirements.append("- é€‚å½“ä½¿ç”¨é€»è¾‘è¿æ¥è¯")
            requirements.append("- æ³¨æ„æ®µè½é—´çš„è¿‡æ¸¡è‡ªç„¶")
        else:
            requirements.append("- å‡å°‘æœºæ¢°åŒ–çš„è¿‡æ¸¡è¯ä½¿ç”¨")
            requirements.append("- é€šè¿‡å†…å®¹é€»è¾‘è‡ªç„¶è¿‡æ¸¡")

        # æ€»ç»“æ€§è¡¨è¾¾
        if summary_usage > 2:
            requirements.append("- é€‚å½“ä½¿ç”¨æ€»ç»“æ€§è¡¨è¾¾")
            requirements.append("- æ³¨é‡å†…å®¹çš„å½’çº³å’Œæç‚¼")

        return requirements

    def _generate_tone_requirements(self, features: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè¯­è°ƒè¦æ±‚"""
        requirements = ["", "ã€è¯­è°ƒé£æ ¼ã€‘"]

        emotional_features = features.get("emotional_tone", {})
        professional_features = features.get("professionalism", {})

        positive_ratio = emotional_features.get("positive_ratio", 0)
        negative_ratio = emotional_features.get("negative_ratio", 0)
        intensity = emotional_features.get("intensity", 0)
        authority_indicators = professional_features.get("authority_indicators", 0)

        # æƒ…æ„Ÿè‰²å½©è¦æ±‚
        if positive_ratio > 0.6:
            requirements.append("- ä¿æŒç§¯ææ­£é¢çš„è¯­è°ƒ")
        elif negative_ratio > 0.4:
            requirements.append("- å¯ä»¥é€‚å½“è¡¨è¾¾å…³åˆ‡å’Œé—®é¢˜æ„è¯†")
        else:
            requirements.append("- ä¿æŒä¸­æ€§å®¢è§‚çš„è¯­è°ƒ")

        # å¼ºåº¦è¦æ±‚
        if intensity > 15:
            requirements.append("- é€‚å½“ä½¿ç”¨å¼ºè°ƒè¯æ±‡ï¼Œä½“ç°é‡è¦æ€§")
        elif intensity < 5:
            requirements.append("- ä¿æŒå¹³å’Œçš„è¡¨è¾¾å¼ºåº¦")

        # æƒå¨æ€§è¦æ±‚
        if authority_indicators > 10:
            requirements.append("- ä½“ç°ä¸“ä¸šæƒå¨æ€§ï¼Œä½¿ç”¨å‡†ç¡®çš„æ•°æ®å’Œå¼•ç”¨")

        return requirements

    def _generate_avoidance_guidelines(self, features: Dict[str, Any], style_type: str) -> List[str]:
        """ç”Ÿæˆé¿å…äº‹é¡¹æŒ‡å¯¼"""
        guidelines = ["", "ã€é¿å…äº‹é¡¹ã€‘"]

        # æ ¹æ®é£æ ¼ç±»å‹æ·»åŠ ç‰¹å®šé¿å…äº‹é¡¹
        if style_type == "business_professional":
            guidelines.extend([
                "- é¿å…è¿‡äºæ„Ÿæ€§æˆ–ä¸»è§‚çš„è¡¨è¾¾",
                "- é¿å…ä½¿ç”¨ç½‘ç»œæµè¡Œè¯­æˆ–ä¿šè¯­",
                "- é¿å…å†—é•¿çš„ä¿®é¥°å’Œåä¸½çš„è¾è—»"
            ])
        elif style_type == "academic_research":
            guidelines.extend([
                "- é¿å…å£è¯­åŒ–è¡¨è¾¾å’Œéæ­£å¼ç”¨è¯",
                "- é¿å…ä¸»è§‚è‡†æ–­ï¼Œç¡®ä¿è®ºè¯ä¸¥è°¨",
                "- é¿å…è¿‡äºç»å¯¹çš„è¡¨è¿°"
            ])
        elif style_type == "concise_practical":
            guidelines.extend([
                "- é¿å…å†—ä½™å’Œé‡å¤è¡¨è¾¾",
                "- é¿å…è¿‡äºå¤æ‚çš„å¥å¼ç»“æ„",
                "- é¿å…ä¸å¿…è¦çš„ä¿®é¥°è¯æ±‡"
            ])

        # é€šç”¨é¿å…äº‹é¡¹
        guidelines.extend([
            "- é¿å…æ˜æ˜¾çš„AIç”Ÿæˆç—•è¿¹",
            "- é¿å…é€»è¾‘ä¸æ¸…æˆ–å‰åçŸ›ç›¾",
            "- é¿å…è¯­æ³•é”™è¯¯å’Œç”¨è¯ä¸å½“"
        ])

        return guidelines
    
    def _generate_template_id(self, document_name: str, features: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡é£æ¨¡æ¿ID"""
        content = f"{document_name}_{json.dumps(features, sort_keys=True)}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:12]
    
    def save_style_template(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿å­˜æ–‡é£æ¨¡æ¿"""
        try:
            template_id = analysis_result.get("template_id")
            if not template_id:
                return {"error": "ç¼ºå°‘æ¨¡æ¿ID"}
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            template_file = os.path.join(self.storage_path, f"{template_id}.json")
            with open(template_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°æ¨¡æ¿ç´¢å¼•
            self._update_style_template_index(template_id, analysis_result)
            
            return {
                "success": True,
                "template_id": template_id,
                "template_name": analysis_result.get("document_name", "æœªå‘½åæ¨¡æ¿"),
                "style_type": analysis_result.get("style_type", "æœªçŸ¥é£æ ¼"),
                "saved_path": template_file
            }
            
        except Exception as e:
            return {"error": f"ä¿å­˜æ–‡é£æ¨¡æ¿å¤±è´¥: {str(e)}"}
    
    def load_style_template(self, template_id: str) -> Dict[str, Any]:
        """åŠ è½½æ–‡é£æ¨¡æ¿"""
        try:
            template_file = os.path.join(self.storage_path, f"{template_id}.json")
            if not os.path.exists(template_file):
                return {"error": f"æ–‡é£æ¨¡æ¿ä¸å­˜åœ¨: {template_id}"}
            
            with open(template_file, 'r', encoding='utf-8') as f:
                template_data = json.load(f)
            
            return template_data
            
        except Exception as e:
            return {"error": f"åŠ è½½æ–‡é£æ¨¡æ¿å¤±è´¥: {str(e)}"}
    
    def list_style_templates(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ–‡é£æ¨¡æ¿"""
        try:
            index_file = os.path.join(self.storage_path, "style_template_index.json")
            if not os.path.exists(index_file):
                return []
            
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            return index_data.get("templates", [])
            
        except Exception as e:
            print(f"åŠ è½½æ–‡é£æ¨¡æ¿ç´¢å¼•å¤±è´¥: {str(e)}")
            return []
    
    def _update_style_template_index(self, template_id: str, template_data: Dict[str, Any]):
        """æ›´æ–°æ–‡é£æ¨¡æ¿ç´¢å¼•"""
        index_file = os.path.join(self.storage_path, "style_template_index.json")
        
        # è¯»å–ç°æœ‰ç´¢å¼•
        if os.path.exists(index_file):
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
        else:
            index_data = {"templates": []}
        
        # æ›´æ–°æˆ–æ·»åŠ æ¨¡æ¿ä¿¡æ¯
        template_info = {
            "template_id": template_id,
            "name": template_data.get("document_name", "æœªå‘½åæ¨¡æ¿"),
            "style_type": template_data.get("style_type", "æœªçŸ¥é£æ ¼"),
            "style_name": self.style_types.get(template_data.get("style_type", ""), {}).get("name", "æœªçŸ¥é£æ ¼"),
            "confidence_score": template_data.get("confidence_score", 0.0),
            "created_time": template_data.get("analysis_time", datetime.now().isoformat()),
            "description": f"æ–‡é£æ¨¡æ¿ï¼š{template_data.get('document_name', 'æœªå‘½åæ–‡æ¡£')}"
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_index = -1
        for i, template in enumerate(index_data["templates"]):
            if template["template_id"] == template_id:
                existing_index = i
                break
        
        if existing_index >= 0:
            index_data["templates"][existing_index] = template_info
        else:
            index_data["templates"].append(template_info)
        
        # ä¿å­˜ç´¢å¼•
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)

    def validate_style_application(self, original_content: str, generated_content: str,
                                 target_style: str, style_features: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é£æ ¼åº”ç”¨æ•ˆæœ"""
        try:
            # åˆ†æç”Ÿæˆå†…å®¹çš„é£æ ¼ç‰¹å¾
            generated_features = self._analyze_style_features(generated_content)

            # è¯†åˆ«ç”Ÿæˆå†…å®¹çš„é£æ ¼ç±»å‹
            identified_style, confidence = self._identify_style_type(generated_content, generated_features)

            # è®¡ç®—é£æ ¼ä¸€è‡´æ€§åˆ†æ•°
            consistency_score = self._calculate_style_consistency(
                style_features, generated_features, target_style
            )

            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            quality_metrics = self._calculate_quality_metrics(
                original_content, generated_content, generated_features
            )

            # ç”Ÿæˆæ”¹è¿›å»ºè®®
            improvement_suggestions = self._generate_improvement_suggestions(
                target_style, generated_features, consistency_score
            )

            return {
                "success": True,
                "target_style": target_style,
                "identified_style": identified_style,
                "style_confidence": confidence,
                "consistency_score": consistency_score,
                "quality_metrics": quality_metrics,
                "generated_features": generated_features,
                "improvement_suggestions": improvement_suggestions,
                "validation_time": datetime.now().isoformat()
            }

        except Exception as e:
            return {"error": f"é£æ ¼éªŒè¯å¤±è´¥: {str(e)}"}

    def _calculate_style_consistency(self, target_features: Dict[str, Any],
                                   generated_features: Dict[str, Any],
                                   target_style: str) -> float:
        """è®¡ç®—é£æ ¼ä¸€è‡´æ€§åˆ†æ•°"""
        try:
            total_score = 0.0
            total_weight = 0.0

            # å¥å¼ç»“æ„ä¸€è‡´æ€§ (æƒé‡: 0.25)
            sentence_score = self._compare_sentence_features(
                target_features.get("sentence_structure", {}),
                generated_features.get("sentence_structure", {})
            )
            total_score += sentence_score * 0.25
            total_weight += 0.25

            # è¯æ±‡é€‰æ‹©ä¸€è‡´æ€§ (æƒé‡: 0.3)
            vocab_score = self._compare_vocabulary_features(
                target_features.get("vocabulary_choice", {}),
                generated_features.get("vocabulary_choice", {})
            )
            total_score += vocab_score * 0.3
            total_weight += 0.3

            # è¡¨è¾¾æ–¹å¼ä¸€è‡´æ€§ (æƒé‡: 0.2)
            expression_score = self._compare_expression_features(
                target_features.get("expression_style", {}),
                generated_features.get("expression_style", {})
            )
            total_score += expression_score * 0.2
            total_weight += 0.2

            # ç»„ç»‡ç»“æ„ä¸€è‡´æ€§ (æƒé‡: 0.15)
            org_score = self._compare_organization_features(
                target_features.get("text_organization", {}),
                generated_features.get("text_organization", {})
            )
            total_score += org_score * 0.15
            total_weight += 0.15

            # è¯­è¨€ä¹ æƒ¯ä¸€è‡´æ€§ (æƒé‡: 0.1)
            habit_score = self._compare_habit_features(
                target_features.get("language_habits", {}),
                generated_features.get("language_habits", {})
            )
            total_score += habit_score * 0.1
            total_weight += 0.1

            return total_score / total_weight if total_weight > 0 else 0.0

        except Exception as e:
            print(f"è®¡ç®—é£æ ¼ä¸€è‡´æ€§æ—¶å‡ºé”™: {str(e)}")
            return 0.0

    def _compare_sentence_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒå¥å¼ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # å¹³å‡å¥é•¿æ¯”è¾ƒ
        target_length = target.get("average_length", 15)
        generated_length = generated.get("average_length", 15)
        length_diff = abs(target_length - generated_length) / max(target_length, 1)
        score += max(0, 1 - length_diff)
        comparisons += 1

        # å¤æ‚å¥æ¯”ä¾‹æ¯”è¾ƒ
        target_complex = target.get("complex_ratio", 0.5)
        generated_complex = generated.get("complex_ratio", 0.5)
        complex_diff = abs(target_complex - generated_complex)
        score += max(0, 1 - complex_diff * 2)
        comparisons += 1

        return score / comparisons if comparisons > 0 else 0.0

    def _compare_vocabulary_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒè¯æ±‡ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # æ­£å¼åº¦æ¯”è¾ƒ
        target_formality = target.get("formality_score", 0)
        generated_formality = generated.get("formality_score", 0)
        if target_formality > 0:
            formality_ratio = min(generated_formality / target_formality, target_formality / generated_formality)
            score += formality_ratio
            comparisons += 1

        # åŠ¨ä½œåŠ¨è¯æ¯”ä¾‹æ¯”è¾ƒ
        target_action = target.get("action_verb_ratio", 0)
        generated_action = generated.get("action_verb_ratio", 0)
        if target_action > 0:
            action_ratio = min(generated_action / target_action, target_action / generated_action)
            score += action_ratio
            comparisons += 1

        return score / comparisons if comparisons > 0 else 0.5

    def _compare_expression_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒè¡¨è¾¾æ–¹å¼ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # è¢«åŠ¨è¯­æ€æ¯”ä¾‹æ¯”è¾ƒ
        target_passive = target.get("passive_active_ratio", 0)
        generated_passive = generated.get("passive_active_ratio", 0)
        passive_diff = abs(target_passive - generated_passive)
        score += max(0, 1 - passive_diff * 2)
        comparisons += 1

        return score / comparisons if comparisons > 0 else 0.0

    def _compare_organization_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒç»„ç»‡ç»“æ„ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # è¿æ¥è¯å¯†åº¦æ¯”è¾ƒ
        target_connector = target.get("connector_density", 0)
        generated_connector = generated.get("connector_density", 0)
        if target_connector > 0:
            connector_ratio = min(generated_connector / target_connector, target_connector / generated_connector)
            score += connector_ratio
            comparisons += 1

        return score / comparisons if comparisons > 0 else 0.5

    def _compare_habit_features(self, target: Dict, generated: Dict) -> float:
        """æ¯”è¾ƒè¯­è¨€ä¹ æƒ¯ç‰¹å¾"""
        score = 0.0
        comparisons = 0

        # å£è¯­åŒ–ç¨‹åº¦æ¯”è¾ƒ
        target_colloquial = target.get("colloquial_level", 0)
        generated_colloquial = generated.get("colloquial_level", 0)
        colloquial_diff = abs(target_colloquial - generated_colloquial)
        score += max(0, 1 - colloquial_diff * 0.1)
        comparisons += 1

        return score / comparisons if comparisons > 0 else 0.0

    def _calculate_quality_metrics(self, original_content: str, generated_content: str,
                                 generated_features: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        try:
            metrics = {}

            # é•¿åº¦æ¯”è¾ƒ
            original_length = len(original_content)
            generated_length = len(generated_content)
            metrics["length_ratio"] = generated_length / original_length if original_length > 0 else 0

            # å¥å­æ•°é‡æ¯”è¾ƒ
            original_sentences = generated_features.get("sentence_structure", {}).get("total_sentences", 0)
            metrics["sentence_count"] = original_sentences

            # å¯è¯»æ€§è¯„ä¼°ï¼ˆåŸºäºå¹³å‡å¥é•¿ï¼‰
            avg_length = generated_features.get("sentence_structure", {}).get("average_length", 15)
            if 15 <= avg_length <= 25:
                metrics["readability_score"] = 1.0
            elif 10 <= avg_length < 15 or 25 < avg_length <= 35:
                metrics["readability_score"] = 0.8
            else:
                metrics["readability_score"] = 0.6

            # è¯æ±‡ä¸°å¯Œåº¦ï¼ˆåŸºäºä¿®é¥°è¯ä½¿ç”¨ï¼‰
            modifier_usage = generated_features.get("vocabulary_choice", {}).get("modifier_usage", 0)
            metrics["vocabulary_richness"] = min(modifier_usage / 10, 1.0)

            # ä¸“ä¸šæ€§è¯„ä¼°
            professionalism = generated_features.get("professionalism", {})
            authority_score = professionalism.get("authority_indicators", 0)
            precision_score = professionalism.get("precision_level", 0)
            metrics["professionalism_score"] = min((authority_score + precision_score) / 20, 1.0)

            return metrics

        except Exception as e:
            print(f"è®¡ç®—è´¨é‡æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
            return {"error": str(e)}

    def _generate_improvement_suggestions(self, target_style: str, generated_features: Dict[str, Any],
                                        consistency_score: float) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        try:
            # åŸºäºä¸€è‡´æ€§åˆ†æ•°ç»™å‡ºæ€»ä½“å»ºè®®
            if consistency_score < 0.6:
                suggestions.append("æ•´ä½“é£æ ¼ä¸€è‡´æ€§è¾ƒä½ï¼Œå»ºè®®é‡æ–°è°ƒæ•´ç”Ÿæˆç­–ç•¥")
            elif consistency_score < 0.8:
                suggestions.append("é£æ ¼åŸºæœ¬ç¬¦åˆè¦æ±‚ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
            else:
                suggestions.append("é£æ ¼ä¸€è‡´æ€§è‰¯å¥½ï¼Œç¬¦åˆç›®æ ‡è¦æ±‚")

            # åŸºäºå…·ä½“ç‰¹å¾ç»™å‡ºå»ºè®®
            sentence_features = generated_features.get("sentence_structure", {})
            vocab_features = generated_features.get("vocabulary_choice", {})
            expression_features = generated_features.get("expression_style", {})

            # å¥å¼å»ºè®®
            avg_length = sentence_features.get("average_length", 15)
            if target_style == "concise_practical" and avg_length > 20:
                suggestions.append("å¥å­åé•¿ï¼Œå»ºè®®ä½¿ç”¨æ›´ç®€æ´çš„è¡¨è¾¾")
            elif target_style == "academic_research" and avg_length < 20:
                suggestions.append("å¥å­åçŸ­ï¼Œå»ºè®®å¢åŠ å¥å¼çš„å¤æ‚æ€§å’Œå®Œæ•´æ€§")

            # è¯æ±‡å»ºè®®
            formality = vocab_features.get("formality_score", 0)
            if target_style == "formal_official" and formality < 10:
                suggestions.append("æ­£å¼åº¦ä¸å¤Ÿï¼Œå»ºè®®ä½¿ç”¨æ›´è§„èŒƒçš„ä¹¦é¢è¯­")
            elif target_style == "narrative_descriptive" and formality > 15:
                suggestions.append("è¿‡äºæ­£å¼ï¼Œå»ºè®®ä½¿ç”¨æ›´ç”ŸåŠ¨è‡ªç„¶çš„è¡¨è¾¾")

            # è¯­æ€å»ºè®®
            passive_ratio = expression_features.get("passive_active_ratio", 0)
            if target_style == "business_professional" and passive_ratio > 0.4:
                suggestions.append("è¢«åŠ¨è¯­æ€ä½¿ç”¨è¿‡å¤šï¼Œå»ºè®®å¤šç”¨ä¸»åŠ¨è¯­æ€")

            # å¦‚æœæ²¡æœ‰å…·ä½“å»ºè®®ï¼Œç»™å‡ºé€šç”¨å»ºè®®
            if len(suggestions) <= 1:
                suggestions.append("ç»§ç»­ä¿æŒå½“å‰çš„å†™ä½œé£æ ¼")
                suggestions.append("æ³¨æ„è¯­è¨€çš„å‡†ç¡®æ€§å’Œé€»è¾‘æ€§")

        except Exception as e:
            suggestions = [f"ç”Ÿæˆæ”¹è¿›å»ºè®®æ—¶å‡ºé”™: {str(e)}"]

        return suggestions
