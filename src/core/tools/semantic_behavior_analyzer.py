#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Semantic Behavior Analyzer - æ ¸å¿ƒæ¨¡å—

Author: AI Assistant (Claude)
Created: 2025-01-28
Last Modified: 2025-01-28
Modified By: AI Assistant (Claude)
AI Assisted: æ˜¯ - Claude 3.5 Sonnet
Version: v1.0
License: MIT
"""


import numpy as np
import json
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime

try:
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
    from sklearn.metrics import silhouette_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Warning: scikit-learn not available for advanced analysis")


class SemanticBehaviorAnalyzer:
    """è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå™¨ - è®¯é£å¤§æ¨¡å‹ä½œä¸ºé£æ ¼è¯„ä¼°å‘˜"""
    
    def __init__(self, llm_client=None):
        """
        åˆå§‹åŒ–è¯­ä¹‰è¡Œä¸ºåˆ†æå™¨
        
        Args:
            llm_client: è®¯é£å¤§æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆä½œä¸ºé£æ ¼è¯„ä¼°å‘˜ï¼‰
        """
        self.llm_client = llm_client
        self.evaluation_templates = self._init_evaluation_templates()
    
    def _init_evaluation_templates(self) -> Dict[str, str]:
        """åˆå§‹åŒ–è¯„ä¼°æç¤ºè¯æ¨¡æ¿"""
        return {
            "cluster_interpretation": """è¯·åˆ†æä»¥ä¸‹æ¦‚å¿µèšç±»ï¼Œæè¿°æ¯ä¸ªèšç±»ä»£è¡¨çš„ä¸»é¢˜å’Œæ¦‚å¿µé—´çš„å…³è”ï¼š

èšç±»ç»“æœï¼š
{cluster_info}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
  "cluster_themes": [
    {{"cluster_id": "cluster_0", "theme": "ä¸»é¢˜æè¿°", "coherence": 1-5, "explanation": "èšç±»å†…æ¦‚å¿µå…³è”è§£é‡Š"}},
    ...
  ],
  "cluster_relationships": [
    {{"cluster1": "cluster_0", "cluster2": "cluster_1", "relationship": "äº’è¡¥/å¯¹ç«‹/ç‹¬ç«‹/åŒ…å«", "strength": 1-5}},
    ...
  ],
  "overall_assessment": {{"semantic_organization": 1-5, "concept_diversity": 1-5, "thematic_clarity": 1-5}}
}}""",

            "novelty_assessment": """è¯·è¯„ä¼°ä»¥ä¸‹æ¦‚å¿µå¯¹çš„å…³è”åˆ›æ–°åº¦ï¼š

æ¦‚å¿µå¯¹åˆ†æï¼š
{concept_pairs}

åŸæ–‡è¯­å¢ƒï¼š
{original_text}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
{{
  "novelty_assessments": [
    {{
      "concept1": "æ¦‚å¿µ1",
      "concept2": "æ¦‚å¿µ2", 
      "novelty_score": 1-5,
      "novelty_type": "å¯Œæœ‰åˆ›æ„çš„è”æƒ³/æ°å½“çš„ç±»æ¯”/ç‰µå¼ºçš„æ¯”é™„/æ— æ„ä¹‰çš„å¹¶åˆ—",
      "explanation": "è¯„ä¼°ç†ç”±",
      "context_relevance": 1-5
    }},
    ...
  ],
  "overall_creativity": {{"average_novelty": 0.0, "creative_density": 1-5, "innovation_style": "æè¿°"}}
}}""",

            "semantic_distance_evaluation": """è¯·è¯„ä¼°ä»¥ä¸‹æ–‡æœ¬çš„è¯­ä¹‰è·ç¦»ç‰¹å¾ï¼š

è¯­ä¹‰è·ç¦»ç»Ÿè®¡ï¼š
{distance_stats}

æ¦‚å¿µåˆ†å¸ƒï¼š
{concept_distribution}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ï¼š
{{
  "distance_characteristics": {{
    "semantic_span": "ç´§å¯†/é€‚ä¸­/åˆ†æ•£",
    "concept_coherence": 1-5,
    "thematic_focus": 1-5,
    "explanation": "è¯­ä¹‰è·ç¦»ç‰¹å¾æè¿°"
  }},
  "writing_style_implications": {{
    "style_type": "ä¸“ä¸šèšç„¦/å¹¿æ³›æ¶‰çŒ/è·³è·ƒæ€ç»´/é€»è¾‘ä¸¥å¯†",
    "cognitive_pattern": "æè¿°è®¤çŸ¥æ¨¡å¼",
    "audience_accessibility": 1-5
  }}
}}""",

            "emotional_semantic_analysis": """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿè¯­ä¹‰ç‰¹å¾ï¼š

æƒ…æ„Ÿè¯æ±‡åˆ†å¸ƒï¼š
{emotional_distribution}

æ¦‚å¿µæƒ…æ„Ÿå€¾å‘ï¼š
{concept_emotions}

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ†æï¼š
{{
  "emotional_patterns": {{
    "dominant_emotion": "ç§¯æ/æ¶ˆæ/ä¸­æ€§/å¤æ‚",
    "emotional_intensity": 1-5,
    "emotional_consistency": 1-5,
    "emotional_sophistication": 1-5
  }},
  "concept_emotional_mapping": [
    {{"concept": "æ¦‚å¿µ", "emotional_association": "æƒ…æ„Ÿå€¾å‘", "strength": 1-5}},
    ...
  ],
  "style_characteristics": {{
    "emotional_expressiveness": 1-5,
    "subjective_tendency": 1-5,
    "persuasive_power": 1-5
  }}
}}"""
        }
    
    def analyze_concept_clustering(self, vector_result: Dict[str, Any], 
                                 cluster_result: Dict[str, Any],
                                 original_text: str = "") -> Dict[str, Any]:
        """
        åˆ†ææ¦‚å¿µèšç±»è¡Œä¸º
        
        Args:
            vector_result: å‘é‡åŒ–ç»“æœ
            cluster_result: èšç±»ç»“æœ
            original_text: åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºä¸Šä¸‹æ–‡åˆ†æï¼‰
        """
        analysis_result = {
            "analysis_time": datetime.now().isoformat(),
            "clustering_metrics": {},
            "llm_interpretation": {},
            "behavioral_indicators": {},
            "success": False
        }
        
        try:
            print("ğŸ” æ­£åœ¨åˆ†ææ¦‚å¿µèšç±»è¡Œä¸º...")
            
            # 1. è®¡ç®—èšç±»é‡åŒ–æŒ‡æ ‡
            clustering_metrics = self._calculate_clustering_metrics(cluster_result, vector_result)
            analysis_result["clustering_metrics"] = clustering_metrics
            
            # 2. LLMèšç±»è§£é‡Šï¼ˆè®¯é£å¤§æ¨¡å‹ä½œä¸ºè¯„ä¼°å‘˜ï¼‰
            if self.llm_client:
                llm_interpretation = self._get_llm_cluster_interpretation(
                    cluster_result, original_text
                )
                analysis_result["llm_interpretation"] = llm_interpretation
            
            # 3. æå–è¡Œä¸ºæŒ‡æ ‡
            behavioral_indicators = self._extract_clustering_behavioral_indicators(
                clustering_metrics, analysis_result.get("llm_interpretation", {})
            )
            analysis_result["behavioral_indicators"] = behavioral_indicators
            
            analysis_result["success"] = True
            print("âœ… æ¦‚å¿µèšç±»è¡Œä¸ºåˆ†æå®Œæˆ")
            
        except Exception as e:
            analysis_result["error"] = str(e)
            print(f"âŒ æ¦‚å¿µèšç±»è¡Œä¸ºåˆ†æå¤±è´¥: {str(e)}")
        
        return analysis_result
    
    def analyze_semantic_distance_patterns(self, vector_result: Dict[str, Any],
                                         similarity_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯­ä¹‰è·ç¦»æ¨¡å¼"""
        analysis_result = {
            "analysis_time": datetime.now().isoformat(),
            "distance_metrics": {},
            "pattern_analysis": {},
            "llm_evaluation": {},
            "success": False
        }
        
        try:
            print("ğŸ” æ­£åœ¨åˆ†æè¯­ä¹‰è·ç¦»æ¨¡å¼...")
            
            # 1. è®¡ç®—è·ç¦»æŒ‡æ ‡
            distance_metrics = self._calculate_distance_metrics(similarity_result)
            analysis_result["distance_metrics"] = distance_metrics
            
            # 2. æ¨¡å¼åˆ†æ
            pattern_analysis = self._analyze_distance_patterns(distance_metrics, vector_result)
            analysis_result["pattern_analysis"] = pattern_analysis
            
            # 3. LLMè¯„ä¼°
            if self.llm_client:
                llm_evaluation = self._get_llm_distance_evaluation(
                    distance_metrics, pattern_analysis
                )
                analysis_result["llm_evaluation"] = llm_evaluation
            
            analysis_result["success"] = True
            print("âœ… è¯­ä¹‰è·ç¦»æ¨¡å¼åˆ†æå®Œæˆ")
            
        except Exception as e:
            analysis_result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰è·ç¦»æ¨¡å¼åˆ†æå¤±è´¥: {str(e)}")
        
        return analysis_result
    
    def assess_associative_novelty(self, vector_result: Dict[str, Any],
                                 similarity_result: Dict[str, Any],
                                 original_text: str = "") -> Dict[str, Any]:
        """è¯„ä¼°è”æƒ³åˆ›æ–°åº¦"""
        assessment_result = {
            "assessment_time": datetime.now().isoformat(),
            "novelty_candidates": [],
            "llm_assessments": {},
            "creativity_metrics": {},
            "success": False
        }
        
        try:
            print("ğŸ” æ­£åœ¨è¯„ä¼°è”æƒ³åˆ›æ–°åº¦...")
            
            # 1. è¯†åˆ«å€™é€‰åˆ›æ–°è”æƒ³å¯¹
            novelty_candidates = self._identify_novelty_candidates(
                vector_result, similarity_result
            )
            assessment_result["novelty_candidates"] = novelty_candidates
            
            # 2. LLMåˆ›æ–°åº¦è¯„ä¼°
            if self.llm_client and novelty_candidates:
                llm_assessments = self._get_llm_novelty_assessment(
                    novelty_candidates, original_text
                )
                assessment_result["llm_assessments"] = llm_assessments
            
            # 3. è®¡ç®—åˆ›æ–°åº¦æŒ‡æ ‡
            creativity_metrics = self._calculate_creativity_metrics(
                novelty_candidates, assessment_result.get("llm_assessments", {})
            )
            assessment_result["creativity_metrics"] = creativity_metrics
            
            assessment_result["success"] = True
            print("âœ… è”æƒ³åˆ›æ–°åº¦è¯„ä¼°å®Œæˆ")
            
        except Exception as e:
            assessment_result["error"] = str(e)
            print(f"âŒ è”æƒ³åˆ›æ–°åº¦è¯„ä¼°å¤±è´¥: {str(e)}")
        
        return assessment_result
    
    def analyze_emotional_semantic_behavior(self, semantic_units: Dict[str, Any],
                                          vector_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿè¯­ä¹‰è¡Œä¸º"""
        analysis_result = {
            "analysis_time": datetime.now().isoformat(),
            "emotional_distribution": {},
            "concept_emotions": {},
            "llm_analysis": {},
            "behavioral_patterns": {},
            "success": False
        }
        
        try:
            print("ğŸ” æ­£åœ¨åˆ†ææƒ…æ„Ÿè¯­ä¹‰è¡Œä¸º...")
            
            # 1. æå–æƒ…æ„Ÿåˆ†å¸ƒ
            emotional_distribution = self._extract_emotional_distribution(semantic_units)
            analysis_result["emotional_distribution"] = emotional_distribution
            
            # 2. åˆ†ææ¦‚å¿µæƒ…æ„Ÿå€¾å‘
            concept_emotions = self._analyze_concept_emotions(semantic_units, vector_result)
            analysis_result["concept_emotions"] = concept_emotions
            
            # 3. LLMæƒ…æ„Ÿè¯­ä¹‰åˆ†æ
            if self.llm_client:
                llm_analysis = self._get_llm_emotional_analysis(
                    emotional_distribution, concept_emotions
                )
                analysis_result["llm_analysis"] = llm_analysis
            
            # 4. æå–è¡Œä¸ºæ¨¡å¼
            behavioral_patterns = self._extract_emotional_behavioral_patterns(
                emotional_distribution, concept_emotions, analysis_result.get("llm_analysis", {})
            )
            analysis_result["behavioral_patterns"] = behavioral_patterns
            
            analysis_result["success"] = True
            print("âœ… æƒ…æ„Ÿè¯­ä¹‰è¡Œä¸ºåˆ†æå®Œæˆ")
            
        except Exception as e:
            analysis_result["error"] = str(e)
            print(f"âŒ æƒ…æ„Ÿè¯­ä¹‰è¡Œä¸ºåˆ†æå¤±è´¥: {str(e)}")
        
        return analysis_result
    
    def _calculate_clustering_metrics(self, cluster_result: Dict[str, Any],
                                    vector_result: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—èšç±»é‡åŒ–æŒ‡æ ‡"""
        metrics = {
            "cluster_count": 0,
            "average_cluster_size": 0,
            "cluster_size_variance": 0,
            "intra_cluster_distances": {},
            "inter_cluster_distances": {},
            "silhouette_score": None
        }
        
        try:
            clusters = cluster_result.get("clusters", {})
            metrics["cluster_count"] = len(clusters)
            
            if clusters:
                # èšç±»å¤§å°ç»Ÿè®¡
                cluster_sizes = [cluster["size"] for cluster in clusters.values()]
                metrics["average_cluster_size"] = np.mean(cluster_sizes)
                metrics["cluster_size_variance"] = np.var(cluster_sizes)
                
                # è®¡ç®—ç°‡å†…å’Œç°‡é—´è·ç¦»
                for cluster_id, cluster_data in clusters.items():
                    concepts = cluster_data["concepts"]
                    if len(concepts) > 1:
                        # ç°‡å†…å¹³å‡è·ç¦»
                        distances = []
                        for i, concept1 in enumerate(concepts):
                            for concept2 in concepts[i+1:]:
                                dist = concept1.get("distance_to_center", 0) + concept2.get("distance_to_center", 0)
                                distances.append(dist)
                        
                        if distances:
                            metrics["intra_cluster_distances"][cluster_id] = {
                                "average": np.mean(distances),
                                "max": np.max(distances),
                                "min": np.min(distances)
                            }
        
        except Exception as e:
            metrics["error"] = str(e)
        
        return metrics
    
    def _get_llm_cluster_interpretation(self, cluster_result: Dict[str, Any],
                                      original_text: str) -> Dict[str, Any]:
        """è·å–LLMèšç±»è§£é‡Š"""
        try:
            # å‡†å¤‡èšç±»ä¿¡æ¯
            clusters = cluster_result.get("clusters", {})
            cluster_info = []
            
            for cluster_id, cluster_data in clusters.items():
                concepts = [concept["name"] for concept in cluster_data["concepts"]]
                cluster_info.append(f"{cluster_id}: {', '.join(concepts)}")
            
            cluster_info_str = "\n".join(cluster_info)
            
            # æ„å»ºæç¤ºè¯
            prompt = self.evaluation_templates["cluster_interpretation"].format(
                cluster_info=cluster_info_str
            )
            
            # è°ƒç”¨LLM
            response = self.llm_client.generate(prompt)
            
            # è§£æå“åº”
            return self._parse_json_response(response, "cluster_interpretation")
            
        except Exception as e:
            return {"error": str(e)}
    
    def _calculate_distance_metrics(self, similarity_result: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—è·ç¦»æŒ‡æ ‡"""
        metrics = {
            "average_similarity": 0,
            "similarity_variance": 0,
            "max_similarity": 0,
            "min_similarity": 1,
            "similarity_distribution": {}
        }
        
        try:
            # ä»ç›¸ä¼¼åº¦ç»Ÿè®¡ä¸­æå–æŒ‡æ ‡
            similarity_stats = similarity_result.get("similarity_statistics", {})
            concept_stats = similarity_stats.get("concept_similarity_stats", {})
            
            if concept_stats:
                metrics["average_similarity"] = concept_stats.get("average", 0)
                metrics["max_similarity"] = concept_stats.get("max", 0)
                metrics["min_similarity"] = concept_stats.get("min", 1)
                metrics["similarity_variance"] = concept_stats.get("std", 0) ** 2
            
            # åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
            concept_similarities = similarity_result.get("concept_similarities", {})
            all_similarities = []
            
            for name1, sims in concept_similarities.items():
                all_similarities.extend(sims.values())
            
            if all_similarities:
                # åˆ†å¸ƒç»Ÿè®¡
                metrics["similarity_distribution"] = {
                    "high_similarity_count": sum(1 for s in all_similarities if s > 0.7),
                    "medium_similarity_count": sum(1 for s in all_similarities if 0.3 <= s <= 0.7),
                    "low_similarity_count": sum(1 for s in all_similarities if s < 0.3),
                    "total_pairs": len(all_similarities)
                }
        
        except Exception as e:
            metrics["error"] = str(e)
        
        return metrics
    
    def _identify_novelty_candidates(self, vector_result: Dict[str, Any],
                                   similarity_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«å€™é€‰åˆ›æ–°è”æƒ³å¯¹"""
        candidates = []
        
        try:
            # ä»æ¦‚å¿µç›¸ä¼¼åº¦ä¸­æ‰¾å‡ºè·ç¦»è¾ƒå¤§ä½†å¯èƒ½æœ‰åˆ›æ–°è”æƒ³çš„æ¦‚å¿µå¯¹
            concept_similarities = similarity_result.get("concept_similarities", {})
            
            for concept1, similarities in concept_similarities.items():
                for concept2, similarity in similarities.items():
                    # é€‰æ‹©ç›¸ä¼¼åº¦è¾ƒä½ä½†ä¸æ˜¯æœ€ä½çš„æ¦‚å¿µå¯¹ä½œä¸ºå€™é€‰
                    if 0.1 <= similarity <= 0.4:  # ä¸­ç­‰è·ç¦»ï¼Œå¯èƒ½æœ‰åˆ›æ–°è”æƒ³
                        candidates.append({
                            "concept1": concept1,
                            "concept2": concept2,
                            "similarity": similarity,
                            "distance": 1 - similarity,
                            "candidate_type": "medium_distance"
                        })
            
            # æŒ‰è·ç¦»æ’åºï¼Œé€‰æ‹©æœ€æœ‰æ½œåŠ›çš„å€™é€‰
            candidates.sort(key=lambda x: x["distance"], reverse=True)
            return candidates[:10]  # æœ€å¤šè¿”å›10ä¸ªå€™é€‰
        
        except Exception as e:
            return [{"error": str(e)}]
    
    def _get_llm_novelty_assessment(self, novelty_candidates: List[Dict[str, Any]],
                                   original_text: str) -> Dict[str, Any]:
        """è·å–LLMåˆ›æ–°åº¦è¯„ä¼°"""
        try:
            # å‡†å¤‡æ¦‚å¿µå¯¹ä¿¡æ¯
            concept_pairs = []
            for candidate in novelty_candidates[:5]:  # é™åˆ¶æ•°é‡
                concept_pairs.append({
                    "concept1": candidate["concept1"],
                    "concept2": candidate["concept2"],
                    "similarity": candidate["similarity"]
                })
            
            # æ„å»ºæç¤ºè¯
            prompt = self.evaluation_templates["novelty_assessment"].format(
                concept_pairs=json.dumps(concept_pairs, ensure_ascii=False, indent=2),
                original_text=original_text[:1000]  # é™åˆ¶æ–‡æœ¬é•¿åº¦
            )
            
            # è°ƒç”¨LLM
            response = self.llm_client.generate(prompt)
            
            # è§£æå“åº”
            return self._parse_json_response(response, "novelty_assessment")
            
        except Exception as e:
            return {"error": str(e)}
    
    def _extract_emotional_distribution(self, semantic_units: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æƒ…æ„Ÿåˆ†å¸ƒ"""
        distribution = {
            "positive_count": 0,
            "negative_count": 0,
            "neutral_count": 0,
            "emotional_intensity_avg": 0,
            "emotional_words": []
        }
        
        try:
            # åˆ†æå½¢å®¹è¯æƒ…æ„Ÿ
            adjectives = semantic_units.get("key_adjectives", [])
            intensities = []
            
            for adj in adjectives:
                sentiment = adj.get("sentiment_polarity", "ä¸­æ€§")
                intensity = adj.get("sentiment_intensity", 3)
                
                if sentiment == "ç§¯æ":
                    distribution["positive_count"] += 1
                elif sentiment == "æ¶ˆæ":
                    distribution["negative_count"] += 1
                else:
                    distribution["neutral_count"] += 1
                
                intensities.append(intensity)
                distribution["emotional_words"].append({
                    "word": adj.get("text", ""),
                    "sentiment": sentiment,
                    "intensity": intensity
                })
            
            if intensities:
                distribution["emotional_intensity_avg"] = np.mean(intensities)
        
        except Exception as e:
            distribution["error"] = str(e)
        
        return distribution
    
    def _parse_json_response(self, response: str, response_type: str) -> Dict[str, Any]:
        """è§£æJSONå“åº”"""
        try:
            # å°è¯•æå–JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                return {"raw_response": response, "parsing_failed": True}
        except:
            return {"raw_response": response, "parsing_failed": True}
    
    def _analyze_distance_patterns(self, distance_metrics: Dict[str, Any],
                                 vector_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè·ç¦»æ¨¡å¼"""
        patterns = {
            "semantic_span": "unknown",
            "concept_distribution": "unknown",
            "clustering_tendency": "unknown"
        }
        
        try:
            avg_sim = distance_metrics.get("average_similarity", 0)
            sim_variance = distance_metrics.get("similarity_variance", 0)
            
            # è¯­ä¹‰è·¨åº¦åˆ†æ
            if avg_sim > 0.6:
                patterns["semantic_span"] = "ç´§å¯†"
            elif avg_sim > 0.3:
                patterns["semantic_span"] = "é€‚ä¸­"
            else:
                patterns["semantic_span"] = "åˆ†æ•£"
            
            # æ¦‚å¿µåˆ†å¸ƒåˆ†æ
            if sim_variance < 0.1:
                patterns["concept_distribution"] = "å‡åŒ€"
            elif sim_variance < 0.3:
                patterns["concept_distribution"] = "é€‚ä¸­"
            else:
                patterns["concept_distribution"] = "ä¸å‡åŒ€"
            
            # èšç±»å€¾å‘
            distribution = distance_metrics.get("similarity_distribution", {})
            high_sim = distribution.get("high_similarity_count", 0)
            total_pairs = distribution.get("total_pairs", 1)
            
            if high_sim / total_pairs > 0.3:
                patterns["clustering_tendency"] = "å¼º"
            elif high_sim / total_pairs > 0.1:
                patterns["clustering_tendency"] = "ä¸­ç­‰"
            else:
                patterns["clustering_tendency"] = "å¼±"
        
        except Exception as e:
            patterns["error"] = str(e)
        
        return patterns
    
    def _extract_clustering_behavioral_indicators(self, clustering_metrics: Dict[str, Any],
                                                llm_interpretation: Dict[str, Any]) -> Dict[str, Any]:
        """æå–èšç±»è¡Œä¸ºæŒ‡æ ‡"""
        indicators = {
            "conceptual_organization": "unknown",
            "thematic_coherence": "unknown", 
            "cognitive_complexity": "unknown"
        }
        
        try:
            cluster_count = clustering_metrics.get("cluster_count", 0)
            avg_size = clustering_metrics.get("average_cluster_size", 0)
            
            # æ¦‚å¿µç»„ç»‡èƒ½åŠ›
            if cluster_count >= 3 and avg_size >= 2:
                indicators["conceptual_organization"] = "è‰¯å¥½"
            elif cluster_count >= 2:
                indicators["conceptual_organization"] = "ä¸€èˆ¬"
            else:
                indicators["conceptual_organization"] = "ç®€å•"
            
            # ä»LLMè§£é‡Šä¸­æå–ä¸»é¢˜è¿è´¯æ€§
            if "overall_assessment" in llm_interpretation:
                assessment = llm_interpretation["overall_assessment"]
                thematic_clarity = assessment.get("thematic_clarity", 3)
                
                if thematic_clarity >= 4:
                    indicators["thematic_coherence"] = "é«˜"
                elif thematic_clarity >= 3:
                    indicators["thematic_coherence"] = "ä¸­ç­‰"
                else:
                    indicators["thematic_coherence"] = "ä½"
        
        except Exception as e:
            indicators["error"] = str(e)
        
        return indicators
    
    def _calculate_creativity_metrics(self, novelty_candidates: List[Dict[str, Any]],
                                    llm_assessments: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—åˆ›æ–°åº¦æŒ‡æ ‡"""
        metrics = {
            "total_candidates": len(novelty_candidates),
            "average_novelty_score": 0,
            "high_novelty_count": 0,
            "creativity_density": 0
        }
        
        try:
            # ä»LLMè¯„ä¼°ä¸­æå–åˆ›æ–°åº¦åˆ†æ•°
            assessments = llm_assessments.get("novelty_assessments", [])
            
            if assessments:
                scores = [a.get("novelty_score", 3) for a in assessments]
                metrics["average_novelty_score"] = np.mean(scores)
                metrics["high_novelty_count"] = sum(1 for s in scores if s >= 4)
                
                # åˆ›æ–°å¯†åº¦ = é«˜åˆ›æ–°åº¦æ¦‚å¿µå¯¹æ•°é‡ / æ€»æ¦‚å¿µå¯¹æ•°é‡
                total_pairs = len(novelty_candidates)
                metrics["creativity_density"] = metrics["high_novelty_count"] / total_pairs if total_pairs > 0 else 0
        
        except Exception as e:
            metrics["error"] = str(e)
        
        return metrics
    
    def _analyze_concept_emotions(self, semantic_units: Dict[str, Any],
                                vector_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ¦‚å¿µæƒ…æ„Ÿå€¾å‘"""
        concept_emotions = {}
        
        try:
            # åˆ†ææ¦‚å¿µçš„æƒ…æ„Ÿå…³è”
            concepts = semantic_units.get("concepts", [])
            adjectives = semantic_units.get("key_adjectives", [])
            
            # ä¸ºæ¯ä¸ªæ¦‚å¿µåˆ†é…æƒ…æ„Ÿå€¾å‘ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            for concept in concepts:
                concept_name = concept.get("text", "")
                # è¿™é‡Œå¯ä»¥é€šè¿‡æ›´å¤æ‚çš„ç®—æ³•æ¥åˆ†ææ¦‚å¿µä¸æƒ…æ„Ÿè¯çš„å…³è”
                concept_emotions[concept_name] = {
                    "emotional_association": "ä¸­æ€§",
                    "strength": 3,
                    "context": concept.get("role", "")
                }
        
        except Exception as e:
            concept_emotions["error"] = str(e)
        
        return concept_emotions
    
    def _extract_emotional_behavioral_patterns(self, emotional_distribution: Dict[str, Any],
                                             concept_emotions: Dict[str, Any],
                                             llm_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æƒ…æ„Ÿè¡Œä¸ºæ¨¡å¼"""
        patterns = {
            "emotional_expressiveness": "unknown",
            "emotional_balance": "unknown",
            "subjective_tendency": "unknown"
        }
        
        try:
            # æƒ…æ„Ÿè¡¨è¾¾åŠ›
            total_emotional = (emotional_distribution.get("positive_count", 0) + 
                             emotional_distribution.get("negative_count", 0))
            total_words = total_emotional + emotional_distribution.get("neutral_count", 0)
            
            if total_words > 0:
                emotional_ratio = total_emotional / total_words
                if emotional_ratio > 0.6:
                    patterns["emotional_expressiveness"] = "é«˜"
                elif emotional_ratio > 0.3:
                    patterns["emotional_expressiveness"] = "ä¸­ç­‰"
                else:
                    patterns["emotional_expressiveness"] = "ä½"
            
            # æƒ…æ„Ÿå¹³è¡¡
            pos_count = emotional_distribution.get("positive_count", 0)
            neg_count = emotional_distribution.get("negative_count", 0)
            
            if pos_count > 0 and neg_count > 0:
                balance_ratio = min(pos_count, neg_count) / max(pos_count, neg_count)
                if balance_ratio > 0.7:
                    patterns["emotional_balance"] = "å¹³è¡¡"
                else:
                    patterns["emotional_balance"] = "åå‘æ€§"
            elif pos_count > neg_count:
                patterns["emotional_balance"] = "ç§¯æå€¾å‘"
            elif neg_count > pos_count:
                patterns["emotional_balance"] = "æ¶ˆæå€¾å‘"
            else:
                patterns["emotional_balance"] = "ä¸­æ€§"
        
        except Exception as e:
            patterns["error"] = str(e)
        
        return patterns

    def _get_llm_distance_evaluation(self, distance_metrics: Dict[str, Any],
                                   pattern_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–LLMè·ç¦»è¯„ä¼°"""
        try:
            # å‡†å¤‡è·ç¦»ç»Ÿè®¡ä¿¡æ¯
            distance_stats = {
                "average_similarity": distance_metrics.get("average_similarity", 0),
                "similarity_variance": distance_metrics.get("similarity_variance", 0),
                "distribution": distance_metrics.get("similarity_distribution", {})
            }

            # å‡†å¤‡æ¦‚å¿µåˆ†å¸ƒä¿¡æ¯
            concept_distribution = {
                "semantic_span": pattern_analysis.get("semantic_span", "unknown"),
                "concept_distribution": pattern_analysis.get("concept_distribution", "unknown"),
                "clustering_tendency": pattern_analysis.get("clustering_tendency", "unknown")
            }

            # æ„å»ºæç¤ºè¯
            prompt = self.evaluation_templates["semantic_distance_evaluation"].format(
                distance_stats=json.dumps(distance_stats, ensure_ascii=False, indent=2),
                concept_distribution=json.dumps(concept_distribution, ensure_ascii=False, indent=2)
            )

            # è°ƒç”¨LLM
            response = self.llm_client.generate(prompt)

            # è§£æå“åº”
            return self._parse_json_response(response, "distance_evaluation")

        except Exception as e:
            return {"error": str(e)}

    def _get_llm_emotional_analysis(self, emotional_distribution: Dict[str, Any],
                                  concept_emotions: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–LLMæƒ…æ„Ÿåˆ†æ"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self.evaluation_templates["emotional_semantic_analysis"].format(
                emotional_distribution=json.dumps(emotional_distribution, ensure_ascii=False, indent=2),
                concept_emotions=json.dumps(concept_emotions, ensure_ascii=False, indent=2)
            )

            # è°ƒç”¨LLM
            response = self.llm_client.generate(prompt)

            # è§£æå“åº”
            return self._parse_json_response(response, "emotional_analysis")

        except Exception as e:
            return {"error": str(e)}
