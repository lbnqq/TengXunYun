"""
è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•å¼•æ“
æ•´åˆæ‰€æœ‰è¯­ä¹‰åˆ†æç»„ä»¶ï¼Œå®ç°å®Œæ•´çš„è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•
è®¯é£å¤§æ¨¡å‹ä½œä¸ºè¯­ä¹‰åˆ†æåŠ©æ‰‹å’Œé£æ ¼è¯„ä¼°å‘˜
"""

import json
import os
from typing import Dict, Any, List, Optional
from datetime import datetime

from .semantic_unit_identifier import SemanticUnitIdentifier
from .semantic_space_mapper import SemanticSpaceMapper
from .semantic_behavior_analyzer import SemanticBehaviorAnalyzer
from .semantic_style_profiler import SemanticStyleProfiler


class SemanticSpaceBehaviorEngine:
    """è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•å¼•æ“ - ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, llm_client=None, storage_path: str = "src/core/knowledge_base/semantic_analysis"):
        """
        åˆå§‹åŒ–è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•å¼•æ“
        
        Args:
            llm_client: è®¯é£å¤§æ¨¡å‹å®¢æˆ·ç«¯
            storage_path: å­˜å‚¨è·¯å¾„
        """
        self.llm_client = llm_client
        self.storage_path = storage_path
        os.makedirs(storage_path, exist_ok=True)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.unit_identifier = SemanticUnitIdentifier(llm_client)
        self.space_mapper = SemanticSpaceMapper(
            cache_dir=os.path.join(storage_path, "vectors")
        )
        self.behavior_analyzer = SemanticBehaviorAnalyzer(llm_client)
        self.style_profiler = SemanticStyleProfiler(
            storage_path=os.path.join(storage_path, "profiles")
        )
        
        # åˆ†æå†å²
        self.analysis_history = []
    
    def analyze_semantic_behavior(self, text: str, document_name: str = None,
                                analysis_depth: str = "comprehensive") -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            document_name: æ–‡æ¡£åç§°
            analysis_depth: åˆ†ææ·±åº¦ ("basic", "standard", "comprehensive")
        
        Returns:
            å®Œæ•´çš„è¯­ä¹‰è¡Œä¸ºåˆ†æç»“æœ
        """
        analysis_id = self._generate_analysis_id()
        
        result = {
            "analysis_id": analysis_id,
            "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
            "analysis_time": datetime.now().isoformat(),
            "analysis_depth": analysis_depth,
            "text_length": len(text),
            "text_preview": text[:200] + "..." if len(text) > 200 else text,
            "stage_results": {},
            "final_profile": {},
            "analysis_summary": {},
            "success": False
        }
        
        try:
            print(f"ğŸš€ å¼€å§‹è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ: {document_name or 'æœªå‘½åæ–‡æ¡£'}")
            print(f"ğŸ“Š åˆ†ææ·±åº¦: {analysis_depth}")
            
            # é˜¶æ®µä¸€ï¼šè¯­ä¹‰å•å…ƒè¯†åˆ«ä¸è¡¨ç¤º
            stage1_result = self._stage1_semantic_unit_identification(text, analysis_depth)
            result["stage_results"]["stage1_identification"] = stage1_result
            
            if not stage1_result.get("success"):
                raise Exception("é˜¶æ®µä¸€ï¼šè¯­ä¹‰å•å…ƒè¯†åˆ«å¤±è´¥")
            
            # é˜¶æ®µäºŒï¼šè¯­ä¹‰ç©ºé—´æ˜ å°„
            stage2_result = self._stage2_semantic_space_mapping(stage1_result, analysis_depth)
            result["stage_results"]["stage2_mapping"] = stage2_result
            
            if not stage2_result.get("success"):
                raise Exception("é˜¶æ®µäºŒï¼šè¯­ä¹‰ç©ºé—´æ˜ å°„å¤±è´¥")
            
            # é˜¶æ®µä¸‰ï¼šè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ
            stage3_result = self._stage3_behavior_analysis(
                stage1_result, stage2_result, text, analysis_depth
            )
            result["stage_results"]["stage3_behavior"] = stage3_result
            
            # é˜¶æ®µå››ï¼šç‰¹å¾èåˆä¸é£æ ¼ç”»åƒæ„å»º
            stage4_result = self._stage4_profile_construction(
                result["stage_results"], document_name
            )
            result["stage_results"]["stage4_profiling"] = stage4_result
            result["final_profile"] = stage4_result.get("profile", {})
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            result["analysis_summary"] = self._generate_analysis_summary(result)
            result["success"] = True
            
            # è®°å½•åˆ†æå†å²
            self._record_analysis_history(result)
            
            print("âœ… è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå®Œæˆ")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå¤±è´¥: {str(e)}")
        
        return result
    
    def _stage1_semantic_unit_identification(self, text: str, analysis_depth: str) -> Dict[str, Any]:
        """é˜¶æ®µä¸€ï¼šè¯­ä¹‰å•å…ƒè¯†åˆ«ä¸è¡¨ç¤º"""
        print("ğŸ” é˜¶æ®µä¸€ï¼šè¯­ä¹‰å•å…ƒè¯†åˆ«ä¸è¡¨ç¤º")
        
        stage_result = {
            "stage_name": "è¯­ä¹‰å•å…ƒè¯†åˆ«ä¸è¡¨ç¤º",
            "start_time": datetime.now().isoformat(),
            "semantic_units": {},
            "unit_statistics": {},
            "success": False
        }
        
        try:
            # 1. è¯­ä¹‰å•å…ƒè¯†åˆ«
            if analysis_depth == "comprehensive":
                identification_result = self.unit_identifier.identify_semantic_units(
                    text, "comprehensive"
                )
            elif analysis_depth == "standard":
                identification_result = self.unit_identifier.identify_semantic_units(
                    text, "concept"
                )
            else:  # basic
                identification_result = self.unit_identifier.identify_semantic_units(
                    text, "entity"
                )
            
            if identification_result.get("success"):
                stage_result["semantic_units"] = identification_result["semantic_units"]
                
                # 2. ç»Ÿè®¡ä¿¡æ¯
                unit_statistics = self.unit_identifier.get_semantic_unit_statistics(
                    identification_result["semantic_units"]
                )
                stage_result["unit_statistics"] = unit_statistics
                
                stage_result["success"] = True
                print("  âœ… è¯­ä¹‰å•å…ƒè¯†åˆ«å®Œæˆ")
            else:
                stage_result["error"] = identification_result.get("error", "è¯†åˆ«å¤±è´¥")
                print("  âŒ è¯­ä¹‰å•å…ƒè¯†åˆ«å¤±è´¥")
        
        except Exception as e:
            stage_result["error"] = str(e)
            print(f"  âŒ é˜¶æ®µä¸€æ‰§è¡Œå¤±è´¥: {str(e)}")
        
        stage_result["end_time"] = datetime.now().isoformat()
        return stage_result
    
    def _stage2_semantic_space_mapping(self, stage1_result: Dict[str, Any], 
                                     analysis_depth: str) -> Dict[str, Any]:
        """é˜¶æ®µäºŒï¼šè¯­ä¹‰ç©ºé—´æ˜ å°„"""
        print("ğŸ—ºï¸ é˜¶æ®µäºŒï¼šè¯­ä¹‰ç©ºé—´æ˜ å°„")
        
        stage_result = {
            "stage_name": "è¯­ä¹‰ç©ºé—´æ˜ å°„",
            "start_time": datetime.now().isoformat(),
            "vector_result": {},
            "similarity_result": {},
            "cluster_result": {},
            "success": False
        }
        
        try:
            semantic_units = stage1_result.get("semantic_units", {})
            
            # 1. å‘é‡ç¼–ç 
            vector_result = self.space_mapper.encode_semantic_units(semantic_units)
            stage_result["vector_result"] = vector_result
            
            if vector_result.get("success"):
                print("  âœ… è¯­ä¹‰å•å…ƒå‘é‡ç¼–ç å®Œæˆ")
                
                # 2. ç›¸ä¼¼åº¦è®¡ç®—
                if analysis_depth in ["standard", "comprehensive"]:
                    similarity_result = self.space_mapper.calculate_semantic_similarities(
                        vector_result, "cosine"
                    )
                    stage_result["similarity_result"] = similarity_result
                    
                    if similarity_result.get("success"):
                        print("  âœ… è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")
                
                # 3. èšç±»åˆ†æ
                if analysis_depth == "comprehensive":
                    cluster_result = self.space_mapper.find_semantic_clusters(vector_result)
                    stage_result["cluster_result"] = cluster_result
                    
                    if cluster_result.get("success"):
                        print("  âœ… è¯­ä¹‰èšç±»åˆ†æå®Œæˆ")
                
                stage_result["success"] = True
            else:
                stage_result["error"] = vector_result.get("error", "å‘é‡ç¼–ç å¤±è´¥")
                print("  âŒ è¯­ä¹‰ç©ºé—´æ˜ å°„å¤±è´¥")
        
        except Exception as e:
            stage_result["error"] = str(e)
            print(f"  âŒ é˜¶æ®µäºŒæ‰§è¡Œå¤±è´¥: {str(e)}")
        
        stage_result["end_time"] = datetime.now().isoformat()
        return stage_result
    
    def _stage3_behavior_analysis(self, stage1_result: Dict[str, Any],
                                stage2_result: Dict[str, Any],
                                original_text: str,
                                analysis_depth: str) -> Dict[str, Any]:
        """é˜¶æ®µä¸‰ï¼šè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ"""
        print("ğŸ§  é˜¶æ®µä¸‰ï¼šè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ")
        
        stage_result = {
            "stage_name": "è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ",
            "start_time": datetime.now().isoformat(),
            "clustering_analysis": {},
            "distance_analysis": {},
            "novelty_assessment": {},
            "emotional_analysis": {},
            "success": False
        }
        
        try:
            vector_result = stage2_result.get("vector_result", {})
            similarity_result = stage2_result.get("similarity_result", {})
            cluster_result = stage2_result.get("cluster_result", {})
            semantic_units = stage1_result.get("semantic_units", {})
            
            # 1. æ¦‚å¿µèšç±»åˆ†æ
            if cluster_result.get("success") and analysis_depth == "comprehensive":
                clustering_analysis = self.behavior_analyzer.analyze_concept_clustering(
                    vector_result, cluster_result, original_text
                )
                stage_result["clustering_analysis"] = clustering_analysis
                
                if clustering_analysis.get("success"):
                    print("  âœ… æ¦‚å¿µèšç±»è¡Œä¸ºåˆ†æå®Œæˆ")
            
            # 2. è¯­ä¹‰è·ç¦»æ¨¡å¼åˆ†æ
            if similarity_result.get("success") and analysis_depth in ["standard", "comprehensive"]:
                distance_analysis = self.behavior_analyzer.analyze_semantic_distance_patterns(
                    vector_result, similarity_result
                )
                stage_result["distance_analysis"] = distance_analysis
                
                if distance_analysis.get("success"):
                    print("  âœ… è¯­ä¹‰è·ç¦»æ¨¡å¼åˆ†æå®Œæˆ")
            
            # 3. è”æƒ³åˆ›æ–°åº¦è¯„ä¼°
            if (similarity_result.get("success") and 
                analysis_depth == "comprehensive" and 
                self.llm_client):
                
                novelty_assessment = self.behavior_analyzer.assess_associative_novelty(
                    vector_result, similarity_result, original_text
                )
                stage_result["novelty_assessment"] = novelty_assessment
                
                if novelty_assessment.get("success"):
                    print("  âœ… è”æƒ³åˆ›æ–°åº¦è¯„ä¼°å®Œæˆ")
            
            # 4. æƒ…æ„Ÿè¯­ä¹‰è¡Œä¸ºåˆ†æ
            if semantic_units and analysis_depth in ["standard", "comprehensive"]:
                emotional_analysis = self.behavior_analyzer.analyze_emotional_semantic_behavior(
                    semantic_units, vector_result
                )
                stage_result["emotional_analysis"] = emotional_analysis
                
                if emotional_analysis.get("success"):
                    print("  âœ… æƒ…æ„Ÿè¯­ä¹‰è¡Œä¸ºåˆ†æå®Œæˆ")
            
            stage_result["success"] = True
            print("  âœ… è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå®Œæˆ")
        
        except Exception as e:
            stage_result["error"] = str(e)
            print(f"  âŒ é˜¶æ®µä¸‰æ‰§è¡Œå¤±è´¥: {str(e)}")
        
        stage_result["end_time"] = datetime.now().isoformat()
        return stage_result
    
    def _stage4_profile_construction(self, stage_results: Dict[str, Any],
                                   document_name: str) -> Dict[str, Any]:
        """é˜¶æ®µå››ï¼šç‰¹å¾èåˆä¸é£æ ¼ç”»åƒæ„å»º"""
        print("ğŸ¨ é˜¶æ®µå››ï¼šç‰¹å¾èåˆä¸é£æ ¼ç”»åƒæ„å»º")
        
        stage_result = {
            "stage_name": "ç‰¹å¾èåˆä¸é£æ ¼ç”»åƒæ„å»º",
            "start_time": datetime.now().isoformat(),
            "profile": {},
            "success": False
        }
        
        try:
            # æ•´åˆæ‰€æœ‰åˆ†æç»“æœ
            analysis_results = {
                "vector_result": stage_results.get("stage2_mapping", {}).get("vector_result", {}),
                "similarity_result": stage_results.get("stage2_mapping", {}).get("similarity_result", {}),
                "cluster_result": stage_results.get("stage2_mapping", {}).get("cluster_result", {}),
                "clustering_analysis": stage_results.get("stage3_behavior", {}).get("clustering_analysis", {}),
                "distance_analysis": stage_results.get("stage3_behavior", {}).get("distance_analysis", {}),
                "novelty_assessment": stage_results.get("stage3_behavior", {}).get("novelty_assessment", {}),
                "emotional_analysis": stage_results.get("stage3_behavior", {}).get("emotional_analysis", {})
            }
            
            # æ„å»ºè¯­ä¹‰é£æ ¼ç”»åƒ
            profile = self.style_profiler.build_semantic_style_profile(
                analysis_results, document_name
            )
            stage_result["profile"] = profile
            
            if profile.get("success"):
                print("  âœ… è¯­ä¹‰é£æ ¼ç”»åƒæ„å»ºå®Œæˆ")
                stage_result["success"] = True
            else:
                stage_result["error"] = profile.get("error", "ç”»åƒæ„å»ºå¤±è´¥")
                print("  âŒ è¯­ä¹‰é£æ ¼ç”»åƒæ„å»ºå¤±è´¥")
        
        except Exception as e:
            stage_result["error"] = str(e)
            print(f"  âŒ é˜¶æ®µå››æ‰§è¡Œå¤±è´¥: {str(e)}")
        
        stage_result["end_time"] = datetime.now().isoformat()
        return stage_result
    
    def _generate_analysis_summary(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            "analysis_success": result.get("success", False),
            "stages_completed": 0,
            "total_processing_time": "unknown",
            "key_findings": [],
            "semantic_characteristics": {},
            "style_profile_summary": {}
        }
        
        try:
            # ç»Ÿè®¡å®Œæˆçš„é˜¶æ®µ
            stage_results = result.get("stage_results", {})
            completed_stages = sum(1 for stage in stage_results.values() if stage.get("success"))
            summary["stages_completed"] = completed_stages
            
            # å…³é”®å‘ç°
            findings = []
            
            # ä»è¯­ä¹‰å•å…ƒè¯†åˆ«ä¸­æå–å‘ç°
            stage1 = stage_results.get("stage1_identification", {})
            if stage1.get("success"):
                unit_stats = stage1.get("unit_statistics", {})
                concept_count = unit_stats.get("concept_count", 0)
                if concept_count > 0:
                    findings.append(f"è¯†åˆ«å‡º {concept_count} ä¸ªæ ¸å¿ƒæ¦‚å¿µ")
            
            # ä»èšç±»åˆ†æä¸­æå–å‘ç°
            stage3 = stage_results.get("stage3_behavior", {})
            clustering_analysis = stage3.get("clustering_analysis", {})
            if clustering_analysis.get("success"):
                clustering_metrics = clustering_analysis.get("clustering_metrics", {})
                cluster_count = clustering_metrics.get("cluster_count", 0)
                if cluster_count > 0:
                    findings.append(f"å‘ç° {cluster_count} ä¸ªæ¦‚å¿µèšç±»")
            
            # ä»é£æ ¼ç”»åƒä¸­æå–å‘ç°
            final_profile = result.get("final_profile", {})
            if final_profile.get("success"):
                style_classification = final_profile.get("style_classification", {})
                primary_style = style_classification.get("primary_style", "")
                if primary_style:
                    findings.append(f"ä¸»è¦é£æ ¼ç±»å‹ï¼š{primary_style}")
            
            summary["key_findings"] = findings
            
            # è¯­ä¹‰ç‰¹å¾
            if final_profile.get("success"):
                style_scores = final_profile.get("style_scores", {})
                summary["semantic_characteristics"] = {
                    "æ¦‚å¿µç»„ç»‡èƒ½åŠ›": style_scores.get("conceptual_organization", 3.0),
                    "è¯­ä¹‰è¿è´¯æ€§": style_scores.get("semantic_coherence", 3.0),
                    "åˆ›æ–°è”æƒ³èƒ½åŠ›": style_scores.get("creative_association", 3.0),
                    "æƒ…æ„Ÿè¡¨è¾¾åŠ›": style_scores.get("emotional_expression", 3.0)
                }
                
                # é£æ ¼ç”»åƒæ‘˜è¦
                profile_summary = final_profile.get("profile_summary", {})
                summary["style_profile_summary"] = {
                    "é£æ ¼ç±»å‹": profile_summary.get("profile_type", "unknown"),
                    "å…³é”®ä¼˜åŠ¿": profile_summary.get("key_strengths", []),
                    "ç‹¬ç‰¹æ€§åˆ†æ•°": profile_summary.get("uniqueness_score", 0.0)
                }
        
        except Exception as e:
            summary["error"] = str(e)
        
        return summary
    
    def _generate_analysis_id(self) -> str:
        """ç”Ÿæˆåˆ†æID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        return f"semantic_analysis_{timestamp}"
    
    def _record_analysis_history(self, result: Dict[str, Any]):
        """è®°å½•åˆ†æå†å²"""
        history_entry = {
            "analysis_id": result.get("analysis_id"),
            "document_name": result.get("document_name"),
            "analysis_time": result.get("analysis_time"),
            "success": result.get("success"),
            "text_length": result.get("text_length"),
            "analysis_depth": result.get("analysis_depth"),
            "stages_completed": result.get("analysis_summary", {}).get("stages_completed", 0)
        }
        
        self.analysis_history.append(history_entry)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.analysis_history) > 50:
            self.analysis_history = self.analysis_history[-50:]
    
    def get_analysis_history(self) -> List[Dict[str, Any]]:
        """è·å–åˆ†æå†å²"""
        return self.analysis_history.copy()
    
    def save_analysis_result(self, result: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        if not filename:
            analysis_id = result.get("analysis_id", "unknown")
            filename = f"semantic_analysis_{analysis_id}.json"
        
        filepath = os.path.join(self.storage_path, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            return filepath
        except Exception as e:
            return f"ä¿å­˜å¤±è´¥: {str(e)}"
    
    def compare_semantic_profiles(self, text1: str, text2: str,
                                doc1_name: str = None, doc2_name: str = None) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰é£æ ¼ç”»åƒ"""
        comparison_result = {
            "comparison_id": self._generate_analysis_id(),
            "comparison_time": datetime.now().isoformat(),
            "document1_analysis": {},
            "document2_analysis": {},
            "profile_comparison": {},
            "success": False
        }
        
        try:
            print("ğŸ”„ å¼€å§‹è¯­ä¹‰é£æ ¼ç”»åƒæ¯”è¾ƒ...")
            
            # åˆ†æç¬¬ä¸€ä¸ªæ–‡æ¡£
            print("åˆ†ææ–‡æ¡£1...")
            analysis1 = self.analyze_semantic_behavior(text1, doc1_name or "æ–‡æ¡£1", "comprehensive")
            comparison_result["document1_analysis"] = analysis1
            
            # åˆ†æç¬¬äºŒä¸ªæ–‡æ¡£
            print("åˆ†ææ–‡æ¡£2...")
            analysis2 = self.analyze_semantic_behavior(text2, doc2_name or "æ–‡æ¡£2", "comprehensive")
            comparison_result["document2_analysis"] = analysis2
            
            # æ¯”è¾ƒé£æ ¼ç”»åƒ
            if (analysis1.get("success") and analysis2.get("success") and
                analysis1.get("final_profile", {}).get("success") and
                analysis2.get("final_profile", {}).get("success")):
                
                profile1 = analysis1["final_profile"]
                profile2 = analysis2["final_profile"]
                
                profile_comparison = self.style_profiler.compare_profiles(profile1, profile2)
                comparison_result["profile_comparison"] = profile_comparison
                
                comparison_result["success"] = True
                print("âœ… è¯­ä¹‰é£æ ¼ç”»åƒæ¯”è¾ƒå®Œæˆ")
            else:
                comparison_result["error"] = "æ–‡æ¡£åˆ†æå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ¯”è¾ƒ"
                print("âŒ è¯­ä¹‰é£æ ¼ç”»åƒæ¯”è¾ƒå¤±è´¥")
        
        except Exception as e:
            comparison_result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰é£æ ¼ç”»åƒæ¯”è¾ƒå¤±è´¥: {str(e)}")
        
        return comparison_result
