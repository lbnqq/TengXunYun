#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Style Processor - æ ¸å¿ƒæ¨¡å—

Author: AI Assistant (Claude)
Created: 2025-01-28
Last Modified: 2025-01-28
Modified By: AI Assistant (Claude)
AI Assisted: æ˜¯ - Claude 3.5 Sonnet
Version: v1.0
License: MIT
"""


import json
import os
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

from .enhanced_style_extractor import EnhancedStyleExtractor
from .llm_style_analyzer import AdvancedLLMStyleAnalyzer
from .feature_fusion_processor import FeatureFusionProcessor
from .style_alignment_engine import StyleAlignmentEngine

# å¯¼å…¥è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•ç»„ä»¶
try:
    from .semantic_space_behavior_engine import SemanticSpaceBehaviorEngine
    SEMANTIC_BEHAVIOR_AVAILABLE = True
except ImportError:
    SEMANTIC_BEHAVIOR_AVAILABLE = False
    print("Warning: Semantic space behavior analysis not available.")


class ComprehensiveStyleProcessor:
    """ç»¼åˆæ–‡é£å¤„ç†å™¨ - ä¸»æ¥å£ç±»"""
    
    def __init__(self, llm_client=None, storage_path: str = "src/core/knowledge_base/comprehensive_style"):
        """
        åˆå§‹åŒ–ç»¼åˆæ–‡é£å¤„ç†å™¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            storage_path: å­˜å‚¨è·¯å¾„
        """
        self.llm_client = llm_client
        self.storage_path = storage_path
        os.makedirs(storage_path, exist_ok=True)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.style_extractor = EnhancedStyleExtractor(llm_client,
            os.path.join(storage_path, "extracted_features"))
        self.llm_analyzer = AdvancedLLMStyleAnalyzer(llm_client)
        self.fusion_processor = FeatureFusionProcessor(
            os.path.join(storage_path, "fusion_models"))
        self.alignment_engine = StyleAlignmentEngine(llm_client,
            os.path.join(storage_path, "alignments"))

        # åˆå§‹åŒ–è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•å¼•æ“
        if SEMANTIC_BEHAVIOR_AVAILABLE and llm_client:
            self.semantic_engine = SemanticSpaceBehaviorEngine(
                llm_client=llm_client,
                storage_path=os.path.join(storage_path, "semantic_behavior")
            )
            self.semantic_analysis_enabled = True
            print("âœ… è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æåŠŸèƒ½å·²å¯ç”¨")
        else:
            self.semantic_engine = None
            self.semantic_analysis_enabled = False
            print("âš ï¸ è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æåŠŸèƒ½æœªå¯ç”¨")
        
        # å¤„ç†å†å²è®°å½•
        self.processing_history = []
    
    def extract_comprehensive_style_features(self, text: str, document_name: str = None,
                                           include_advanced_analysis: bool = True) -> Dict[str, Any]:
        """
        æå–ç»¼åˆæ–‡é£ç‰¹å¾
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            document_name: æ–‡æ¡£åç§°
            include_advanced_analysis: æ˜¯å¦åŒ…å«é«˜çº§åˆ†æ
        """
        print(f"å¼€å§‹æå–æ–‡é£ç‰¹å¾: {document_name or 'æœªå‘½åæ–‡æ¡£'}")
        
        result = {
            "processing_id": self._generate_processing_id(),
            "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
            "processing_time": datetime.now().isoformat(),
            "text_length": len(text),
            "basic_features": {},
            "advanced_features": {},
            "comprehensive_analysis": {},
            "processing_summary": {},
            "success": False
        }
        
        try:
            # 1. åŸºç¡€ç‰¹å¾æå–
            print("æ­£åœ¨æå–åŸºç¡€ç‰¹å¾...")
            basic_features = self.style_extractor.extract_comprehensive_features(text, document_name)
            result["basic_features"] = basic_features
            
            # 2. é«˜çº§LLMåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if include_advanced_analysis and self.llm_client:
                print("æ­£åœ¨è¿›è¡Œé«˜çº§LLMåˆ†æ...")
                advanced_features = {}
                
                # ç»¼åˆæ–‡é£åˆ†æ
                comprehensive_analysis = self.llm_analyzer.comprehensive_style_analysis(text)
                advanced_features["comprehensive_analysis"] = comprehensive_analysis
                
                # æˆè¯­å’Œä¿®è¾åˆ†æ
                rhetoric_analysis = self.llm_analyzer.analyze_idioms_and_rhetoric(text)
                advanced_features["rhetoric_analysis"] = rhetoric_analysis
                
                # æ­£å¼ç¨‹åº¦åˆ†æ
                formality_analysis = self.llm_analyzer.analyze_formality(text)
                advanced_features["formality_analysis"] = formality_analysis
                
                result["advanced_features"] = advanced_features
            
            # 3. ç‰¹å¾èåˆ
            if basic_features.get("success") and result.get("advanced_features"):
                print("æ­£åœ¨è¿›è¡Œç‰¹å¾èåˆ...")
                fusion_result = self.fusion_processor.fuse_features(
                    basic_features.get("quantitative_features", {}),
                    basic_features.get("llm_features", {}),
                    fusion_method="weighted_concat"
                )
                result["comprehensive_analysis"]["fusion_result"] = fusion_result
            
            # 4. ç”Ÿæˆå¤„ç†æ‘˜è¦
            result["processing_summary"] = self._generate_processing_summary(result)
            result["success"] = True
            
            # 5. è®°å½•å¤„ç†å†å²
            self._record_processing_history(result)
            
            print("æ–‡é£ç‰¹å¾æå–å®Œæˆ!")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"æ–‡é£ç‰¹å¾æå–å¤±è´¥: {str(e)}")
        
        return result
    
    def compare_document_styles(self, text1: str, text2: str, 
                              doc1_name: str = None, doc2_name: str = None) -> Dict[str, Any]:
        """
        æ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£çš„æ–‡é£
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æ¡£æ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æ¡£æ–‡æœ¬
            doc1_name: ç¬¬ä¸€ä¸ªæ–‡æ¡£åç§°
            doc2_name: ç¬¬äºŒä¸ªæ–‡æ¡£åç§°
        """
        print(f"å¼€å§‹æ¯”è¾ƒæ–‡æ¡£é£æ ¼: {doc1_name or 'æ–‡æ¡£1'} vs {doc2_name or 'æ–‡æ¡£2'}")
        
        result = {
            "comparison_id": self._generate_processing_id(),
            "comparison_time": datetime.now().isoformat(),
            "document1_name": doc1_name or "æ–‡æ¡£1",
            "document2_name": doc2_name or "æ–‡æ¡£2",
            "document1_features": {},
            "document2_features": {},
            "similarity_analysis": {},
            "difference_analysis": {},
            "comparison_summary": {},
            "success": False
        }
        
        try:
            # 1. æå–ä¸¤ä¸ªæ–‡æ¡£çš„ç‰¹å¾
            print("æ­£åœ¨æå–ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„ç‰¹å¾...")
            doc1_features = self.extract_comprehensive_style_features(text1, doc1_name, True)
            result["document1_features"] = doc1_features
            
            print("æ­£åœ¨æå–ç¬¬äºŒä¸ªæ–‡æ¡£çš„ç‰¹å¾...")
            doc2_features = self.extract_comprehensive_style_features(text2, doc2_name, True)
            result["document2_features"] = doc2_features
            
            # 2. è®¡ç®—ç›¸ä¼¼åº¦
            if (doc1_features.get("success") and doc2_features.get("success") and
                doc1_features.get("basic_features", {}).get("feature_vector") and
                doc2_features.get("basic_features", {}).get("feature_vector")):
                
                print("æ­£åœ¨è®¡ç®—é£æ ¼ç›¸ä¼¼åº¦...")
                similarity_result = self.alignment_engine.similarity_calculator.calculate_similarity(
                    doc1_features["basic_features"]["feature_vector"],
                    doc2_features["basic_features"]["feature_vector"],
                    method="cosine"
                )
                result["similarity_analysis"] = similarity_result
            
            # 3. LLMå¯¹æ¯”åˆ†æ
            if self.llm_client:
                print("æ­£åœ¨è¿›è¡ŒLLMå¯¹æ¯”åˆ†æ...")
                llm_comparison = self.llm_analyzer.compare_styles(text1, text2)
                result["difference_analysis"] = llm_comparison
            
            # 4. ç”Ÿæˆæ¯”è¾ƒæ‘˜è¦
            result["comparison_summary"] = self._generate_comparison_summary(result)
            result["success"] = True
            
            print("æ–‡æ¡£é£æ ¼æ¯”è¾ƒå®Œæˆ!")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"æ–‡æ¡£é£æ ¼æ¯”è¾ƒå¤±è´¥: {str(e)}")
        
        return result
    
    def align_text_style(self, source_text: str, target_text: str, content_to_align: str,
                        source_name: str = None, target_name: str = None) -> Dict[str, Any]:
        """
        å°†æ–‡æœ¬å¯¹é½åˆ°ç›®æ ‡é£æ ¼
        
        Args:
            source_text: æºæ–‡æ¡£æ–‡æœ¬ï¼ˆå½“å‰é£æ ¼ï¼‰
            target_text: ç›®æ ‡æ–‡æ¡£æ–‡æœ¬ï¼ˆç›®æ ‡é£æ ¼ï¼‰
            content_to_align: éœ€è¦å¯¹é½çš„å†…å®¹
            source_name: æºæ–‡æ¡£åç§°
            target_name: ç›®æ ‡æ–‡æ¡£åç§°
        """
        print(f"å¼€å§‹æ–‡é£å¯¹é½: {source_name or 'æºæ–‡æ¡£'} -> {target_name or 'ç›®æ ‡æ–‡æ¡£'}")
        
        result = {
            "alignment_id": self._generate_processing_id(),
            "alignment_time": datetime.now().isoformat(),
            "source_name": source_name or "æºæ–‡æ¡£",
            "target_name": target_name or "ç›®æ ‡æ–‡æ¡£",
            "original_content": content_to_align,
            "aligned_content": "",
            "source_features": {},
            "target_features": {},
            "alignment_result": {},
            "quality_assessment": {},
            "success": False
        }
        
        try:
            # 1. æå–æºæ–‡æ¡£å’Œç›®æ ‡æ–‡æ¡£çš„ç‰¹å¾
            print("æ­£åœ¨åˆ†ææºæ–‡æ¡£é£æ ¼...")
            source_features = self.extract_comprehensive_style_features(source_text, source_name, True)
            result["source_features"] = source_features
            
            print("æ­£åœ¨åˆ†æç›®æ ‡æ–‡æ¡£é£æ ¼...")
            target_features = self.extract_comprehensive_style_features(target_text, target_name, True)
            result["target_features"] = target_features
            
            # 2. æ‰§è¡Œæ–‡é£å¯¹é½
            if source_features.get("success") and target_features.get("success"):
                print("æ­£åœ¨æ‰§è¡Œæ–‡é£å¯¹é½...")
                alignment_result = self.alignment_engine.align_style(
                    source_features, target_features, content_to_align, "comprehensive"
                )
                result["alignment_result"] = alignment_result
                
                if alignment_result.get("success"):
                    transfer_result = alignment_result.get("transfer_result", {})
                    result["aligned_content"] = transfer_result.get("rewritten_content", "")
                    result["quality_assessment"] = alignment_result.get("alignment_quality", {})
            
            result["success"] = True
            print("æ–‡é£å¯¹é½å®Œæˆ!")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"æ–‡é£å¯¹é½å¤±è´¥: {str(e)}")
        
        return result

    def analyze_semantic_behavior(self, text: str, document_name: str = None,
                                analysis_depth: str = "comprehensive") -> Dict[str, Any]:
        """
        æ‰§è¡Œè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ

        Args:
            text: æ–‡æœ¬å†…å®¹
            document_name: æ–‡æ¡£åç§°
            analysis_depth: åˆ†ææ·±åº¦ ("basic", "standard", "comprehensive")
        """
        if not self.semantic_analysis_enabled:
            return {"error": "è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æåŠŸèƒ½æœªå¯ç”¨"}

        print(f"ğŸ§  å¼€å§‹è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ: {document_name or 'æœªå‘½åæ–‡æ¡£'}")

        result = {
            "analysis_type": "semantic_behavior",
            "analysis_time": datetime.now().isoformat(),
            "document_name": document_name or "æœªå‘½åæ–‡æ¡£",
            "analysis_depth": analysis_depth,
            "semantic_analysis": {},
            "integration_with_traditional": {},
            "comprehensive_insights": {},
            "success": False
        }

        try:
            # 1. æ‰§è¡Œè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ
            semantic_analysis = self.semantic_engine.analyze_semantic_behavior(
                text, document_name, analysis_depth
            )
            result["semantic_analysis"] = semantic_analysis

            # 2. å¦‚æœè¯­ä¹‰åˆ†ææˆåŠŸï¼Œå°è¯•ä¸ä¼ ç»Ÿåˆ†æç»“åˆ
            if semantic_analysis.get("success"):
                print("ğŸ”„ æ­£åœ¨æ•´åˆä¼ ç»Ÿæ–‡é£åˆ†æ...")
                traditional_analysis = self.extract_comprehensive_style_features(
                    text, document_name, include_advanced_analysis=True
                )
                result["traditional_analysis"] = traditional_analysis

                # 3. æ•´åˆä¸¤ç§åˆ†æç»“æœ
                if traditional_analysis.get("success"):
                    integration_result = self._integrate_semantic_and_traditional_analysis(
                        semantic_analysis, traditional_analysis
                    )
                    result["integration_with_traditional"] = integration_result

                    # 4. ç”Ÿæˆç»¼åˆæ´å¯Ÿ
                    comprehensive_insights = self._generate_comprehensive_insights(
                        semantic_analysis, traditional_analysis, integration_result
                    )
                    result["comprehensive_insights"] = comprehensive_insights

            result["success"] = True
            print("âœ… è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå®Œæˆ")

        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æå¤±è´¥: {str(e)}")

        return result

    def compare_semantic_profiles(self, text1: str, text2: str,
                                doc1_name: str = None, doc2_name: str = None) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£çš„è¯­ä¹‰é£æ ¼ç”»åƒ"""
        if not self.semantic_analysis_enabled:
            return {"error": "è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æåŠŸèƒ½æœªå¯ç”¨"}

        print(f"ğŸ” å¼€å§‹è¯­ä¹‰é£æ ¼ç”»åƒæ¯”è¾ƒ: {doc1_name or 'æ–‡æ¡£1'} vs {doc2_name or 'æ–‡æ¡£2'}")

        try:
            comparison_result = self.semantic_engine.compare_semantic_profiles(
                text1, text2, doc1_name, doc2_name
            )

            if comparison_result.get("success"):
                print("âœ… è¯­ä¹‰é£æ ¼ç”»åƒæ¯”è¾ƒå®Œæˆ")
            else:
                print("âŒ è¯­ä¹‰é£æ ¼ç”»åƒæ¯”è¾ƒå¤±è´¥")

            return comparison_result

        except Exception as e:
            return {
                "error": str(e),
                "success": False,
                "comparison_time": datetime.now().isoformat()
            }

    def _integrate_semantic_and_traditional_analysis(self, semantic_analysis: Dict[str, Any],
                                                   traditional_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆè¯­ä¹‰åˆ†æå’Œä¼ ç»Ÿåˆ†æç»“æœ"""
        integration = {
            "integration_time": datetime.now().isoformat(),
            "feature_correlation": {},
            "complementary_insights": {},
            "unified_assessment": {},
            "success": False
        }

        try:
            # 1. ç‰¹å¾å…³è”åˆ†æ
            semantic_profile = semantic_analysis.get("final_profile", {})
            traditional_features = traditional_analysis.get("basic_features", {})

            if semantic_profile.get("success") and traditional_features.get("success"):
                # æå–å…³é”®æŒ‡æ ‡è¿›è¡Œå…³è”
                semantic_scores = semantic_profile.get("style_scores", {})
                traditional_vector = traditional_features.get("feature_vector", [])

                integration["feature_correlation"] = {
                    "semantic_conceptual_organization": semantic_scores.get("conceptual_organization", 3.0),
                    "traditional_feature_count": len(traditional_vector),
                    "semantic_creativity": semantic_scores.get("creative_association", 3.0),
                    "traditional_complexity": traditional_features.get("processing_summary", {}).get("features_extracted", 0)
                }

            # 2. äº’è¡¥æ´å¯Ÿ
            complementary_insights = []

            # è¯­ä¹‰åˆ†æçš„ç‹¬ç‰¹å‘ç°
            semantic_summary = semantic_analysis.get("analysis_summary", {})
            semantic_findings = semantic_summary.get("key_findings", [])
            for finding in semantic_findings:
                complementary_insights.append(f"è¯­ä¹‰åˆ†æå‘ç°: {finding}")

            # ä¼ ç»Ÿåˆ†æçš„ç‹¬ç‰¹å‘ç°
            traditional_summary = traditional_analysis.get("processing_summary", {})
            traditional_modules = traditional_summary.get("analysis_modules_used", [])
            if traditional_modules:
                complementary_insights.append(f"ä¼ ç»Ÿåˆ†ææ¨¡å—: {', '.join(traditional_modules)}")

            integration["complementary_insights"] = complementary_insights

            # 3. ç»Ÿä¸€è¯„ä¼°
            unified_assessment = self._create_unified_assessment(
                semantic_analysis, traditional_analysis
            )
            integration["unified_assessment"] = unified_assessment

            integration["success"] = True

        except Exception as e:
            integration["error"] = str(e)

        return integration

    def _create_unified_assessment(self, semantic_analysis: Dict[str, Any],
                                 traditional_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºç»Ÿä¸€è¯„ä¼°"""
        assessment = {
            "overall_style_type": "unknown",
            "confidence_level": 0.0,
            "key_characteristics": [],
            "writing_strengths": [],
            "improvement_suggestions": []
        }

        try:
            # ä»è¯­ä¹‰åˆ†æä¸­æå–é£æ ¼ç±»å‹
            semantic_profile = semantic_analysis.get("final_profile", {})
            if semantic_profile.get("success"):
                style_classification = semantic_profile.get("style_classification", {})
                assessment["overall_style_type"] = style_classification.get("primary_style", "unknown")

                # å…³é”®ç‰¹å¾
                characteristics = style_classification.get("style_characteristics", [])
                assessment["key_characteristics"].extend(characteristics)

                # å†™ä½œä¼˜åŠ¿
                profile_summary = semantic_profile.get("profile_summary", {})
                strengths = profile_summary.get("key_strengths", [])
                assessment["writing_strengths"].extend(strengths)

            # ä»ä¼ ç»Ÿåˆ†æä¸­è¡¥å……ä¿¡æ¯
            traditional_features = traditional_analysis.get("basic_features", {})
            if traditional_features.get("success"):
                processing_summary = traditional_features.get("processing_summary", {})
                key_chars = processing_summary.get("key_characteristics", [])
                assessment["key_characteristics"].extend(key_chars)

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºä¸¤ç§åˆ†æçš„æˆåŠŸç¨‹åº¦ï¼‰
            semantic_success = 1.0 if semantic_analysis.get("success") else 0.0
            traditional_success = 1.0 if traditional_analysis.get("success") else 0.0
            assessment["confidence_level"] = (semantic_success + traditional_success) / 2.0

            # å»é‡
            assessment["key_characteristics"] = list(set(assessment["key_characteristics"]))
            assessment["writing_strengths"] = list(set(assessment["writing_strengths"]))

        except Exception as e:
            assessment["error"] = str(e)

        return assessment

    def _generate_comprehensive_insights(self, semantic_analysis: Dict[str, Any],
                                       traditional_analysis: Dict[str, Any],
                                       integration_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæ´å¯Ÿ"""
        insights = {
            "analysis_depth": "comprehensive",
            "multi_dimensional_assessment": {},
            "cross_validation_results": {},
            "actionable_recommendations": [],
            "unique_discoveries": []
        }

        try:
            # å¤šç»´åº¦è¯„ä¼°
            semantic_profile = semantic_analysis.get("final_profile", {})
            traditional_features = traditional_analysis.get("basic_features", {})

            if semantic_profile.get("success") and traditional_features.get("success"):
                semantic_scores = semantic_profile.get("style_scores", {})

                insights["multi_dimensional_assessment"] = {
                    "è¯­ä¹‰å±‚é¢": {
                        "æ¦‚å¿µç»„ç»‡èƒ½åŠ›": semantic_scores.get("conceptual_organization", 3.0),
                        "åˆ›æ–°è”æƒ³èƒ½åŠ›": semantic_scores.get("creative_association", 3.0),
                        "æƒ…æ„Ÿè¡¨è¾¾åŠ›": semantic_scores.get("emotional_expression", 3.0)
                    },
                    "ä¼ ç»Ÿå±‚é¢": {
                        "ç‰¹å¾ä¸°å¯Œåº¦": len(traditional_features.get("feature_vector", [])),
                        "åˆ†ææ¨¡å—æ•°": len(traditional_features.get("processing_summary", {}).get("analysis_modules_used", []))
                    }
                }

            # äº¤å‰éªŒè¯ç»“æœ
            unified_assessment = integration_result.get("unified_assessment", {})
            insights["cross_validation_results"] = {
                "é£æ ¼ç±»å‹ä¸€è‡´æ€§": "é«˜" if unified_assessment.get("confidence_level", 0) > 0.8 else "ä¸­ç­‰",
                "ç‰¹å¾äº’è¡¥æ€§": "å¼º" if len(integration_result.get("complementary_insights", [])) > 3 else "å¼±"
            }

            # å¯æ“ä½œå»ºè®®
            recommendations = []

            # åŸºäºè¯­ä¹‰åˆ†æçš„å»ºè®®
            semantic_summary = semantic_analysis.get("analysis_summary", {})
            semantic_chars = semantic_summary.get("semantic_characteristics", {})

            for char, score in semantic_chars.items():
                if score < 3.0:
                    recommendations.append(f"å»ºè®®æå‡{char}ï¼ˆå½“å‰åˆ†æ•°: {score:.1f}ï¼‰")

            # åŸºäºä¼ ç»Ÿåˆ†æçš„å»ºè®®
            if traditional_features.get("success"):
                recommendations.append("å»ºè®®ä¿æŒå½“å‰çš„æ–‡é£ç‰¹å¾ä¸€è‡´æ€§")

            insights["actionable_recommendations"] = recommendations

            # ç‹¬ç‰¹å‘ç°
            unique_discoveries = []

            # è¯­ä¹‰åˆ†æçš„ç‹¬ç‰¹å‘ç°
            if semantic_profile.get("success"):
                profile_summary = semantic_profile.get("profile_summary", {})
                uniqueness_score = profile_summary.get("uniqueness_score", 0.0)
                if uniqueness_score > 0.7:
                    unique_discoveries.append(f"æ–‡é£ç‹¬ç‰¹æ€§è¾ƒé«˜ï¼ˆç‹¬ç‰¹æ€§åˆ†æ•°: {uniqueness_score:.2f}ï¼‰")

            insights["unique_discoveries"] = unique_discoveries

        except Exception as e:
            insights["error"] = str(e)

        return insights

    def batch_process_documents(self, documents: List[Dict[str, str]],
                              processing_type: str = "extract") -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« {"text": "...", "name": "..."}
            processing_type: å¤„ç†ç±»å‹ ("extract", "compare", "align")
        """
        print(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£ï¼Œå¤„ç†ç±»å‹: {processing_type}")
        
        result = {
            "batch_id": self._generate_processing_id(),
            "batch_time": datetime.now().isoformat(),
            "processing_type": processing_type,
            "total_documents": len(documents),
            "successful_processes": 0,
            "failed_processes": 0,
            "processing_results": [],
            "batch_summary": {}
        }
        
        try:
            for i, doc in enumerate(documents):
                print(f"å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}: {doc.get('name', f'æ–‡æ¡£{i+1}')}")
                
                try:
                    if processing_type == "extract":
                        process_result = self.extract_comprehensive_style_features(
                            doc["text"], doc.get("name", f"æ–‡æ¡£{i+1}")
                        )
                    else:
                        # å…¶ä»–å¤„ç†ç±»å‹çš„å®ç°
                        process_result = {"error": f"Unsupported processing type: {processing_type}"}
                    
                    if process_result.get("success"):
                        result["successful_processes"] += 1
                    else:
                        result["failed_processes"] += 1
                    
                    process_result["batch_index"] = i
                    result["processing_results"].append(process_result)
                    
                except Exception as e:
                    result["failed_processes"] += 1
                    result["processing_results"].append({
                        "batch_index": i,
                        "document_name": doc.get("name", f"æ–‡æ¡£{i+1}"),
                        "success": False,
                        "error": str(e)
                    })
            
            # ç”Ÿæˆæ‰¹é‡æ‘˜è¦
            result["batch_summary"] = {
                "success_rate": result["successful_processes"] / result["total_documents"] if result["total_documents"] > 0 else 0,
                "processing_time": datetime.now().isoformat(),
                "average_processing_time": "æœªè®¡ç®—"  # å¯ä»¥æ·»åŠ æ—¶é—´ç»Ÿè®¡
            }
            
            print("æ‰¹é‡å¤„ç†å®Œæˆ!")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
        
        return result
    
    def _generate_processing_id(self) -> str:
        """ç”Ÿæˆå¤„ç†ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        return f"style_proc_{timestamp}"
    
    def _generate_processing_summary(self, processing_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå¤„ç†æ‘˜è¦"""
        summary = {
            "processing_success": processing_result.get("success", False),
            "features_extracted": 0,
            "analysis_modules_used": [],
            "key_characteristics": []
        }
        
        # ç»Ÿè®¡æå–çš„ç‰¹å¾æ•°é‡
        if processing_result.get("basic_features", {}).get("success"):
            summary["features_extracted"] += len(processing_result["basic_features"].get("feature_vector", []))
            summary["analysis_modules_used"].append("åŸºç¡€ç‰¹å¾æå–")
        
        if processing_result.get("advanced_features"):
            summary["analysis_modules_used"].extend(["LLMç»¼åˆåˆ†æ", "ä¿®è¾åˆ†æ", "æ­£å¼ç¨‹åº¦åˆ†æ"])
        
        # æå–å…³é”®ç‰¹å¾
        llm_features = processing_result.get("basic_features", {}).get("llm_features", {})
        if "overall_style_profile" in llm_features:
            profile = llm_features["overall_style_profile"]
            summary["key_characteristics"] = profile.get("dominant_characteristics", [])
        
        return summary
    
    def _generate_comparison_summary(self, comparison_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯”è¾ƒæ‘˜è¦"""
        summary = {
            "comparison_success": comparison_result.get("success", False),
            "similarity_score": 0.0,
            "main_differences": [],
            "style_distance": "æœªçŸ¥"
        }
        
        # ç›¸ä¼¼åº¦åˆ†æ•°
        similarity_analysis = comparison_result.get("similarity_analysis", {})
        if similarity_analysis.get("success"):
            summary["similarity_score"] = similarity_analysis.get("similarity_score", 0.0)
            
            # é£æ ¼è·ç¦»åˆ†ç±»
            score = summary["similarity_score"]
            if score >= 0.8:
                summary["style_distance"] = "éå¸¸ç›¸ä¼¼"
            elif score >= 0.6:
                summary["style_distance"] = "è¾ƒä¸ºç›¸ä¼¼"
            elif score >= 0.4:
                summary["style_distance"] = "ä¸­ç­‰å·®å¼‚"
            elif score >= 0.2:
                summary["style_distance"] = "è¾ƒå¤§å·®å¼‚"
            else:
                summary["style_distance"] = "æ˜¾è‘—å·®å¼‚"
        
        # ä¸»è¦å·®å¼‚
        difference_analysis = comparison_result.get("difference_analysis", {})
        if difference_analysis.get("success"):
            parsed_comparison = difference_analysis.get("parsed_comparison", {})
            summary_info = parsed_comparison.get("summary", {})
            if "ä¸»è¦å·®å¼‚" in summary_info:
                summary["main_differences"] = [summary_info["ä¸»è¦å·®å¼‚"]]
        
        return summary
    
    def _record_processing_history(self, processing_result: Dict[str, Any]):
        """è®°å½•å¤„ç†å†å²"""
        history_entry = {
            "processing_id": processing_result.get("processing_id"),
            "document_name": processing_result.get("document_name"),
            "processing_time": processing_result.get("processing_time"),
            "success": processing_result.get("success"),
            "text_length": processing_result.get("text_length"),
            "features_count": len(processing_result.get("basic_features", {}).get("feature_vector", []))
        }
        
        self.processing_history.append(history_entry)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.processing_history) > 100:
            self.processing_history = self.processing_history[-100:]
    
    def get_processing_history(self) -> List[Dict[str, Any]]:
        """è·å–å¤„ç†å†å²"""
        return self.processing_history.copy()
    
    def save_processing_result(self, processing_result: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜å¤„ç†ç»“æœ"""
        if not filename:
            processing_id = processing_result.get("processing_id", "unknown")
            filename = f"comprehensive_style_result_{processing_id}.json"
        
        filepath = os.path.join(self.storage_path, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(processing_result, f, ensure_ascii=False, indent=2)
            return filepath
        except Exception as e:
            return f"ä¿å­˜å¤±è´¥: {str(e)}"
