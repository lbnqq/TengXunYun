#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Semantic Space Mapper - æ ¸å¿ƒæ¨¡å—

Author: AI Assistant (Claude)
Created: 2025-01-28
Last Modified: 2025-01-28
Modified By: AI Assistant (Claude)
AI Assisted: æ˜¯ - Claude 3.5 Sonnet
Version: v1.0
License: MIT
"""


import numpy as np
import json
import os
from typing import Dict, Any, List, Tuple, Optional, Union
from datetime import datetime

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("Warning: sentence-transformers not available. Install with: pip install sentence-transformers")

try:
    from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Warning: scikit-learn not available for advanced vector operations")


class SemanticSpaceMapper:
    """è¯­ä¹‰ç©ºé—´æ˜ å°„å™¨ - å°†è¯­ä¹‰å•å…ƒè½¬åŒ–ä¸ºå‘é‡è¡¨ç¤º"""
    
    def __init__(self, model_name: str = "paraphrase-multilingual-mpnet-base-v2", 
                 cache_dir: str = "src/core/knowledge_base/semantic_vectors"):
        """
        åˆå§‹åŒ–è¯­ä¹‰ç©ºé—´æ˜ å°„å™¨
        
        Args:
            model_name: Sentence-BERTæ¨¡å‹åç§°
            cache_dir: å‘é‡ç¼“å­˜ç›®å½•
        """
        self.model_name = model_name
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–Sentence-BERTæ¨¡å‹
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                print(f"ğŸ”„ æ­£åœ¨åŠ è½½Sentence-BERTæ¨¡å‹: {model_name}")

                # å°è¯•ä½¿ç”¨é•œåƒæº
                os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

                # å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                try:
                    self.sentence_model = SentenceTransformer(model_name)
                    self.model_available = True
                    print("âœ… Sentence-BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e1:
                    print(f"âš ï¸ ä¸»æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹: {str(e1)}")
                    try:
                        # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹
                        backup_model = "all-MiniLM-L6-v2"
                        self.sentence_model = SentenceTransformer(backup_model)
                        self.model_available = True
                        print(f"âœ… å¤‡ç”¨æ¨¡å‹åŠ è½½æˆåŠŸ: {backup_model}")
                    except Exception as e2:
                        print(f"âŒ å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {str(e2)}")
                        print("ğŸ’¡ ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡æ¨¡å¼")
                        self.sentence_model = None
                        self.model_available = False
                        self.use_mock_vectors = True

            except Exception as e:
                print(f"âŒ Sentence-BERTåˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.sentence_model = None
                self.model_available = False
                self.use_mock_vectors = True
        else:
            self.sentence_model = None
            self.model_available = False
            self.use_mock_vectors = True
        
        # å‘é‡ç¼“å­˜
        self.vector_cache = {}
        self.load_vector_cache()

        # æ¨¡æ‹Ÿå‘é‡æ¨¡å¼æ ‡å¿—
        if not hasattr(self, 'use_mock_vectors'):
            self.use_mock_vectors = False
    
    def load_vector_cache(self):
        """åŠ è½½å‘é‡ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, "vector_cache.json")
        try:
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    # å°†åˆ—è¡¨è½¬æ¢å›numpyæ•°ç»„
                    for key, value in cache_data.items():
                        if isinstance(value, list):
                            self.vector_cache[key] = np.array(value)
                print(f"âœ… åŠ è½½å‘é‡ç¼“å­˜: {len(self.vector_cache)} ä¸ªå‘é‡")
        except Exception as e:
            print(f"âš ï¸ å‘é‡ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
    
    def save_vector_cache(self):
        """ä¿å­˜å‘é‡ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, "vector_cache.json")
        try:
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            cache_data = {}
            for key, value in self.vector_cache.items():
                if isinstance(value, np.ndarray):
                    cache_data[key] = value.tolist()
                else:
                    cache_data[key] = value
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ä¿å­˜å‘é‡ç¼“å­˜: {len(cache_data)} ä¸ªå‘é‡")
        except Exception as e:
            print(f"âš ï¸ å‘é‡ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    def encode_semantic_units(self, semantic_units: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°†è¯­ä¹‰å•å…ƒç¼–ç ä¸ºå‘é‡

        Args:
            semantic_units: è¯­ä¹‰å•å…ƒè¯†åˆ«ç»“æœ

        Returns:
            å‘é‡åŒ–ç»“æœ
        """
        if not self.model_available and not self.use_mock_vectors:
            return {"error": "Sentence-BERT model not available and mock vectors disabled"}
        
        result = {
            "encoding_time": datetime.now().isoformat(),
            "model_name": self.model_name,
            "concept_vectors": {},
            "entity_vectors": {},
            "phrase_vectors": {},
            "adjective_vectors": {},
            "verb_vectors": {},
            "vector_statistics": {},
            "success": False
        }
        
        try:
            print("ğŸ”„ æ­£åœ¨å°†è¯­ä¹‰å•å…ƒç¼–ç ä¸ºå‘é‡...")
            
            # ç¼–ç æ¦‚å¿µ
            concepts = semantic_units.get("concepts", [])
            if concepts:
                concept_texts = [c.get("text", "") for c in concepts]
                concept_vectors = self._encode_texts_with_cache(concept_texts)
                result["concept_vectors"] = self._create_vector_dict(concepts, concept_vectors, "text")
            
            # ç¼–ç å®ä½“
            entities = semantic_units.get("named_entities", [])
            if entities:
                entity_texts = [e.get("text", "") for e in entities]
                entity_vectors = self._encode_texts_with_cache(entity_texts)
                result["entity_vectors"] = self._create_vector_dict(entities, entity_vectors, "text")
            
            # ç¼–ç å…³é”®çŸ­è¯­
            phrases = semantic_units.get("key_phrases", [])
            if phrases:
                phrase_texts = [p.get("text", "") for p in phrases]
                phrase_vectors = self._encode_texts_with_cache(phrase_texts)
                result["phrase_vectors"] = self._create_vector_dict(phrases, phrase_vectors, "text")
            
            # ç¼–ç å½¢å®¹è¯
            adjectives = semantic_units.get("key_adjectives", [])
            if adjectives:
                adj_texts = [a.get("text", "") for a in adjectives]
                adj_vectors = self._encode_texts_with_cache(adj_texts)
                result["adjective_vectors"] = self._create_vector_dict(adjectives, adj_vectors, "text")
            
            # ç¼–ç åŠ¨è¯
            verbs = semantic_units.get("key_verbs", [])
            if verbs:
                verb_texts = [v.get("text", "") for v in verbs]
                verb_vectors = self._encode_texts_with_cache(verb_texts)
                result["verb_vectors"] = self._create_vector_dict(verbs, verb_vectors, "text")
            
            # è®¡ç®—å‘é‡ç»Ÿè®¡ä¿¡æ¯
            result["vector_statistics"] = self._calculate_vector_statistics(result)
            result["success"] = True
            
            # ä¿å­˜ç¼“å­˜
            self.save_vector_cache()
            
            print("âœ… è¯­ä¹‰å•å…ƒå‘é‡ç¼–ç å®Œæˆ")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰å•å…ƒå‘é‡ç¼–ç å¤±è´¥: {str(e)}")
        
        return result
    
    def _encode_texts_with_cache(self, texts: List[str]) -> List[np.ndarray]:
        """ä½¿ç”¨ç¼“å­˜ç¼–ç æ–‡æœ¬"""
        vectors = []
        texts_to_encode = []
        cache_indices = []
        
        # æ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            if text in self.vector_cache:
                vectors.append(self.vector_cache[text])
            else:
                vectors.append(None)
                texts_to_encode.append(text)
                cache_indices.append(i)
        
        # ç¼–ç æœªç¼“å­˜çš„æ–‡æœ¬
        if texts_to_encode:
            try:
                if self.model_available and self.sentence_model:
                    new_vectors = self.sentence_model.encode(texts_to_encode)
                else:
                    # ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡
                    print("ğŸ’¡ ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡è¿›è¡Œç¼–ç ")
                    new_vectors = self._generate_mock_vectors(texts_to_encode)

                # æ›´æ–°ç¼“å­˜å’Œç»“æœ
                for i, (text, vector) in enumerate(zip(texts_to_encode, new_vectors)):
                    self.vector_cache[text] = vector
                    vectors[cache_indices[i]] = vector

            except Exception as e:
                print(f"âš ï¸ æ–‡æœ¬ç¼–ç å¤±è´¥: {str(e)}")
                # ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡ä½œä¸ºfallback
                new_vectors = self._generate_mock_vectors(texts_to_encode)
                for i, (text, vector) in enumerate(zip(texts_to_encode, new_vectors)):
                    self.vector_cache[text] = vector
                    vectors[cache_indices[i]] = vector
        
        return [v for v in vectors if v is not None]

    def _generate_mock_vectors(self, texts: List[str]) -> List[np.ndarray]:
        """ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡ç”¨äºæµ‹è¯•"""
        import hashlib

        vectors = []
        vector_dim = 384  # ä½¿ç”¨è¾ƒå°çš„ç»´åº¦

        for text in texts:
            # åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆç¡®å®šæ€§çš„å‘é‡
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()

            # å°†å“ˆå¸Œå€¼è½¬æ¢ä¸ºå‘é‡
            vector = np.zeros(vector_dim)
            for i, char in enumerate(text_hash[:vector_dim//16]):
                idx = i * 16
                if idx < vector_dim:
                    vector[idx] = ord(char) / 255.0  # å½’ä¸€åŒ–åˆ°0-1

            # æ·»åŠ ä¸€äº›åŸºäºæ–‡æœ¬é•¿åº¦çš„ç‰¹å¾
            length_feature = min(len(text) / 100.0, 1.0)
            vector[0] = length_feature

            # æ·»åŠ ä¸€äº›éšæœºæ€§ä½†ä¿æŒç¡®å®šæ€§
            np.random.seed(hash(text) % (2**32))
            noise = np.random.normal(0, 0.1, vector_dim)
            vector = vector + noise

            # å½’ä¸€åŒ–å‘é‡
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm

            vectors.append(vector)

        return vectors
    
    def _create_vector_dict(self, items: List[Dict], vectors: List[np.ndarray], text_key: str) -> Dict[str, Any]:
        """åˆ›å»ºå‘é‡å­—å…¸"""
        vector_dict = {}
        
        for item, vector in zip(items, vectors):
            text = item.get(text_key, "")
            if text and vector is not None:
                vector_dict[text] = {
                    "vector": vector.tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
                    "metadata": {k: v for k, v in item.items() if k != text_key},
                    "vector_norm": float(np.linalg.norm(vector)),
                    "vector_dim": len(vector)
                }
        
        return vector_dict
    
    def _calculate_vector_statistics(self, vector_result: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—å‘é‡ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_vectors": 0,
            "vector_dimensions": 0,
            "category_counts": {},
            "average_norms": {},
            "vector_density": 0.0
        }
        
        try:
            all_vectors = []
            
            # æ”¶é›†æ‰€æœ‰å‘é‡
            for category in ["concept_vectors", "entity_vectors", "phrase_vectors", 
                           "adjective_vectors", "verb_vectors"]:
                category_data = vector_result.get(category, {})
                category_vectors = []
                
                for item_name, item_data in category_data.items():
                    if "vector" in item_data:
                        vector = np.array(item_data["vector"])
                        all_vectors.append(vector)
                        category_vectors.append(vector)
                
                stats["category_counts"][category] = len(category_vectors)
                
                if category_vectors:
                    norms = [np.linalg.norm(v) for v in category_vectors]
                    stats["average_norms"][category] = float(np.mean(norms))
            
            # æ•´ä½“ç»Ÿè®¡
            if all_vectors:
                stats["total_vectors"] = len(all_vectors)
                stats["vector_dimensions"] = len(all_vectors[0])
                
                # è®¡ç®—å‘é‡å¯†åº¦ï¼ˆå¹³å‡å‘é‡é—´è·ç¦»ï¼‰
                if len(all_vectors) > 1 and SKLEARN_AVAILABLE:
                    distances = euclidean_distances(all_vectors)
                    # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªèº«è·ç¦»ä¸º0ï¼‰
                    mask = np.ones(distances.shape, dtype=bool)
                    np.fill_diagonal(mask, False)
                    stats["vector_density"] = float(np.mean(distances[mask]))
        
        except Exception as e:
            stats["error"] = str(e)
        
        return stats
    
    def calculate_semantic_similarities(self, vector_result: Dict[str, Any], 
                                      similarity_type: str = "cosine") -> Dict[str, Any]:
        """
        è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        
        Args:
            vector_result: å‘é‡åŒ–ç»“æœ
            similarity_type: ç›¸ä¼¼åº¦ç±»å‹ ("cosine", "euclidean")
        """
        if not SKLEARN_AVAILABLE:
            return {"error": "scikit-learn not available for similarity calculation"}
        
        similarity_result = {
            "calculation_time": datetime.now().isoformat(),
            "similarity_type": similarity_type,
            "concept_similarities": {},
            "cross_category_similarities": {},
            "similarity_statistics": {},
            "success": False
        }
        
        try:
            print(f"ğŸ”„ æ­£åœ¨è®¡ç®—{similarity_type}ç›¸ä¼¼åº¦...")
            
            # æå–æ¦‚å¿µå‘é‡
            concept_vectors = vector_result.get("concept_vectors", {})
            if len(concept_vectors) > 1:
                concept_names = list(concept_vectors.keys())
                concept_matrix = np.array([concept_vectors[name]["vector"] for name in concept_names])
                
                # è®¡ç®—æ¦‚å¿µé—´ç›¸ä¼¼åº¦
                if similarity_type == "cosine":
                    similarities = cosine_similarity(concept_matrix)
                elif similarity_type == "euclidean":
                    distances = euclidean_distances(concept_matrix)
                    # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
                    max_distance = np.max(distances)
                    similarities = 1 - (distances / max_distance) if max_distance > 0 else distances
                
                # æ„å»ºç›¸ä¼¼åº¦å­—å…¸
                concept_sim_dict = {}
                for i, name1 in enumerate(concept_names):
                    concept_sim_dict[name1] = {}
                    for j, name2 in enumerate(concept_names):
                        if i != j:  # æ’é™¤è‡ªèº«
                            concept_sim_dict[name1][name2] = float(similarities[i][j])
                
                similarity_result["concept_similarities"] = concept_sim_dict
            
            # è®¡ç®—è·¨ç±»åˆ«ç›¸ä¼¼åº¦
            cross_similarities = self._calculate_cross_category_similarities(
                vector_result, similarity_type
            )
            similarity_result["cross_category_similarities"] = cross_similarities
            
            # è®¡ç®—ç›¸ä¼¼åº¦ç»Ÿè®¡
            similarity_result["similarity_statistics"] = self._calculate_similarity_statistics(
                similarity_result
            )
            
            similarity_result["success"] = True
            print("âœ… è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")
            
        except Exception as e:
            similarity_result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
        
        return similarity_result
    
    def _calculate_cross_category_similarities(self, vector_result: Dict[str, Any], 
                                             similarity_type: str) -> Dict[str, Any]:
        """è®¡ç®—è·¨ç±»åˆ«ç›¸ä¼¼åº¦"""
        cross_similarities = {}
        
        categories = ["concept_vectors", "entity_vectors", "phrase_vectors"]
        
        for i, cat1 in enumerate(categories):
            for cat2 in categories[i+1:]:
                vectors1 = vector_result.get(cat1, {})
                vectors2 = vector_result.get(cat2, {})
                
                if vectors1 and vectors2:
                    # è®¡ç®—ç±»åˆ«é—´çš„å¹³å‡ç›¸ä¼¼åº¦
                    similarities = []
                    
                    for name1, data1 in vectors1.items():
                        for name2, data2 in vectors2.items():
                            vec1 = np.array(data1["vector"]).reshape(1, -1)
                            vec2 = np.array(data2["vector"]).reshape(1, -1)
                            
                            if similarity_type == "cosine":
                                sim = cosine_similarity(vec1, vec2)[0][0]
                            elif similarity_type == "euclidean":
                                dist = euclidean_distances(vec1, vec2)[0][0]
                                sim = 1 / (1 + dist)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                            
                            similarities.append(sim)
                    
                    if similarities:
                        cross_similarities[f"{cat1}_vs_{cat2}"] = {
                            "average_similarity": float(np.mean(similarities)),
                            "max_similarity": float(np.max(similarities)),
                            "min_similarity": float(np.min(similarities)),
                            "std_similarity": float(np.std(similarities))
                        }
        
        return cross_similarities
    
    def _calculate_similarity_statistics(self, similarity_result: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—ç›¸ä¼¼åº¦ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "concept_similarity_stats": {},
            "cross_category_stats": {},
            "overall_stats": {}
        }
        
        try:
            # æ¦‚å¿µç›¸ä¼¼åº¦ç»Ÿè®¡
            concept_similarities = similarity_result.get("concept_similarities", {})
            if concept_similarities:
                all_concept_sims = []
                for name1, sims in concept_similarities.items():
                    all_concept_sims.extend(sims.values())
                
                if all_concept_sims:
                    stats["concept_similarity_stats"] = {
                        "average": float(np.mean(all_concept_sims)),
                        "max": float(np.max(all_concept_sims)),
                        "min": float(np.min(all_concept_sims)),
                        "std": float(np.std(all_concept_sims))
                    }
            
            # è·¨ç±»åˆ«ç›¸ä¼¼åº¦ç»Ÿè®¡
            cross_similarities = similarity_result.get("cross_category_similarities", {})
            if cross_similarities:
                avg_cross_sims = [data["average_similarity"] for data in cross_similarities.values()]
                if avg_cross_sims:
                    stats["cross_category_stats"] = {
                        "average": float(np.mean(avg_cross_sims)),
                        "max": float(np.max(avg_cross_sims)),
                        "min": float(np.min(avg_cross_sims))
                    }
            
            # æ•´ä½“ç»Ÿè®¡
            all_similarities = []
            if concept_similarities:
                for sims in concept_similarities.values():
                    all_similarities.extend(sims.values())
            if cross_similarities:
                for data in cross_similarities.values():
                    all_similarities.append(data["average_similarity"])
            
            if all_similarities:
                stats["overall_stats"] = {
                    "total_comparisons": len(all_similarities),
                    "average_similarity": float(np.mean(all_similarities)),
                    "similarity_variance": float(np.var(all_similarities))
                }
        
        except Exception as e:
            stats["error"] = str(e)
        
        return stats
    
    def find_semantic_clusters(self, vector_result: Dict[str, Any], 
                             n_clusters: int = None) -> Dict[str, Any]:
        """å‘ç°è¯­ä¹‰èšç±»"""
        if not SKLEARN_AVAILABLE:
            return {"error": "scikit-learn not available for clustering"}
        
        from sklearn.cluster import KMeans
        
        cluster_result = {
            "clustering_time": datetime.now().isoformat(),
            "n_clusters": n_clusters,
            "clusters": {},
            "cluster_statistics": {},
            "success": False
        }
        
        try:
            # æ”¶é›†æ‰€æœ‰æ¦‚å¿µå‘é‡
            concept_vectors = vector_result.get("concept_vectors", {})
            if len(concept_vectors) < 2:
                return {"error": "Not enough concepts for clustering"}
            
            concept_names = list(concept_vectors.keys())
            concept_matrix = np.array([concept_vectors[name]["vector"] for name in concept_names])
            
            # è‡ªåŠ¨ç¡®å®šèšç±»æ•°é‡
            if n_clusters is None:
                n_clusters = min(max(2, len(concept_names) // 3), 5)
            
            # æ‰§è¡ŒK-meansèšç±»
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(concept_matrix)
            
            # ç»„ç»‡èšç±»ç»“æœ
            clusters = {}
            for i, (name, label) in enumerate(zip(concept_names, cluster_labels)):
                cluster_id = f"cluster_{label}"
                if cluster_id not in clusters:
                    clusters[cluster_id] = {
                        "concepts": [],
                        "center": kmeans.cluster_centers_[label].tolist(),
                        "size": 0
                    }
                
                clusters[cluster_id]["concepts"].append({
                    "name": name,
                    "metadata": concept_vectors[name]["metadata"],
                    "distance_to_center": float(np.linalg.norm(
                        concept_matrix[i] - kmeans.cluster_centers_[label]
                    ))
                })
                clusters[cluster_id]["size"] += 1
            
            cluster_result["clusters"] = clusters
            cluster_result["n_clusters"] = n_clusters
            
            # è®¡ç®—èšç±»ç»Ÿè®¡
            cluster_result["cluster_statistics"] = self._calculate_cluster_statistics(
                clusters, kmeans
            )
            
            cluster_result["success"] = True
            print(f"âœ… è¯­ä¹‰èšç±»å®Œæˆï¼Œå‘ç° {n_clusters} ä¸ªèšç±»")
            
        except Exception as e:
            cluster_result["error"] = str(e)
            print(f"âŒ è¯­ä¹‰èšç±»å¤±è´¥: {str(e)}")
        
        return cluster_result
    
    def _calculate_cluster_statistics(self, clusters: Dict[str, Any], kmeans) -> Dict[str, Any]:
        """è®¡ç®—èšç±»ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_clusters": len(clusters),
            "cluster_sizes": {},
            "average_cluster_size": 0,
            "inertia": float(kmeans.inertia_),
            "silhouette_score": None
        }
        
        try:
            # èšç±»å¤§å°ç»Ÿè®¡
            sizes = []
            for cluster_id, cluster_data in clusters.items():
                size = cluster_data["size"]
                stats["cluster_sizes"][cluster_id] = size
                sizes.append(size)
            
            if sizes:
                stats["average_cluster_size"] = float(np.mean(sizes))
            
            # è®¡ç®—è½®å»“ç³»æ•°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                from sklearn.metrics import silhouette_score
                # è¿™é‡Œéœ€è¦åŸå§‹æ•°æ®å’Œæ ‡ç­¾ï¼Œæš‚æ—¶è·³è¿‡
                pass
            except ImportError:
                pass
        
        except Exception as e:
            stats["error"] = str(e)
        
        return stats
