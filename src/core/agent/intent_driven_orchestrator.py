"""
ç«¯åˆ°ç«¯ç”¨æˆ·æ„å›¾é©±åŠ¨çš„æ–‡æ¡£å¤„ç†ç¼–æ’å™¨

åŸºäºç”¨æˆ·ä¸Šä¼ æ–‡æ¡£çš„çŠ¶æ€è‡ªåŠ¨è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„å¤„ç†æµç¨‹ï¼š
1. ç©ºè¡¨æ ¼/æ¨¡æ¿ â†’ æ™ºèƒ½å¡«æŠ¥
2. æ ¼å¼æ··ä¹± â†’ æ ¼å¼æ•´ç†  
3. å†…å®¹ä¸å®Œæ•´ â†’ å†…å®¹è¡¥å…¨
4. AIGCç—•è¿¹æ˜æ˜¾ â†’ é£æ ¼æ”¹å†™
"""

import json
import os
from typing import Dict, Any, List, Tuple
from datetime import datetime
import logging

from ..tools import DocumentParserTool, ComplexDocumentFiller, DocumentFormatExtractor
from ..tools import ContentFillerTool, StyleGeneratorTool, VirtualReviewerTool
from ..guidance import ScenarioInferenceModule


class DocumentRoleAnalyzer:
    """æ–‡æ¡£è§’è‰²åˆ†æå™¨ - æ™ºèƒ½æ¨æ–­æ–‡æ¡£åœ¨å¤„ç†æµç¨‹ä¸­çš„è§’è‰²"""

    def __init__(self, llm_client=None):
        self.llm_client = llm_client

        # æ–‡æ¡£è§’è‰²è¯„ä¼°æŒ‡æ ‡
        self.role_indicators = {
            "format_reference": {
                "structure_completeness": 0.3,  # ç»“æ„å®Œæ•´æ€§æƒé‡
                "format_consistency": 0.25,     # æ ¼å¼ä¸€è‡´æ€§æƒé‡
                "creation_time": 0.2,           # åˆ›å»ºæ—¶é—´æƒé‡
                "content_quality": 0.15,        # å†…å®¹è´¨é‡æƒé‡
                "file_naming": 0.1              # æ–‡ä»¶å‘½åæƒé‡
            },
            "style_reference": {
                "writing_quality": 0.35,        # å†™ä½œè´¨é‡æƒé‡
                "style_consistency": 0.25,      # é£æ ¼ä¸€è‡´æ€§æƒé‡
                "content_depth": 0.2,           # å†…å®¹æ·±åº¦æƒé‡
                "language_naturalness": 0.15,   # è¯­è¨€è‡ªç„¶åº¦æƒé‡
                "logical_coherence": 0.05       # é€»è¾‘è¿è´¯æ€§æƒé‡
            },
            "target_document": {
                "incompleteness": 0.4,          # ä¸å®Œæ•´ç¨‹åº¦æƒé‡
                "format_issues": 0.3,           # æ ¼å¼é—®é¢˜æƒé‡
                "content_gaps": 0.2,            # å†…å®¹ç¼ºå¤±æƒé‡
                "processing_urgency": 0.1       # å¤„ç†ç´§æ€¥åº¦æƒé‡
            }
        }


class DocumentIntentAnalyzer:
    """æ–‡æ¡£æ„å›¾åˆ†æå™¨ - è¯†åˆ«ç”¨æˆ·çœŸæ­£æƒ³è¦ä»€ä¹ˆ"""

    def __init__(self, llm_client=None):
        self.llm_client = llm_client
        self.role_analyzer = DocumentRoleAnalyzer(llm_client)

        # AIGCæ£€æµ‹å…³é”®è¯å’Œæ¨¡å¼
        self.aigc_indicators = {
            "phrases": [
                "ä½œä¸ºä¸€ä¸ª", "æ€»çš„æ¥è¯´", "ç»¼ä¸Šæ‰€è¿°", "éœ€è¦æ³¨æ„çš„æ˜¯", "å€¼å¾—ä¸€æçš„æ˜¯",
                "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å", "å¦å¤–", "æ­¤å¤–", "å› æ­¤", "æ‰€ä»¥",
                "åœ¨è¿™ç§æƒ…å†µä¸‹", "åŸºäºä»¥ä¸Š", "é€šè¿‡åˆ†æ", "å¯ä»¥çœ‹å‡º"
            ],
            "patterns": [
                r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼Œ,]",  # ç¬¬ä¸€ï¼Œç¬¬äºŒï¼Œ...
                r"[1-9]\.[1-9]\.",  # 1.1. 1.2. æ ¼å¼
                r"ç»¼åˆ[è€ƒè™‘åˆ†æ]",
                r"é€šè¿‡[ä»¥ä¸Šä¸Šè¿°]",
                r"åŸºäº[ä»¥ä¸Šä¸Šè¿°]"
            ],
            "structure_indicators": [
                "å¼•è¨€", "æ¦‚è¿°", "èƒŒæ™¯", "ç›®æ ‡", "æ–¹æ³•", "ç»“æœ", "ç»“è®º", "å»ºè®®"
            ]
        }
    
    def analyze_multi_document_roles(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†æå¤šä¸ªæ–‡æ¡£çš„è§’è‰²å’Œå¤„ç†æ„å›¾

        Args:
            documents: [{"name": str, "content": str, "metadata": dict}, ...]

        Returns:
            {
                "document_roles": {doc_name: {"role": str, "confidence": float, "evidence": []}},
                "recommended_workflow": {"primary_action": str, "steps": []},
                "default_selections": {"reference_doc": str, "target_docs": []},
                "user_confirmation_needed": bool
            }
        """
        try:
            if not documents:
                return {"error": "æ²¡æœ‰æä¾›æ–‡æ¡£è¿›è¡Œåˆ†æ"}

            # 1. åˆ†ææ¯ä¸ªæ–‡æ¡£çš„ç‰¹å¾
            document_analyses = {}
            for doc in documents:
                analysis = self._analyze_single_document_role(
                    doc["content"], doc["name"], doc.get("metadata", {})
                )
                document_analyses[doc["name"]] = analysis

            # 2. ç¡®å®šæ–‡æ¡£è§’è‰²å’Œå…³ç³»
            role_assignments = self._assign_document_roles(document_analyses)

            # 3. æ¨èå¤„ç†å·¥ä½œæµç¨‹
            workflow = self._recommend_processing_workflow(role_assignments)

            # 4. ç”Ÿæˆé»˜è®¤é€‰é¡¹
            defaults = self._generate_default_selections(role_assignments, workflow)

            # 5. åˆ¤æ–­æ˜¯å¦éœ€è¦ç”¨æˆ·ç¡®è®¤
            confirmation_needed = self._assess_confirmation_need(role_assignments, workflow)

            return {
                "document_roles": role_assignments,
                "recommended_workflow": workflow,
                "default_selections": defaults,
                "user_confirmation_needed": confirmation_needed,
                "confidence_summary": self._generate_confidence_summary(role_assignments),
                "analysis_time": datetime.now().isoformat()
            }

        except Exception as e:
            return {"error": f"å¤šæ–‡æ¡£è§’è‰²åˆ†æå¤±è´¥: {str(e)}"}

    def _analyze_single_document_role(self, content: str, name: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡æ¡£çš„è§’è‰²ç‰¹å¾"""
        import os
        from datetime import datetime

        # åŸºç¡€åˆ†æ
        basic_analysis = self._analyze_document_basics(content)

        # æ–‡ä»¶å…ƒæ•°æ®åˆ†æ
        file_analysis = {
            "creation_time": metadata.get("creation_time"),
            "file_size": metadata.get("file_size", len(content)),
            "file_extension": os.path.splitext(name)[1].lower(),
            "naming_pattern": self._analyze_file_naming(name)
        }

        # å†…å®¹è´¨é‡åˆ†æ
        quality_analysis = self._analyze_content_quality(content)

        # æ ¼å¼è§„èŒƒæ€§åˆ†æ
        format_analysis = self._analyze_format_quality(content)

        # è§’è‰²å¾—åˆ†è®¡ç®—
        role_scores = self._calculate_role_scores(
            basic_analysis, file_analysis, quality_analysis, format_analysis
        )

        # ç¡®å®šæœ€å¯èƒ½çš„è§’è‰²
        primary_role = max(role_scores.items(), key=lambda x: x[1]["score"])

        return {
            "primary_role": primary_role[0],
            "role_confidence": primary_role[1]["score"],
            "role_evidence": primary_role[1]["evidence"],
            "all_role_scores": role_scores,
            "basic_analysis": basic_analysis,
            "quality_analysis": quality_analysis,
            "format_analysis": format_analysis,
            "file_analysis": file_analysis
        }

    def _analyze_file_naming(self, filename: str) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶å‘½åæ¨¡å¼"""
        import re

        naming_analysis = {
            "has_template_keywords": False,
            "has_version_info": False,
            "has_date_info": False,
            "formality_level": "neutral",
            "naming_score": 0.5
        }

        filename_lower = filename.lower()

        # æ¨¡æ¿å…³é”®è¯æ£€æµ‹
        template_keywords = ["æ¨¡æ¿", "template", "æ ¼å¼", "format", "æ ·æœ¬", "sample", "èŒƒä¾‹", "example"]
        if any(keyword in filename_lower for keyword in template_keywords):
            naming_analysis["has_template_keywords"] = True
            naming_analysis["naming_score"] += 0.3

        # ç‰ˆæœ¬ä¿¡æ¯æ£€æµ‹
        version_patterns = [r"v\d+", r"ç‰ˆæœ¬\d+", r"_\d+\.\d+", r"final", r"æœ€ç»ˆ"]
        if any(re.search(pattern, filename_lower) for pattern in version_patterns):
            naming_analysis["has_version_info"] = True
            naming_analysis["naming_score"] += 0.2

        # æ—¥æœŸä¿¡æ¯æ£€æµ‹
        date_patterns = [r"\d{4}[-_]\d{2}[-_]\d{2}", r"\d{4}å¹´\d{1,2}æœˆ", r"\d{8}"]
        if any(re.search(pattern, filename) for pattern in date_patterns):
            naming_analysis["has_date_info"] = True
            naming_analysis["naming_score"] += 0.1

        return naming_analysis

    def analyze_document_intent(self, document_content: str, document_name: str = None) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£æ„å›¾
        
        Returns:
            {
                "primary_intent": "fill_form|format_cleanup|content_completion|style_rewrite",
                "confidence": 0.0-1.0,
                "evidence": [],
                "secondary_intents": [],
                "processing_priority": "high|medium|low",
                "recommended_actions": []
            }
        """
        try:
            # 1. åŸºç¡€æ–‡æ¡£åˆ†æ
            basic_analysis = self._analyze_document_basics(document_content)
            
            # 2. æ„å›¾æ£€æµ‹
            intent_scores = self._calculate_intent_scores(document_content, basic_analysis)
            
            # 3. ç¡®å®šä¸»è¦æ„å›¾
            primary_intent = max(intent_scores.items(), key=lambda x: x[1]["score"])
            
            # 4. ç”Ÿæˆå¤„ç†å»ºè®®
            recommendations = self._generate_processing_recommendations(
                primary_intent[0], intent_scores, basic_analysis
            )
            
            return {
                "primary_intent": primary_intent[0],
                "confidence": primary_intent[1]["score"],
                "evidence": primary_intent[1]["evidence"],
                "secondary_intents": self._get_secondary_intents(intent_scores, primary_intent[0]),
                "processing_priority": self._determine_priority(primary_intent[1]["score"]),
                "recommended_actions": recommendations,
                "analysis_metadata": {
                    "document_name": document_name,
                    "analysis_time": datetime.now().isoformat(),
                    "basic_stats": basic_analysis,
                    "all_intent_scores": intent_scores
                }
            }
            
        except Exception as e:
            return {"error": f"æ„å›¾åˆ†æå¤±è´¥: {str(e)}"}
    
    def _analyze_document_basics(self, content: str) -> Dict[str, Any]:
        """åŸºç¡€æ–‡æ¡£åˆ†æ"""
        if not content or not content.strip():
            return {
                "is_empty": True,
                "word_count": 0,
                "line_count": 0,
                "has_structure": False,
                "has_tables": False,
                "has_forms": False
            }
        
        lines = content.split('\n')
        words = content.split()
        
        # æ£€æµ‹è¡¨æ ¼å’Œè¡¨å•
        has_tables = any('|' in line or '\t' in line for line in lines)
        has_forms = any(any(indicator in line for indicator in ['___', '____', 'ï¼š', ':', 'â–¡', 'â˜']) for line in lines)
        
        # æ£€æµ‹ç»“æ„åŒ–å†…å®¹
        structure_indicators = ['#', 'ä¸€ã€', 'äºŒã€', '1.', '2.', 'ï¼ˆä¸€ï¼‰', 'ï¼ˆäºŒï¼‰']
        has_structure = any(any(indicator in line for indicator in structure_indicators) for line in lines)
        
        return {
            "is_empty": len(content.strip()) == 0,
            "word_count": len(words),
            "line_count": len([line for line in lines if line.strip()]),
            "has_structure": has_structure,
            "has_tables": has_tables,
            "has_forms": has_forms,
            "avg_line_length": sum(len(line) for line in lines) / max(len(lines), 1),
            "empty_line_ratio": len([line for line in lines if not line.strip()]) / max(len(lines), 1)
        }
    
    def _calculate_intent_scores(self, content: str, basic_analysis: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """è®¡ç®—å„ç§æ„å›¾çš„å¾—åˆ†"""
        scores = {
            "fill_form": {"score": 0.0, "evidence": []},
            "format_cleanup": {"score": 0.0, "evidence": []},
            "content_completion": {"score": 0.0, "evidence": []},
            "style_rewrite": {"score": 0.0, "evidence": []}
        }
        
        # 1. æ™ºèƒ½å¡«æŠ¥æ„å›¾æ£€æµ‹
        if basic_analysis["is_empty"] or basic_analysis["word_count"] < 50:
            scores["fill_form"]["score"] += 0.8
            scores["fill_form"]["evidence"].append("æ–‡æ¡£å†…å®¹æå°‘æˆ–ä¸ºç©º")
        
        if basic_analysis["has_forms"]:
            scores["fill_form"]["score"] += 0.6
            scores["fill_form"]["evidence"].append("æ£€æµ‹åˆ°è¡¨å•å…ƒç´ ")
        
        if basic_analysis["has_tables"]:
            scores["fill_form"]["score"] += 0.4
            scores["fill_form"]["evidence"].append("æ£€æµ‹åˆ°è¡¨æ ¼ç»“æ„")
        
        # 2. æ ¼å¼æ•´ç†æ„å›¾æ£€æµ‹
        if basic_analysis["empty_line_ratio"] > 0.3:
            scores["format_cleanup"]["score"] += 0.5
            scores["format_cleanup"]["evidence"].append("ç©ºè¡Œæ¯”ä¾‹è¿‡é«˜")
        
        if basic_analysis["avg_line_length"] < 10 or basic_analysis["avg_line_length"] > 100:
            scores["format_cleanup"]["score"] += 0.4
            scores["format_cleanup"]["evidence"].append("è¡Œé•¿åº¦ä¸è§„èŒƒ")
        
        if not basic_analysis["has_structure"] and basic_analysis["word_count"] > 200:
            scores["format_cleanup"]["score"] += 0.6
            scores["format_cleanup"]["evidence"].append("ç¼ºå°‘ç»“æ„åŒ–æ ¼å¼")
        
        # 3. å†…å®¹è¡¥å…¨æ„å›¾æ£€æµ‹
        if 50 < basic_analysis["word_count"] < 300:
            scores["content_completion"]["score"] += 0.5
            scores["content_completion"]["evidence"].append("å†…å®¹é•¿åº¦åçŸ­")
        
        incomplete_indicators = ['å¾…è¡¥å……', 'å¾…å®Œå–„', '...', 'çœç•¥', 'ç­‰ç­‰', 'TODO', 'TBD']
        if any(indicator in content for indicator in incomplete_indicators):
            scores["content_completion"]["score"] += 0.7
            scores["content_completion"]["evidence"].append("æ£€æµ‹åˆ°æœªå®Œæˆæ ‡è®°")
        
        # 4. é£æ ¼æ”¹å†™æ„å›¾æ£€æµ‹ï¼ˆAIGCæ£€æµ‹ï¼‰
        aigc_score = self._detect_aigc_content(content)
        scores["style_rewrite"]["score"] = aigc_score["score"]
        scores["style_rewrite"]["evidence"] = aigc_score["evidence"]
        
        return scores
    
    def _detect_aigc_content(self, content: str) -> Dict[str, Any]:
        """æ£€æµ‹AIGCç”Ÿæˆå†…å®¹"""
        import re
        
        score = 0.0
        evidence = []
        
        # æ£€æµ‹AIGCå¸¸ç”¨çŸ­è¯­
        phrase_count = sum(1 for phrase in self.aigc_indicators["phrases"] if phrase in content)
        if phrase_count >= 3:
            score += 0.4
            evidence.append(f"æ£€æµ‹åˆ°{phrase_count}ä¸ªAIå¸¸ç”¨çŸ­è¯­")
        
        # æ£€æµ‹AIGCå¸¸ç”¨æ¨¡å¼
        pattern_count = 0
        for pattern in self.aigc_indicators["patterns"]:
            if re.search(pattern, content):
                pattern_count += 1
        
        if pattern_count >= 2:
            score += 0.3
            evidence.append(f"æ£€æµ‹åˆ°{pattern_count}ä¸ªAIå†™ä½œæ¨¡å¼")
        
        # æ£€æµ‹è¿‡äºè§„æ•´çš„ç»“æ„
        structure_count = sum(1 for indicator in self.aigc_indicators["structure_indicators"] if indicator in content)
        if structure_count >= 4:
            score += 0.3
            evidence.append("æ£€æµ‹åˆ°è¿‡äºè§„æ•´çš„æ–‡æ¡£ç»“æ„")
        
        # æ£€æµ‹é‡å¤æ€§è¡¨è¾¾
        sentences = content.split('ã€‚')
        if len(sentences) > 5:
            similar_starts = {}
            for sentence in sentences:
                if len(sentence.strip()) > 10:
                    start = sentence.strip()[:3]
                    similar_starts[start] = similar_starts.get(start, 0) + 1
            
            max_similar = max(similar_starts.values()) if similar_starts else 0
            if max_similar >= 3:
                score += 0.2
                evidence.append("æ£€æµ‹åˆ°é‡å¤æ€§å¥å¼ç»“æ„")
        
        return {"score": min(score, 1.0), "evidence": evidence}

    def _analyze_content_quality(self, content: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹è´¨é‡"""
        if not content or not content.strip():
            return {
                "quality_score": 0.0,
                "writing_naturalness": 0.0,
                "logical_coherence": 0.0,
                "content_depth": 0.0,
                "language_quality": 0.0
            }

        # å†™ä½œè‡ªç„¶åº¦è¯„ä¼°
        naturalness_score = self._assess_writing_naturalness(content)

        # é€»è¾‘è¿è´¯æ€§è¯„ä¼°
        coherence_score = self._assess_logical_coherence(content)

        # å†…å®¹æ·±åº¦è¯„ä¼°
        depth_score = self._assess_content_depth(content)

        # è¯­è¨€è´¨é‡è¯„ä¼°
        language_score = self._assess_language_quality(content)

        # ç»¼åˆè´¨é‡å¾—åˆ†
        overall_score = (
            naturalness_score * 0.3 +
            coherence_score * 0.25 +
            depth_score * 0.25 +
            language_score * 0.2
        )

        return {
            "quality_score": overall_score,
            "writing_naturalness": naturalness_score,
            "logical_coherence": coherence_score,
            "content_depth": depth_score,
            "language_quality": language_score
        }

    def _assess_writing_naturalness(self, content: str) -> float:
        """è¯„ä¼°å†™ä½œè‡ªç„¶åº¦ï¼ˆæ£€æµ‹AIGCç—•è¿¹ï¼‰"""
        score = 1.0  # ä»æ»¡åˆ†å¼€å§‹ï¼Œå‘ç°AIGCç‰¹å¾åˆ™æ‰£åˆ†

        # æ£€æµ‹AIGCå¸¸ç”¨çŸ­è¯­
        phrase_count = sum(1 for phrase in self.aigc_indicators["phrases"] if phrase in content)
        score -= min(phrase_count * 0.1, 0.4)  # æœ€å¤šæ‰£0.4åˆ†

        # æ£€æµ‹è¿‡äºè§„æ•´çš„ç»“æ„
        structure_count = sum(1 for indicator in self.aigc_indicators["structure_indicators"] if indicator in content)
        if structure_count >= 5:
            score -= 0.3
        elif structure_count >= 3:
            score -= 0.2

        # æ£€æµ‹é‡å¤æ€§è¡¨è¾¾æ¨¡å¼
        import re
        patterns_found = sum(1 for pattern in self.aigc_indicators["patterns"] if re.search(pattern, content))
        score -= min(patterns_found * 0.15, 0.3)

        return max(score, 0.0)

    def _assess_logical_coherence(self, content: str) -> float:
        """è¯„ä¼°é€»è¾‘è¿è´¯æ€§"""
        sentences = content.split('ã€‚')
        if len(sentences) < 3:
            return 0.5  # å†…å®¹å¤ªçŸ­ï¼Œç»™ä¸­ç­‰åˆ†

        # æ£€æµ‹é€»è¾‘è¿æ¥è¯çš„ä½¿ç”¨
        logical_connectors = ["å› æ­¤", "æ‰€ä»¥", "ä½†æ˜¯", "ç„¶è€Œ", "å¦å¤–", "æ­¤å¤–", "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å"]
        connector_count = sum(1 for connector in logical_connectors if connector in content)

        # æ£€æµ‹æ®µè½ç»“æ„
        paragraphs = content.split('\n\n')
        has_clear_structure = len(paragraphs) > 1 and all(len(p.strip()) > 50 for p in paragraphs[:3])

        score = 0.5  # åŸºç¡€åˆ†
        score += min(connector_count * 0.1, 0.3)  # é€»è¾‘è¿æ¥è¯åŠ åˆ†
        if has_clear_structure:
            score += 0.2  # æ¸…æ™°ç»“æ„åŠ åˆ†

        return min(score, 1.0)

    def _assess_content_depth(self, content: str) -> float:
        """è¯„ä¼°å†…å®¹æ·±åº¦"""
        word_count = len(content.split())

        # åŸºäºå­—æ•°çš„åŸºç¡€è¯„åˆ†
        if word_count < 100:
            base_score = 0.2
        elif word_count < 500:
            base_score = 0.5
        elif word_count < 1500:
            base_score = 0.8
        else:
            base_score = 1.0

        # æ£€æµ‹ä¸“ä¸šæœ¯è¯­å’Œå…·ä½“ç»†èŠ‚
        professional_indicators = ["åˆ†æ", "ç ”ç©¶", "æ–¹æ³•", "ç»“æœ", "æ•°æ®", "æŒ‡æ ‡", "è¯„ä¼°", "å»ºè®®"]
        professional_count = sum(1 for indicator in professional_indicators if indicator in content)

        # æ£€æµ‹å…·ä½“æ•°å­—å’Œäº‹å®
        import re
        numbers = len(re.findall(r'\d+(?:\.\d+)?%?', content))

        depth_bonus = min(professional_count * 0.05 + numbers * 0.02, 0.3)

        return min(base_score + depth_bonus, 1.0)

    def _assess_language_quality(self, content: str) -> float:
        """è¯„ä¼°è¯­è¨€è´¨é‡"""
        # æ£€æµ‹è¯­æ³•é”™è¯¯å’Œä¸è§„èŒƒè¡¨è¾¾
        quality_issues = 0

        # æ£€æµ‹å¸¸è§è¯­æ³•é—®é¢˜
        grammar_issues = ["çš„çš„", "äº†äº†", "åœ¨åœ¨", "æ˜¯æ˜¯"]  # é‡å¤è¯
        quality_issues += sum(1 for issue in grammar_issues if issue in content)

        # æ£€æµ‹æ ‡ç‚¹ç¬¦å·ä½¿ç”¨
        import re
        punctuation_density = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', content)) / max(len(content), 1)
        if punctuation_density < 0.02 or punctuation_density > 0.1:
            quality_issues += 1

        # åŸºç¡€åˆ†å‡å»é—®é¢˜åˆ†
        score = 1.0 - min(quality_issues * 0.1, 0.5)

        return max(score, 0.3)  # æœ€ä½0.3åˆ†

    def _analyze_format_quality(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ ¼å¼è´¨é‡"""
        format_analysis = {
            "structure_score": 0.0,
            "consistency_score": 0.0,
            "completeness_score": 0.0,
            "overall_format_score": 0.0
        }

        if not content or not content.strip():
            return format_analysis

        lines = content.split('\n')

        # ç»“æ„åŒ–ç¨‹åº¦è¯„ä¼°
        structure_indicators = ['#', 'ä¸€ã€', 'äºŒã€', '1.', '2.', 'ï¼ˆä¸€ï¼‰', 'ï¼ˆäºŒï¼‰']
        has_structure = any(any(indicator in line for indicator in structure_indicators) for line in lines)
        format_analysis["structure_score"] = 0.8 if has_structure else 0.3

        # æ ¼å¼ä¸€è‡´æ€§è¯„ä¼°
        empty_line_ratio = len([line for line in lines if not line.strip()]) / max(len(lines), 1)
        consistency_score = 0.8 if 0.1 <= empty_line_ratio <= 0.3 else 0.5
        format_analysis["consistency_score"] = consistency_score

        # å®Œæ•´æ€§è¯„ä¼°
        has_title = len(lines) > 0 and len(lines[0].strip()) < 50 and lines[0].strip()
        has_content = len([line for line in lines if line.strip()]) > 3
        completeness_score = 0.5
        if has_title:
            completeness_score += 0.2
        if has_content:
            completeness_score += 0.3
        format_analysis["completeness_score"] = completeness_score

        # ç»¼åˆæ ¼å¼å¾—åˆ†
        format_analysis["overall_format_score"] = (
            format_analysis["structure_score"] * 0.4 +
            format_analysis["consistency_score"] * 0.3 +
            format_analysis["completeness_score"] * 0.3
        )

        return format_analysis
    
    def _get_secondary_intents(self, intent_scores: Dict[str, Dict[str, Any]], primary_intent: str) -> List[Dict[str, Any]]:
        """è·å–æ¬¡è¦æ„å›¾"""
        secondary = []
        for intent, data in intent_scores.items():
            if intent != primary_intent and data["score"] > 0.3:
                secondary.append({
                    "intent": intent,
                    "score": data["score"],
                    "evidence": data["evidence"][:2]  # åªä¿ç•™å‰ä¸¤ä¸ªè¯æ®
                })
        
        return sorted(secondary, key=lambda x: x["score"], reverse=True)[:2]
    
    def _determine_priority(self, confidence: float) -> str:
        """ç¡®å®šå¤„ç†ä¼˜å…ˆçº§"""
        if confidence >= 0.8:
            return "high"
        elif confidence >= 0.5:
            return "medium"
        else:
            return "low"
    
    def _generate_processing_recommendations(self, primary_intent: str, intent_scores: Dict[str, Dict[str, Any]], basic_analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå¤„ç†å»ºè®®"""
        recommendations = []
        
        if primary_intent == "fill_form":
            recommendations.append("å¯åŠ¨æ™ºèƒ½å¡«æŠ¥æµç¨‹ï¼Œè‡ªåŠ¨è¯†åˆ«å¡«å†™é¡¹å¹¶å¼•å¯¼ç”¨æˆ·å®Œæˆ")
            if basic_analysis["has_tables"]:
                recommendations.append("é‡ç‚¹å…³æ³¨è¡¨æ ¼æ•°æ®çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§")
        
        elif primary_intent == "format_cleanup":
            recommendations.append("æ‰§è¡Œæ ¼å¼æ•´ç†ï¼Œç»Ÿä¸€æ–‡æ¡£æ ·å¼å’Œç»“æ„")
            if not basic_analysis["has_structure"]:
                recommendations.append("å»ºè®®æ·»åŠ æ ‡é¢˜å±‚çº§å’Œæ®µè½ç»“æ„")
        
        elif primary_intent == "content_completion":
            recommendations.append("åˆ†æå†…å®¹ç¼ºå¤±ï¼Œæ™ºèƒ½è¡¥å…¨ç›¸å…³ä¿¡æ¯")
            recommendations.append("ä¿æŒåŸæœ‰å†™ä½œé£æ ¼å’Œé€»è¾‘è¿è´¯æ€§")
        
        elif primary_intent == "style_rewrite":
            recommendations.append("æ£€æµ‹åˆ°AIç”Ÿæˆç—•è¿¹ï¼Œå»ºè®®è¿›è¡Œäººæ€§åŒ–æ”¹å†™")
            recommendations.append("é‡ç‚¹ä¼˜åŒ–è¡¨è¾¾æ–¹å¼ï¼Œå¢åŠ è‡ªç„¶æ€§å’Œä¸ªæ€§åŒ–")
        
        # æ·»åŠ æ¬¡è¦æ„å›¾çš„å»ºè®®
        for secondary in self._get_secondary_intents(intent_scores, primary_intent):
            if secondary["score"] > 0.5:
                recommendations.append(f"åŒæ—¶è€ƒè™‘{secondary['intent']}éœ€æ±‚")
        
        return recommendations


class IntentDrivenOrchestrator:
    """æ„å›¾é©±åŠ¨çš„ç«¯åˆ°ç«¯æ–‡æ¡£å¤„ç†ç¼–æ’å™¨"""
    
    def __init__(self, llm_client=None, kb_path: str = "src/core/knowledge_base"):
        self.llm_client = llm_client
        self.intent_analyzer = DocumentIntentAnalyzer(llm_client)
        
        # åˆå§‹åŒ–å·¥å…·
        self.tools = {
            "document_parser": DocumentParserTool(),
            "document_filler": ComplexDocumentFiller(),
            "format_extractor": DocumentFormatExtractor(),
            "content_filler": ContentFillerTool(),
            "style_generator": StyleGeneratorTool(llm_client, os.path.join(kb_path, "style_templates.yaml")),
            "virtual_reviewer": VirtualReviewerTool(llm_client, {})
        }
        
        # å¤„ç†çŠ¶æ€
        self.processing_state = {
            "document_path": None,
            "document_content": None,
            "intent_analysis": None,
            "processing_results": [],
            "final_output": None
        }
        
        self.log_path = os.path.join("logs", "intent_analysis.log")
        os.makedirs(os.path.dirname(self.log_path), exist_ok=True)
        logging.basicConfig(filename=self.log_path, level=logging.INFO, format='%(message)s')
    
    def log_intent_analysis(self, document_name: str, intent_result: dict, user_feedback: dict = None):
        import json
        from datetime import datetime
        log_entry = {
            "document_name": document_name,
            "primary_intent": intent_result.get("primary_intent"),
            "confidence": intent_result.get("confidence"),
            "evidence": intent_result.get("evidence"),
            "analysis_time": intent_result.get("analysis_metadata", {}).get("analysis_time"),
            "user_feedback": user_feedback,
            "log_time": datetime.now().isoformat()
        }
        logging.info(json.dumps(log_entry, ensure_ascii=False))

    def process_document_end_to_end(self, file_path: str, user_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ç«¯åˆ°ç«¯æ–‡æ¡£å¤„ç†ä¸»æµç¨‹
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤„ç†ç»“æœï¼ŒåŒ…å«æœ€ç»ˆè¾“å‡ºå’Œå¤„ç†è¿‡ç¨‹ä¿¡æ¯
        """
        try:
            # 1. æ–‡æ¡£è§£æ
            parse_result = self._parse_document(file_path)
            if "error" in parse_result:
                return parse_result
            
            # 2. æ„å›¾åˆ†æ
            intent_result = self._analyze_user_intent(parse_result["content"], user_context)
            if "error" in intent_result:
                return intent_result
            
            # æ—¥å¿—è®°å½•
            self.log_intent_analysis(os.path.basename(file_path), intent_result)
            
            # 3. è‡ªåŠ¨å¤„ç†æ‰§è¡Œ
            processing_result = self._execute_intent_based_processing(intent_result)
            if "error" in processing_result:
                return processing_result
            
            # 4. ç»“æœæ•´åˆå’Œè¾“å‡º
            final_result = self._generate_final_output(processing_result)
            
            return {
                "success": True,
                "intent_analysis": intent_result,
                "processing_results": processing_result,
                "final_output": final_result,
                "user_message": self._generate_user_message(intent_result, final_result),
                "processing_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {"error": f"ç«¯åˆ°ç«¯å¤„ç†å¤±è´¥: {str(e)}"}
    
    def _parse_document(self, file_path: str) -> Dict[str, Any]:
        """è§£ææ–‡æ¡£"""
        self.processing_state["document_path"] = file_path
        
        parse_result = self.tools["document_parser"].execute(file_path=file_path)
        if "error" in parse_result:
            return parse_result
        
        self.processing_state["document_content"] = parse_result.get("text_content", "")
        return {"content": self.processing_state["document_content"], "structure": parse_result}
    
    def _analyze_user_intent(self, content: str, user_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·æ„å›¾"""
        document_name = os.path.basename(self.processing_state["document_path"]) if self.processing_state["document_path"] else None
        
        intent_result = self.intent_analyzer.analyze_document_intent(content, document_name)
        self.processing_state["intent_analysis"] = intent_result
        
        return intent_result
    
    def _execute_intent_based_processing(self, intent_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºæ„å›¾æ‰§è¡Œå¤„ç†"""
        primary_intent = intent_analysis.get("primary_intent")
        confidence = intent_analysis.get("confidence", 0.0)
        
        if confidence < 0.3:
            return {"error": "æ„å›¾è¯†åˆ«ç½®ä¿¡åº¦è¿‡ä½ï¼Œæ— æ³•è‡ªåŠ¨å¤„ç†"}
        
        processing_results = []
        
        # æ ¹æ®ä¸»è¦æ„å›¾æ‰§è¡Œç›¸åº”å¤„ç†
        if primary_intent == "fill_form":
            result = self._execute_form_filling()
            processing_results.append({"type": "form_filling", "result": result})
        
        elif primary_intent == "format_cleanup":
            result = self._execute_format_cleanup()
            processing_results.append({"type": "format_cleanup", "result": result})
        
        elif primary_intent == "content_completion":
            result = self._execute_content_completion()
            processing_results.append({"type": "content_completion", "result": result})
        
        elif primary_intent == "style_rewrite":
            result = self._execute_style_rewrite()
            processing_results.append({"type": "style_rewrite", "result": result})
        
        # å¤„ç†æ¬¡è¦æ„å›¾
        for secondary in intent_analysis.get("secondary_intents", []):
            if secondary["score"] > 0.5:
                secondary_result = self._execute_secondary_intent(secondary["intent"])
                processing_results.append({"type": f"secondary_{secondary['intent']}", "result": secondary_result})
        
        self.processing_state["processing_results"] = processing_results
        return {"processing_results": processing_results}
    
    def _execute_form_filling(self) -> Dict[str, Any]:
        """æ‰§è¡Œè¡¨å•å¡«å†™"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨æ™ºèƒ½å¡«æŠ¥æµç¨‹
        return {"status": "form_filling_initiated", "message": "æ™ºèƒ½å¡«æŠ¥æµç¨‹å·²å¯åŠ¨"}
    
    def _execute_format_cleanup(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ ¼å¼æ•´ç†"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨æ ¼å¼æ•´ç†æµç¨‹
        return {"status": "format_cleaned", "message": "æ–‡æ¡£æ ¼å¼å·²æ•´ç†"}
    
    def _execute_content_completion(self) -> Dict[str, Any]:
        """æ‰§è¡Œå†…å®¹è¡¥å…¨"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å†…å®¹è¡¥å…¨æµç¨‹
        return {"status": "content_completed", "message": "å†…å®¹å·²æ™ºèƒ½è¡¥å…¨"}
    
    def _execute_style_rewrite(self) -> Dict[str, Any]:
        """æ‰§è¡Œé£æ ¼æ”¹å†™"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨é£æ ¼æ”¹å†™æµç¨‹
        return {"status": "style_rewritten", "message": "æ–‡æ¡£é£æ ¼å·²ä¼˜åŒ–"}
    
    def _execute_secondary_intent(self, intent: str) -> Dict[str, Any]:
        """æ‰§è¡Œæ¬¡è¦æ„å›¾"""
        return {"status": f"{intent}_processed", "message": f"æ¬¡è¦éœ€æ±‚{intent}å·²å¤„ç†"}
    
    def _generate_final_output(self, processing_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆè¾“å‡º"""
        return {
            "output_type": "processed_document",
            "content": self.processing_state["document_content"],  # è¿™é‡Œåº”è¯¥æ˜¯å¤„ç†åçš„å†…å®¹
            "metadata": {
                "original_file": self.processing_state["document_path"],
                "processing_steps": len(processing_result.get("processing_results", [])),
                "completion_time": datetime.now().isoformat()
            }
        }
    
    def _generate_user_message(self, intent_analysis: Dict[str, Any], final_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆç”¨æˆ·å‹å¥½çš„æ¶ˆæ¯"""
        primary_intent = intent_analysis.get("primary_intent")
        confidence = intent_analysis.get("confidence", 0.0)
        
        intent_names = {
            "fill_form": "æ™ºèƒ½å¡«æŠ¥",
            "format_cleanup": "æ ¼å¼æ•´ç†", 
            "content_completion": "å†…å®¹è¡¥å…¨",
            "style_rewrite": "é£æ ¼ä¼˜åŒ–"
        }
        
        intent_name = intent_names.get(primary_intent, "æ–‡æ¡£å¤„ç†")
        
        message = f"âœ… å·²è‡ªåŠ¨è¯†åˆ«æ‚¨çš„éœ€æ±‚ä¸ºï¼š{intent_name}ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.1%}ï¼‰\n"
        message += f"ğŸ“‹ å¤„ç†å®Œæˆï¼Œæ–‡æ¡£å·²æŒ‰ç…§æ‚¨çš„éœ€æ±‚è¿›è¡Œä¼˜åŒ–ã€‚\n"
        
        if intent_analysis.get("secondary_intents"):
            message += f"ğŸ”„ åŒæ—¶å¤„ç†äº†å…¶ä»–ç›¸å…³éœ€æ±‚ï¼Œç¡®ä¿æ–‡æ¡£è´¨é‡ã€‚"
        
        return message
