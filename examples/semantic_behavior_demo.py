"""
è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•æ¼”ç¤ºç¨‹åº
å±•ç¤ºå¦‚ä½•ä½¿ç”¨è®¯é£å¤§æ¨¡å‹ä½œä¸ºè¯­ä¹‰åˆ†æåŠ©æ‰‹å’Œé£æ ¼è¯„ä¼°å‘˜
"""

import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from core.tools.semantic_space_behavior_engine import SemanticSpaceBehaviorEngine
from core.tools.comprehensive_style_processor import ComprehensiveStyleProcessor


class MockXunfeiLLMClient:
    """æ¨¡æ‹Ÿè®¯é£å¤§æ¨¡å‹å®¢æˆ·ç«¯"""
    
    def generate(self, prompt: str) -> str:
        """æ¨¡æ‹ŸLLMå“åº”"""
        if "è¯­ä¹‰å•å…ƒè¯†åˆ«" in prompt or "concepts" in prompt:
            return """
{
  "concepts": [
    {"text": "äººå·¥æ™ºèƒ½", "role": "æ ¸å¿ƒæ¦‚å¿µ", "importance": 5},
    {"text": "æœºå™¨å­¦ä¹ ", "role": "ç›¸å…³æ¦‚å¿µ", "importance": 4},
    {"text": "æ·±åº¦å­¦ä¹ ", "role": "ç›¸å…³æ¦‚å¿µ", "importance": 4},
    {"text": "ç¥ç»ç½‘ç»œ", "role": "ç›¸å…³æ¦‚å¿µ", "importance": 3}
  ],
  "named_entities": [
    {"text": "è°·æ­Œ", "type": "ç»„ç»‡å", "context": "ç§‘æŠ€å…¬å¸"},
    {"text": "OpenAI", "type": "ç»„ç»‡å", "context": "AIç ”ç©¶æœºæ„"},
    {"text": "GPT", "type": "äº§å“å", "context": "è¯­è¨€æ¨¡å‹"}
  ],
  "key_adjectives": [
    {"text": "æ™ºèƒ½", "context": "æ™ºèƒ½ç³»ç»Ÿ", "sentiment_intensity": 4, "sentiment_polarity": "ç§¯æ"},
    {"text": "å…ˆè¿›", "context": "å…ˆè¿›æŠ€æœ¯", "sentiment_intensity": 4, "sentiment_polarity": "ç§¯æ"},
    {"text": "å¤æ‚", "context": "å¤æ‚ç®—æ³•", "sentiment_intensity": 3, "sentiment_polarity": "ä¸­æ€§"}
  ],
  "key_verbs": [
    {"text": "å‘å±•", "context": "æŠ€æœ¯å‘å±•", "action_type": "å˜åŒ–", "intensity": 4},
    {"text": "çªç ´", "context": "æŠ€æœ¯çªç ´", "action_type": "åŠ¨ä½œ", "intensity": 5},
    {"text": "åº”ç”¨", "context": "æŠ€æœ¯åº”ç”¨", "action_type": "åŠ¨ä½œ", "intensity": 3}
  ],
  "key_phrases": [
    {"text": "è‡ªç„¶è¯­è¨€å¤„ç†", "role": "æŠ€æœ¯æœ¯è¯­", "domain": "äººå·¥æ™ºèƒ½"},
    {"text": "è®¡ç®—æœºè§†è§‰", "role": "æŠ€æœ¯æœ¯è¯­", "domain": "äººå·¥æ™ºèƒ½"},
    {"text": "ç®—æ³•ä¼˜åŒ–", "role": "æŠ€æœ¯æœ¯è¯­", "domain": "è®¡ç®—æœºç§‘å­¦"}
  ],
  "semantic_relations": [
    {"entity1": "äººå·¥æ™ºèƒ½", "relation": "åŒ…å«", "entity2": "æœºå™¨å­¦ä¹ ", "strength": 5},
    {"entity1": "æœºå™¨å­¦ä¹ ", "relation": "åŒ…å«", "entity2": "æ·±åº¦å­¦ä¹ ", "strength": 4}
  ]
}
"""
        elif "èšç±»" in prompt and "ä¸»é¢˜" in prompt:
            return """
{
  "cluster_themes": [
    {"cluster_id": "cluster_0", "theme": "AIæ ¸å¿ƒæŠ€æœ¯", "coherence": 5, "explanation": "äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µèšé›†ï¼Œä¸»é¢˜æ˜ç¡®"},
    {"cluster_id": "cluster_1", "theme": "æŠ€æœ¯åº”ç”¨", "coherence": 4, "explanation": "è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰åº”ç”¨é¢†åŸŸæ¦‚å¿µ"}
  ],
  "cluster_relationships": [
    {"cluster1": "cluster_0", "cluster2": "cluster_1", "relationship": "æ”¯æ’‘", "strength": 4}
  ],
  "overall_assessment": {"semantic_organization": 5, "concept_diversity": 4, "thematic_clarity": 5}
}
"""
        elif "åˆ›æ–°åº¦" in prompt or "novelty" in prompt:
            return """
{
  "novelty_assessments": [
    {
      "concept1": "äººå·¥æ™ºèƒ½",
      "concept2": "è‰ºæœ¯åˆ›ä½œ",
      "novelty_score": 4,
      "novelty_type": "å¯Œæœ‰åˆ›æ„çš„è”æƒ³",
      "explanation": "å°†AIæŠ€æœ¯ä¸è‰ºæœ¯åˆ›ä½œç»“åˆæ˜¯å¾ˆæœ‰åˆ›æ„çš„æƒ³æ³•",
      "context_relevance": 4
    }
  ],
  "overall_creativity": {"average_novelty": 4.0, "creative_density": 4, "innovation_style": "æŠ€æœ¯ä¸äººæ–‡çš„åˆ›æ–°ç»“åˆ"}
}
"""
        elif "è¯­ä¹‰è·ç¦»" in prompt:
            return """
{
  "distance_characteristics": {
    "semantic_span": "é€‚ä¸­",
    "concept_coherence": 4,
    "thematic_focus": 4,
    "explanation": "æ¦‚å¿µé—´è¯­ä¹‰è·ç¦»é€‚ä¸­ï¼Œæ—¢æœ‰æ ¸å¿ƒä¸»é¢˜åˆæœ‰é€‚åº¦æ‰©å±•"
  },
  "writing_style_implications": {
    "style_type": "ä¸“ä¸šèšç„¦",
    "cognitive_pattern": "ç³»ç»Ÿæ€§æ€ç»´ï¼Œé€»è¾‘æ¸…æ™°",
    "audience_accessibility": 3
  }
}
"""
        elif "æƒ…æ„Ÿè¯­ä¹‰" in prompt:
            return """
{
  "emotional_patterns": {
    "dominant_emotion": "ç§¯æ",
    "emotional_intensity": 4,
    "emotional_consistency": 4,
    "emotional_sophistication": 3
  },
  "concept_emotional_mapping": [
    {"concept": "äººå·¥æ™ºèƒ½", "emotional_association": "ç§¯ææœŸå¾…", "strength": 4},
    {"concept": "æŠ€æœ¯å‘å±•", "emotional_association": "ä¹è§‚", "strength": 4}
  ],
  "style_characteristics": {
    "emotional_expressiveness": 3,
    "subjective_tendency": 2,
    "persuasive_power": 4
  }
}
"""
        else:
            return "è¯„åˆ†ï¼š4\nç†ç”±ï¼šè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è®¯é£å¤§æ¨¡å‹å“åº”ï¼Œå±•ç¤ºäº†è¯­ä¹‰åˆ†æèƒ½åŠ›ã€‚"


def demonstrate_semantic_unit_identification():
    """æ¼”ç¤ºè¯­ä¹‰å•å…ƒè¯†åˆ«åŠŸèƒ½"""
    print("=" * 60)
    print("é˜¶æ®µä¸€ï¼šè¯­ä¹‰å•å…ƒè¯†åˆ«ä¸è¡¨ç¤º")
    print("=" * 60)
    
    mock_llm = MockXunfeiLLMClient()
    
    # ç¤ºä¾‹æ–‡æœ¬
    text = """
    äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨ç»å†å‰æ‰€æœªæœ‰çš„å‘å±•é˜¶æ®µã€‚è°·æ­Œã€OpenAIç­‰ç§‘æŠ€å·¨å¤´
    åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚è¿™äº›å…ˆè¿›çš„ç¥ç»ç½‘ç»œç®—æ³•
    ä¸ä»…åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¹Ÿå±•ç°äº†
    ä»¤äººæƒŠå¹çš„èƒ½åŠ›ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒAIç³»ç»Ÿæ­£åœ¨å˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½ï¼Œ
    ä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥äº†æ— é™çš„å¯èƒ½æ€§ã€‚
    """
    
    from core.tools.semantic_unit_identifier import SemanticUnitIdentifier
    
    identifier = SemanticUnitIdentifier(mock_llm)
    
    print("ğŸ” æ­£åœ¨è¯†åˆ«è¯­ä¹‰å•å…ƒ...")
    result = identifier.identify_semantic_units(text, "comprehensive")
    
    if result.get("success"):
        print("âœ… è¯­ä¹‰å•å…ƒè¯†åˆ«æˆåŠŸ")
        
        semantic_units = result["semantic_units"]
        
        # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
        concepts = semantic_units.get("concepts", [])
        print(f"\nğŸ“‹ è¯†åˆ«å‡º {len(concepts)} ä¸ªæ¦‚å¿µ:")
        for concept in concepts[:5]:
            print(f"  - {concept.get('text', '')} (é‡è¦æ€§: {concept.get('importance', 0)})")
        
        entities = semantic_units.get("named_entities", [])
        print(f"\nğŸ¢ è¯†åˆ«å‡º {len(entities)} ä¸ªå®ä½“:")
        for entity in entities[:3]:
            print(f"  - {entity.get('text', '')} ({entity.get('type', '')})")
        
        adjectives = semantic_units.get("key_adjectives", [])
        print(f"\nğŸ’­ è¯†åˆ«å‡º {len(adjectives)} ä¸ªå…³é”®å½¢å®¹è¯:")
        for adj in adjectives[:3]:
            print(f"  - {adj.get('text', '')} (æƒ…æ„Ÿ: {adj.get('sentiment_polarity', '')})")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = identifier.get_semantic_unit_statistics(semantic_units)
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ¦‚å¿µæ•°é‡: {stats.get('concept_count', 0)}")
        print(f"  - å®ä½“æ•°é‡: {stats.get('entity_count', 0)}")
        print(f"  - å½¢å®¹è¯æ•°é‡: {stats.get('adjective_count', 0)}")
        
    else:
        print("âŒ è¯­ä¹‰å•å…ƒè¯†åˆ«å¤±è´¥")
        print(f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    return result


def demonstrate_semantic_space_mapping(semantic_units_result):
    """æ¼”ç¤ºè¯­ä¹‰ç©ºé—´æ˜ å°„åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("é˜¶æ®µäºŒï¼šè¯­ä¹‰ç©ºé—´æ˜ å°„")
    print("=" * 60)
    
    if not semantic_units_result.get("success"):
        print("âŒ æ— æ³•è¿›è¡Œè¯­ä¹‰ç©ºé—´æ˜ å°„ï¼Œè¯­ä¹‰å•å…ƒè¯†åˆ«å¤±è´¥")
        return {}
    
    from core.tools.semantic_space_mapper import SemanticSpaceMapper
    
    mapper = SemanticSpaceMapper()
    semantic_units = semantic_units_result["semantic_units"]
    
    print("ğŸ—ºï¸ æ­£åœ¨è¿›è¡Œè¯­ä¹‰ç©ºé—´æ˜ å°„...")
    
    # 1. å‘é‡ç¼–ç 
    print("  ğŸ”„ ç¼–ç è¯­ä¹‰å•å…ƒä¸ºå‘é‡...")
    vector_result = mapper.encode_semantic_units(semantic_units)
    
    if vector_result.get("success"):
        print("  âœ… å‘é‡ç¼–ç å®Œæˆ")
        
        stats = vector_result.get("vector_statistics", {})
        print(f"    - æ€»å‘é‡æ•°: {stats.get('total_vectors', 0)}")
        print(f"    - å‘é‡ç»´åº¦: {stats.get('vector_dimensions', 0)}")
        print(f"    - å‘é‡å¯†åº¦: {stats.get('vector_density', 0):.3f}")
        
        # 2. ç›¸ä¼¼åº¦è®¡ç®—
        print("  ğŸ”„ è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦...")
        similarity_result = mapper.calculate_semantic_similarities(vector_result, "cosine")
        
        if similarity_result.get("success"):
            print("  âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")
            
            sim_stats = similarity_result.get("similarity_statistics", {})
            concept_stats = sim_stats.get("concept_similarity_stats", {})
            if concept_stats:
                print(f"    - å¹³å‡ç›¸ä¼¼åº¦: {concept_stats.get('average', 0):.3f}")
                print(f"    - æœ€å¤§ç›¸ä¼¼åº¦: {concept_stats.get('max', 0):.3f}")
                print(f"    - æœ€å°ç›¸ä¼¼åº¦: {concept_stats.get('min', 0):.3f}")
        
        # 3. èšç±»åˆ†æ
        print("  ğŸ”„ è¿›è¡Œè¯­ä¹‰èšç±»...")
        cluster_result = mapper.find_semantic_clusters(vector_result)
        
        if cluster_result.get("success"):
            print("  âœ… è¯­ä¹‰èšç±»å®Œæˆ")
            
            clusters = cluster_result.get("clusters", {})
            print(f"    - èšç±»æ•°é‡: {len(clusters)}")
            
            for cluster_id, cluster_data in clusters.items():
                concepts = [c["name"] for c in cluster_data["concepts"]]
                print(f"    - {cluster_id}: {', '.join(concepts[:3])}")
        
        return {
            "vector_result": vector_result,
            "similarity_result": similarity_result,
            "cluster_result": cluster_result
        }
    
    else:
        print("âŒ è¯­ä¹‰ç©ºé—´æ˜ å°„å¤±è´¥")
        return {}


def demonstrate_semantic_behavior_analysis(mapping_results, original_text):
    """æ¼”ç¤ºè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ"""
    print("\n" + "=" * 60)
    print("é˜¶æ®µä¸‰ï¼šè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ")
    print("=" * 60)
    
    if not mapping_results:
        print("âŒ æ— æ³•è¿›è¡Œè¡Œä¸ºåˆ†æï¼Œè¯­ä¹‰ç©ºé—´æ˜ å°„å¤±è´¥")
        return {}
    
    from core.tools.semantic_behavior_analyzer import SemanticBehaviorAnalyzer
    
    mock_llm = MockXunfeiLLMClient()
    analyzer = SemanticBehaviorAnalyzer(mock_llm)
    
    vector_result = mapping_results.get("vector_result", {})
    similarity_result = mapping_results.get("similarity_result", {})
    cluster_result = mapping_results.get("cluster_result", {})
    
    behavior_results = {}
    
    # 1. æ¦‚å¿µèšç±»è¡Œä¸ºåˆ†æ
    if cluster_result.get("success"):
        print("ğŸ§  æ­£åœ¨åˆ†ææ¦‚å¿µèšç±»è¡Œä¸º...")
        clustering_analysis = analyzer.analyze_concept_clustering(
            vector_result, cluster_result, original_text
        )
        
        if clustering_analysis.get("success"):
            print("  âœ… æ¦‚å¿µèšç±»è¡Œä¸ºåˆ†æå®Œæˆ")
            
            behavioral_indicators = clustering_analysis.get("behavioral_indicators", {})
            print(f"    - æ¦‚å¿µç»„ç»‡èƒ½åŠ›: {behavioral_indicators.get('conceptual_organization', 'unknown')}")
            print(f"    - ä¸»é¢˜è¿è´¯æ€§: {behavioral_indicators.get('thematic_coherence', 'unknown')}")
            
            behavior_results["clustering_analysis"] = clustering_analysis
    
    # 2. è¯­ä¹‰è·ç¦»æ¨¡å¼åˆ†æ
    if similarity_result.get("success"):
        print("  ğŸ”„ åˆ†æè¯­ä¹‰è·ç¦»æ¨¡å¼...")
        distance_analysis = analyzer.analyze_semantic_distance_patterns(
            vector_result, similarity_result
        )
        
        if distance_analysis.get("success"):
            print("  âœ… è¯­ä¹‰è·ç¦»æ¨¡å¼åˆ†æå®Œæˆ")
            
            pattern_analysis = distance_analysis.get("pattern_analysis", {})
            print(f"    - è¯­ä¹‰è·¨åº¦: {pattern_analysis.get('semantic_span', 'unknown')}")
            print(f"    - æ¦‚å¿µåˆ†å¸ƒ: {pattern_analysis.get('concept_distribution', 'unknown')}")
            
            behavior_results["distance_analysis"] = distance_analysis
    
    # 3. è”æƒ³åˆ›æ–°åº¦è¯„ä¼°
    if similarity_result.get("success"):
        print("  ğŸ”„ è¯„ä¼°è”æƒ³åˆ›æ–°åº¦...")
        novelty_assessment = analyzer.assess_associative_novelty(
            vector_result, similarity_result, original_text
        )
        
        if novelty_assessment.get("success"):
            print("  âœ… è”æƒ³åˆ›æ–°åº¦è¯„ä¼°å®Œæˆ")
            
            creativity_metrics = novelty_assessment.get("creativity_metrics", {})
            print(f"    - å¹³å‡åˆ›æ–°åº¦: {creativity_metrics.get('average_novelty_score', 0):.1f}")
            print(f"    - é«˜åˆ›æ–°åº¦æ•°é‡: {creativity_metrics.get('high_novelty_count', 0)}")
            
            behavior_results["novelty_assessment"] = novelty_assessment
    
    return behavior_results


def demonstrate_style_profile_construction(all_results, document_name):
    """æ¼”ç¤ºé£æ ¼ç”»åƒæ„å»º"""
    print("\n" + "=" * 60)
    print("é˜¶æ®µå››ï¼šç‰¹å¾èåˆä¸é£æ ¼ç”»åƒæ„å»º")
    print("=" * 60)
    
    from core.tools.semantic_style_profiler import SemanticStyleProfiler
    
    profiler = SemanticStyleProfiler()
    
    print("ğŸ¨ æ­£åœ¨æ„å»ºè¯­ä¹‰é£æ ¼ç”»åƒ...")
    
    # æ•´åˆæ‰€æœ‰åˆ†æç»“æœ
    analysis_results = {
        "vector_result": all_results.get("mapping_results", {}).get("vector_result", {}),
        "similarity_result": all_results.get("mapping_results", {}).get("similarity_result", {}),
        "cluster_result": all_results.get("mapping_results", {}).get("cluster_result", {}),
        **all_results.get("behavior_results", {})
    }
    
    profile = profiler.build_semantic_style_profile(analysis_results, document_name)
    
    if profile.get("success"):
        print("âœ… è¯­ä¹‰é£æ ¼ç”»åƒæ„å»ºå®Œæˆ")
        
        # æ˜¾ç¤ºé£æ ¼åˆ†æ•°
        style_scores = profile.get("style_scores", {})
        print("\nğŸ“Š é£æ ¼ç»´åº¦è¯„åˆ†:")
        for dimension, score in style_scores.items():
            dimension_name = profiler.style_dimensions.get(dimension, dimension)
            print(f"  - {dimension_name}: {score:.1f}/5.0")
        
        # æ˜¾ç¤ºé£æ ¼åˆ†ç±»
        classification = profile.get("style_classification", {})
        print(f"\nğŸ·ï¸ é£æ ¼åˆ†ç±»:")
        print(f"  - ä¸»è¦é£æ ¼: {classification.get('primary_style', 'unknown')}")
        print(f"  - é£æ ¼å¼ºåº¦: {classification.get('style_strength', 0):.1f}")
        
        characteristics = classification.get("style_characteristics", [])
        if characteristics:
            print(f"  - é£æ ¼ç‰¹å¾: {', '.join(characteristics)}")
        
        # æ˜¾ç¤ºç”»åƒæ‘˜è¦
        summary = profile.get("profile_summary", {})
        print(f"\nğŸ“‹ ç”»åƒæ‘˜è¦:")
        print(f"  - é£æ ¼ç±»å‹: {summary.get('profile_type', 'unknown')}")
        print(f"  - ç‹¬ç‰¹æ€§åˆ†æ•°: {summary.get('uniqueness_score', 0):.2f}")
        
        strengths = summary.get("key_strengths", [])
        if strengths:
            print(f"  - å…³é”®ä¼˜åŠ¿: {', '.join(strengths)}")
        
        improvements = summary.get("potential_improvements", [])
        if improvements:
            print(f"  - æ”¹è¿›å»ºè®®: {', '.join(improvements)}")
    
    else:
        print("âŒ è¯­ä¹‰é£æ ¼ç”»åƒæ„å»ºå¤±è´¥")
        print(f"é”™è¯¯: {profile.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    return profile


def demonstrate_comprehensive_integration():
    """æ¼”ç¤ºç»¼åˆé›†æˆåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ç»¼åˆé›†æˆæ¼”ç¤ºï¼šè¯­ä¹‰åˆ†æ + ä¼ ç»Ÿåˆ†æ")
    print("=" * 60)
    
    mock_llm = MockXunfeiLLMClient()
    processor = ComprehensiveStyleProcessor(
        llm_client=mock_llm,
        storage_path="demo_comprehensive_storage"
    )
    
    text = """
    åœ¨äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œæˆ‘ä»¬è§è¯äº†æŠ€æœ¯çš„å·¨å¤§å˜é©ã€‚ä»æ—©æœŸçš„
    ä¸“å®¶ç³»ç»Ÿåˆ°ç°åœ¨çš„æ·±åº¦å­¦ä¹ ï¼ŒAIæŠ€æœ¯ç»å†äº†å¤šæ¬¡é‡å¤§çªç ´ã€‚è°·æ­Œçš„
    AlphaGoæˆ˜èƒœå›´æ£‹ä¸–ç•Œå† å†›ï¼ŒOpenAIçš„GPTç³»åˆ—æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†
    æ–¹é¢çš„å“è¶Šè¡¨ç°ï¼Œéƒ½æ ‡å¿—ç€äººå·¥æ™ºèƒ½è¿›å…¥äº†ä¸€ä¸ªå…¨æ–°çš„æ—¶ä»£ã€‚
    
    è¿™äº›æŠ€æœ¯è¿›æ­¥ä¸ä»…æ”¹å˜äº†æˆ‘ä»¬çš„å·¥ä½œæ–¹å¼ï¼Œä¹Ÿæ·±åˆ»å½±å“ç€ç¤¾ä¼šçš„
    å„ä¸ªå±‚é¢ã€‚æ™ºèƒ½æ¨èç³»ç»Ÿè®©æˆ‘ä»¬çš„ç”Ÿæ´»æ›´åŠ ä¾¿åˆ©ï¼Œè‡ªåŠ¨é©¾é©¶æŠ€æœ¯
    æ­£åœ¨é‡å¡‘äº¤é€šè¡Œä¸šï¼Œè€ŒåŒ»ç–—AIåˆ™ä¸ºç–¾ç—…è¯Šæ–­å¸¦æ¥äº†æ–°çš„å¸Œæœ›ã€‚
    
    ç„¶è€Œï¼Œéšç€AIæŠ€æœ¯çš„æ™®åŠï¼Œæˆ‘ä»¬ä¹Ÿé¢ä¸´ç€æ–°çš„æŒ‘æˆ˜ã€‚å¦‚ä½•ç¡®ä¿AI
    ç³»ç»Ÿçš„å…¬å¹³æ€§å’Œé€æ˜åº¦ï¼Ÿå¦‚ä½•å¹³è¡¡æŠ€æœ¯å‘å±•ä¸éšç§ä¿æŠ¤ï¼Ÿè¿™äº›é—®é¢˜
    éœ€è¦æˆ‘ä»¬æ·±å…¥æ€è€ƒï¼Œå¹¶å¯»æ‰¾åˆé€‚çš„è§£å†³æ–¹æ¡ˆã€‚
    """
    
    if processor.semantic_analysis_enabled:
        print("ğŸš€ å¼€å§‹ç»¼åˆåˆ†æ...")
        
        result = processor.analyze_semantic_behavior(
            text, "AIæŠ€æœ¯å‘å±•ç»¼è¿°", "comprehensive"
        )
        
        if result.get("success"):
            print("âœ… ç»¼åˆåˆ†æå®Œæˆ")
            
            # æ˜¾ç¤ºè¯­ä¹‰åˆ†æç»“æœ
            semantic_analysis = result.get("semantic_analysis", {})
            if semantic_analysis.get("success"):
                summary = semantic_analysis.get("analysis_summary", {})
                print(f"\nğŸ§  è¯­ä¹‰åˆ†ææ‘˜è¦:")
                print(f"  - å®Œæˆé˜¶æ®µ: {summary.get('stages_completed', 0)}/4")
                
                findings = summary.get("key_findings", [])
                for finding in findings[:3]:
                    print(f"  - {finding}")
                
                semantic_chars = summary.get("semantic_characteristics", {})
                if semantic_chars:
                    print(f"\nğŸ“Š è¯­ä¹‰ç‰¹å¾:")
                    for char, score in semantic_chars.items():
                        print(f"    - {char}: {score:.1f}")
            
            # æ˜¾ç¤ºç»¼åˆæ´å¯Ÿ
            comprehensive_insights = result.get("comprehensive_insights", {})
            if comprehensive_insights:
                print(f"\nğŸ’¡ ç»¼åˆæ´å¯Ÿ:")
                
                multi_dim = comprehensive_insights.get("multi_dimensional_assessment", {})
                if multi_dim:
                    print("  å¤šç»´åº¦è¯„ä¼°:")
                    for level, assessment in multi_dim.items():
                        print(f"    {level}: {assessment}")
                
                recommendations = comprehensive_insights.get("actionable_recommendations", [])
                if recommendations:
                    print("  å¯æ“ä½œå»ºè®®:")
                    for rec in recommendations[:3]:
                        print(f"    - {rec}")
        
        else:
            print("âŒ ç»¼åˆåˆ†æå¤±è´¥")
            print(f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    else:
        print("âš ï¸ è¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æåŠŸèƒ½æœªå¯ç”¨ï¼Œä»…è¿›è¡Œä¼ ç»Ÿåˆ†æ")
        
        traditional_result = processor.extract_comprehensive_style_features(
            text, "AIæŠ€æœ¯å‘å±•ç»¼è¿°", True
        )
        
        if traditional_result.get("success"):
            print("âœ… ä¼ ç»Ÿåˆ†æå®Œæˆ")
            
            summary = traditional_result.get("processing_summary", {})
            print(f"  - æå–ç‰¹å¾æ•°: {summary.get('features_extracted', 0)}")
            print(f"  - åˆ†ææ¨¡å—: {', '.join(summary.get('analysis_modules_used', []))}")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•æ¼”ç¤º")
    print(f"æ¼”ç¤ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("åŸºäºè®¯é£å¤§æ¨¡å‹çš„è¯­ä¹‰åˆ†æåŠ©æ‰‹å’Œé£æ ¼è¯„ä¼°å‘˜")
    
    # ç¤ºä¾‹æ–‡æœ¬
    demo_text = """
    äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨ç»å†å‰æ‰€æœªæœ‰çš„å‘å±•é˜¶æ®µã€‚è°·æ­Œã€OpenAIç­‰ç§‘æŠ€å·¨å¤´
    åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚è¿™äº›å…ˆè¿›çš„ç¥ç»ç½‘ç»œç®—æ³•
    ä¸ä»…åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¹Ÿå±•ç°äº†
    ä»¤äººæƒŠå¹çš„èƒ½åŠ›ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒAIç³»ç»Ÿæ­£åœ¨å˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½ï¼Œ
    ä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥äº†æ— é™çš„å¯èƒ½æ€§ã€‚
    """
    
    try:
        # é˜¶æ®µä¸€ï¼šè¯­ä¹‰å•å…ƒè¯†åˆ«
        semantic_units_result = demonstrate_semantic_unit_identification()
        
        # é˜¶æ®µäºŒï¼šè¯­ä¹‰ç©ºé—´æ˜ å°„
        mapping_results = demonstrate_semantic_space_mapping(semantic_units_result)
        
        # é˜¶æ®µä¸‰ï¼šè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ
        behavior_results = demonstrate_semantic_behavior_analysis(mapping_results, demo_text)
        
        # é˜¶æ®µå››ï¼šé£æ ¼ç”»åƒæ„å»º
        all_results = {
            "semantic_units_result": semantic_units_result,
            "mapping_results": mapping_results,
            "behavior_results": behavior_results
        }
        
        profile = demonstrate_style_profile_construction(all_results, "AIæŠ€æœ¯æ¼”ç¤ºæ–‡æ¡£")
        
        # ç»¼åˆé›†æˆæ¼”ç¤º
        demonstrate_comprehensive_integration()
        
        print("\n" + "=" * 60)
        print("æ¼”ç¤ºæ€»ç»“")
        print("=" * 60)
        print("âœ… è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•æ¼”ç¤ºå®Œæˆ")
        print("\nğŸ”§ æŠ€æœ¯å®ç°è¦ç‚¹:")
        print("  1. è®¯é£å¤§æ¨¡å‹ä½œä¸ºè¯­ä¹‰åˆ†æåŠ©æ‰‹ï¼Œè¯†åˆ«è¯­ä¹‰å•å…ƒ")
        print("  2. Sentence-BERTä½œä¸ºé‡åŒ–å¼•æ“ï¼Œç”Ÿæˆå‘é‡è¡¨ç¤º")
        print("  3. è®¯é£å¤§æ¨¡å‹ä½œä¸ºé£æ ¼è¯„ä¼°å‘˜ï¼Œæä¾›æ·±åº¦åˆ†æ")
        print("  4. å¤šç»´åº¦ç‰¹å¾èåˆï¼Œæ„å»ºå®Œæ•´é£æ ¼ç”»åƒ")
        
        print("\nğŸ’¡ åº”ç”¨ä»·å€¼:")
        print("  - æ·±åº¦æŒ–æ˜ä¸­æ–‡æ–‡é£çš„ä¸ªæ€§åŒ–ç‰¹å¾")
        print("  - ç»“åˆå®šé‡åˆ†æå’Œå®šæ€§è¯„ä¼°çš„ä¼˜åŠ¿")
        print("  - ä¸ºæ–‡é£åˆ†æå’Œå¯¹é½æä¾›ç§‘å­¦ä¾æ®")
        print("  - æ”¯æŒä½œè€…è¯†åˆ«ã€é£æ ¼åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("è¯·æ£€æŸ¥ä¾èµ–åŒ…å®‰è£…å’Œé…ç½®æ˜¯å¦æ­£ç¡®")


if __name__ == "__main__":
    main()
