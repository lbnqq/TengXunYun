#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯­ä¹‰è¡Œä¸ºæ¼”ç¤º

Author: AI Assistant (Claude)
Created: 2025-01-28
Last Modified: 2025-01-28
Modified By: AI Assistant (Claude)
AI Assisted: æ˜¯ - Claude 3.5 Sonnet
Version: v1.0
License: MIT
"""











import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from core.tools.semantic_space_behavior_engine import SemanticSpaceBehaviorEngine
from core.tools.comprehensive_style_processor import ComprehensiveStyleProcessor


class MockXunfeiLLMClient:
        if "è¯­ä¹‰å•å…ƒè¯†åˆ«" in prompt or "concepts" in prompt:
        elif "èšç±»" in prompt and "ä¸»é¢˜" in prompt:
        elif "åˆ›æ–°åº¦" in prompt or "novelty" in prompt:
        elif "è¯­ä¹‰è·ç¦»" in prompt:
        elif "æƒ…æ„Ÿè¯­ä¹‰" in prompt:
        else:
            return "è¯„åˆ†ï¼š4\nç†ç”±ï¼šè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è®¯é£å¤§æ¨¡å‹å“åº”ï¼Œå±•ç¤ºäº†è¯­ä¹‰åˆ†æèƒ½åŠ›ã€‚"


def demonstrate_semantic_unit_identification():
    äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨ç»å†å‰æ‰€æœªæœ‰çš„å‘å±•é˜¶æ®µã€‚è°·æ­Œã€OpenAIç­‰ç§‘æŠ€å·¨å¤´
    åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚è¿™äº›å…ˆè¿›çš„ç¥ç»ç½‘ç»œç®—æ³•
    ä¸ä»…åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¹Ÿå±•ç°äº†
    ä»¤äººæƒŠå¹çš„èƒ½åŠ›ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒAIç³»ç»Ÿæ­£åœ¨å˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½ï¼Œ
    ä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥äº†æ— é™çš„å¯èƒ½æ€§ã€‚
    print("\n" + "=" * 60)
    print("é˜¶æ®µäºŒï¼šè¯­ä¹‰ç©ºé—´æ˜ å°„")
    print("=" * 60)
    
    if not semantic_units_result.get("success"):
        print("âŒ æ— æ³•è¿›è¡Œè¯­ä¹‰ç©ºé—´æ˜ å°„ï¼Œè¯­ä¹‰å•å…ƒè¯†åˆ«å¤±è´¥")
        return {}
    
    from core.tools.semantic_space_mapper import SemanticSpaceMapper
    
    mapper = SemanticSpaceMapper()
    semantic_units = semantic_units_result["semantic_units"]
    
    print("ğŸ—ºï¸ æ­£åœ¨è¿›è¡Œè¯­ä¹‰ç©ºé—´æ˜ å°„...")
    
    # 1. å‘é‡ç¼–ç 
    print("  ğŸ”„ ç¼–ç è¯­ä¹‰å•å…ƒä¸ºå‘é‡...")
    vector_result = mapper.encode_semantic_units(semantic_units)
    
    if vector_result.get("success"):
        print("  âœ… å‘é‡ç¼–ç å®Œæˆ")
        
        stats = vector_result.get("vector_statistics", {})
        print(f"    - æ€»å‘é‡æ•°: {stats.get('total_vectors', 0)}")
        print(f"    - å‘é‡ç»´åº¦: {stats.get('vector_dimensions', 0)}")
        print(f"    - å‘é‡å¯†åº¦: {stats.get('vector_density', 0):.3f}")
        
        # 2. ç›¸ä¼¼åº¦è®¡ç®—
        print("  ğŸ”„ è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦...")
        similarity_result = mapper.calculate_semantic_similarities(vector_result, "cosine")
        
        if similarity_result.get("success"):
            print("  âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")
            
            sim_stats = similarity_result.get("similarity_statistics", {})
            concept_stats = sim_stats.get("concept_similarity_stats", {})
            if concept_stats:
                print(f"    - å¹³å‡ç›¸ä¼¼åº¦: {concept_stats.get('average', 0):.3f}")
                print(f"    - æœ€å¤§ç›¸ä¼¼åº¦: {concept_stats.get('max', 0):.3f}")
                print(f"    - æœ€å°ç›¸ä¼¼åº¦: {concept_stats.get('min', 0):.3f}")
        
        # 3. èšç±»åˆ†æ
        print("  ğŸ”„ è¿›è¡Œè¯­ä¹‰èšç±»...")
        cluster_result = mapper.find_semantic_clusters(vector_result)
        
        if cluster_result.get("success"):
            print("  âœ… è¯­ä¹‰èšç±»å®Œæˆ")
            
            clusters = cluster_result.get("clusters", {})
            print(f"    - èšç±»æ•°é‡: {len(clusters)}")
            
            for cluster_id, cluster_data in clusters.items():
                concepts = [c["name"] for c in cluster_data["concepts"]]
                print(f"    - {cluster_id}: {', '.join(concepts[:3])}")
        
        return {
            "vector_result": vector_result,
            "similarity_result": similarity_result,
            "cluster_result": cluster_result
        }
    
    else:
        print("âŒ è¯­ä¹‰ç©ºé—´æ˜ å°„å¤±è´¥")
        return {}


def demonstrate_semantic_behavior_analysis(mapping_results, original_text):
    print("\n" + "=" * 60)
    print("é˜¶æ®µå››ï¼šç‰¹å¾èåˆä¸é£æ ¼ç”»åƒæ„å»º")
    print("=" * 60)
    
    from core.tools.semantic_style_profiler import SemanticStyleProfiler
    
    profiler = SemanticStyleProfiler()
    
    print("ğŸ¨ æ­£åœ¨æ„å»ºè¯­ä¹‰é£æ ¼ç”»åƒ...")
    
    # æ•´åˆæ‰€æœ‰åˆ†æç»“æœ
    analysis_results = {
        "vector_result": all_results.get("mapping_results", {}).get("vector_result", {}),
        "similarity_result": all_results.get("mapping_results", {}).get("similarity_result", {}),
        "cluster_result": all_results.get("mapping_results", {}).get("cluster_result", {}),
        **all_results.get("behavior_results", {})
    }
    
    profile = profiler.build_semantic_style_profile(analysis_results, document_name)
    
    if profile.get("success"):
        print("âœ… è¯­ä¹‰é£æ ¼ç”»åƒæ„å»ºå®Œæˆ")
        
        # æ˜¾ç¤ºé£æ ¼åˆ†æ•°
        style_scores = profile.get("style_scores", {})
        print("\nğŸ“Š é£æ ¼ç»´åº¦è¯„åˆ†:")
        for dimension, score in style_scores.items():
            dimension_name = profiler.style_dimensions.get(dimension, dimension)
            print(f"  - {dimension_name}: {score:.1f}/5.0")
        
        # æ˜¾ç¤ºé£æ ¼åˆ†ç±»
        classification = profile.get("style_classification", {})
        print(f"\nğŸ·ï¸ é£æ ¼åˆ†ç±»:")
        print(f"  - ä¸»è¦é£æ ¼: {classification.get('primary_style', 'unknown')}")
        print(f"  - é£æ ¼å¼ºåº¦: {classification.get('style_strength', 0):.1f}")
        
        characteristics = classification.get("style_characteristics", [])
        if characteristics:
            print(f"  - é£æ ¼ç‰¹å¾: {', '.join(characteristics)}")
        
        # æ˜¾ç¤ºç”»åƒæ‘˜è¦
        summary = profile.get("profile_summary", {})
        print(f"\nğŸ“‹ ç”»åƒæ‘˜è¦:")
        print(f"  - é£æ ¼ç±»å‹: {summary.get('profile_type', 'unknown')}")
        print(f"  - ç‹¬ç‰¹æ€§åˆ†æ•°: {summary.get('uniqueness_score', 0):.2f}")
        
        strengths = summary.get("key_strengths", [])
        if strengths:
            print(f"  - å…³é”®ä¼˜åŠ¿: {', '.join(strengths)}")
        
        improvements = summary.get("potential_improvements", [])
        if improvements:
            print(f"  - æ”¹è¿›å»ºè®®: {', '.join(improvements)}")
    
    else:
        print("âŒ è¯­ä¹‰é£æ ¼ç”»åƒæ„å»ºå¤±è´¥")
        print(f"é”™è¯¯: {profile.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    return profile


def demonstrate_comprehensive_integration():
    åœ¨äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œæˆ‘ä»¬è§è¯äº†æŠ€æœ¯çš„å·¨å¤§å˜é©ã€‚ä»æ—©æœŸçš„
    ä¸“å®¶ç³»ç»Ÿåˆ°ç°åœ¨çš„æ·±åº¦å­¦ä¹ ï¼ŒAIæŠ€æœ¯ç»å†äº†å¤šæ¬¡é‡å¤§çªç ´ã€‚è°·æ­Œçš„
    AlphaGoæˆ˜èƒœå›´æ£‹ä¸–ç•Œå† å†›ï¼ŒOpenAIçš„GPTç³»åˆ—æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†
    æ–¹é¢çš„å“è¶Šè¡¨ç°ï¼Œéƒ½æ ‡å¿—ç€äººå·¥æ™ºèƒ½è¿›å…¥äº†ä¸€ä¸ªå…¨æ–°çš„æ—¶ä»£ã€‚
    
    è¿™äº›æŠ€æœ¯è¿›æ­¥ä¸ä»…æ”¹å˜äº†æˆ‘ä»¬çš„å·¥ä½œæ–¹å¼ï¼Œä¹Ÿæ·±åˆ»å½±å“ç€ç¤¾ä¼šçš„
    å„ä¸ªå±‚é¢ã€‚æ™ºèƒ½æ¨èç³»ç»Ÿè®©æˆ‘ä»¬çš„ç”Ÿæ´»æ›´åŠ ä¾¿åˆ©ï¼Œè‡ªåŠ¨é©¾é©¶æŠ€æœ¯
    æ­£åœ¨é‡å¡‘äº¤é€šè¡Œä¸šï¼Œè€ŒåŒ»ç–—AIåˆ™ä¸ºç–¾ç—…è¯Šæ–­å¸¦æ¥äº†æ–°çš„å¸Œæœ›ã€‚
    
    ç„¶è€Œï¼Œéšç€AIæŠ€æœ¯çš„æ™®åŠï¼Œæˆ‘ä»¬ä¹Ÿé¢ä¸´ç€æ–°çš„æŒ‘æˆ˜ã€‚å¦‚ä½•ç¡®ä¿AI
    ç³»ç»Ÿçš„å…¬å¹³æ€§å’Œé€æ˜åº¦ï¼Ÿå¦‚ä½•å¹³è¡¡æŠ€æœ¯å‘å±•ä¸éšç§ä¿æŠ¤ï¼Ÿè¿™äº›é—®é¢˜
    éœ€è¦æˆ‘ä»¬æ·±å…¥æ€è€ƒï¼Œå¹¶å¯»æ‰¾åˆé€‚çš„è§£å†³æ–¹æ¡ˆã€‚
    print("ğŸ¯ è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•æ¼”ç¤º")
    print(f"æ¼”ç¤ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("åŸºäºè®¯é£å¤§æ¨¡å‹çš„è¯­ä¹‰åˆ†æåŠ©æ‰‹å’Œé£æ ¼è¯„ä¼°å‘˜")
    
    # ç¤ºä¾‹æ–‡æœ¬
    
    try:
        # é˜¶æ®µä¸€ï¼šè¯­ä¹‰å•å…ƒè¯†åˆ«
        semantic_units_result = demonstrate_semantic_unit_identification()
        
        # é˜¶æ®µäºŒï¼šè¯­ä¹‰ç©ºé—´æ˜ å°„
        mapping_results = demonstrate_semantic_space_mapping(semantic_units_result)
        
        # é˜¶æ®µä¸‰ï¼šè¯­ä¹‰ç©ºé—´è¡Œä¸ºåˆ†æ
        behavior_results = demonstrate_semantic_behavior_analysis(mapping_results, demo_text)
        
        # é˜¶æ®µå››ï¼šé£æ ¼ç”»åƒæ„å»º
        all_results = {
            "semantic_units_result": semantic_units_result,
            "mapping_results": mapping_results,
            "behavior_results": behavior_results
        }
        
        profile = demonstrate_style_profile_construction(all_results, "AIæŠ€æœ¯æ¼”ç¤ºæ–‡æ¡£")
        
        # ç»¼åˆé›†æˆæ¼”ç¤º
        demonstrate_comprehensive_integration()
        
        print("\n" + "=" * 60)
        print("æ¼”ç¤ºæ€»ç»“")
        print("=" * 60)
        print("âœ… è¯­ä¹‰ç©ºé—´è¡Œä¸ºç®—æ³•æ¼”ç¤ºå®Œæˆ")
        print("\nğŸ”§ æŠ€æœ¯å®ç°è¦ç‚¹:")
        print("  1. è®¯é£å¤§æ¨¡å‹ä½œä¸ºè¯­ä¹‰åˆ†æåŠ©æ‰‹ï¼Œè¯†åˆ«è¯­ä¹‰å•å…ƒ")
        print("  2. Sentence-BERTä½œä¸ºé‡åŒ–å¼•æ“ï¼Œç”Ÿæˆå‘é‡è¡¨ç¤º")
        print("  3. è®¯é£å¤§æ¨¡å‹ä½œä¸ºé£æ ¼è¯„ä¼°å‘˜ï¼Œæä¾›æ·±åº¦åˆ†æ")
        print("  4. å¤šç»´åº¦ç‰¹å¾èåˆï¼Œæ„å»ºå®Œæ•´é£æ ¼ç”»åƒ")
        
        print("\nğŸ’¡ åº”ç”¨ä»·å€¼:")
        print("  - æ·±åº¦æŒ–æ˜ä¸­æ–‡æ–‡é£çš„ä¸ªæ€§åŒ–ç‰¹å¾")
        print("  - ç»“åˆå®šé‡åˆ†æå’Œå®šæ€§è¯„ä¼°çš„ä¼˜åŠ¿")
        print("  - ä¸ºæ–‡é£åˆ†æå’Œå¯¹é½æä¾›ç§‘å­¦ä¾æ®")
        print("  - æ”¯æŒä½œè€…è¯†åˆ«ã€é£æ ¼åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("è¯·æ£€æŸ¥ä¾èµ–åŒ…å®‰è£…å’Œé…ç½®æ˜¯å¦æ­£ç¡®")


if __name__ == "__main__":
    main()
