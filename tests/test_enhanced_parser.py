#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºæ–‡æ¡£è§£æå™¨çš„åŠŸèƒ½
Test script for the Enhanced Document Parser
"""

import json
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.tools.document_parser import EnhancedDocumentParserTool

def test_enhanced_parser():
    """æµ‹è¯•å¢å¼ºæ–‡æ¡£è§£æå™¨çš„å„ç§åŠŸèƒ½"""
    
    print("ğŸš€ æµ‹è¯•å¢å¼ºæ–‡æ¡£è§£æå™¨")
    print("=" * 50)
    
    # åˆå§‹åŒ–è§£æå™¨
    parser = EnhancedDocumentParserTool()
    
    # æµ‹è¯•æ–‡æ¡£è·¯å¾„
    test_doc_path = "test_document.txt"
    
    if not os.path.exists(test_doc_path):
        print(f"âŒ æµ‹è¯•æ–‡æ¡£ {test_doc_path} ä¸å­˜åœ¨")
        return
    
    print(f"ğŸ“„ è§£ææ–‡æ¡£: {test_doc_path}")
    print()
    
    # æµ‹è¯•ä¸åŒæ·±åº¦çš„åˆ†æ
    analysis_depths = ["basic", "standard", "deep"]
    
    for depth in analysis_depths:
        print(f"ğŸ” æ‰§è¡Œ {depth.upper()} çº§åˆ«åˆ†æ...")
        print("-" * 30)
        
        try:
            result = parser.execute(test_doc_path, analysis_depth=depth)
            
            if "error" in result:
                print(f"âŒ é”™è¯¯: {result['error']}")
                continue
            
            print(f"âœ… åˆ†æå®Œæˆ - æ·±åº¦: {result.get('analysis_depth', 'unknown')}")
            
            # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            if "text_content" in result:
                content_length = len(result["text_content"])
                print(f"ğŸ“ æ–‡æ¡£å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
            
            # æ˜¾ç¤ºç»“æ„ä¿¡æ¯
            if "basic_structure" in result:
                structure = result["basic_structure"]
                print(f"ğŸ“Š åŸºæœ¬ç»“æ„:")
                for key, value in structure.items():
                    if isinstance(value, (int, str, bool)):
                        print(f"   - {key}: {value}")
            
            # æ˜¾ç¤ºç»“æ„åŒ–åˆ†æ
            if "structural_analysis" in result:
                struct_analysis = result["structural_analysis"]
                print(f"ğŸ—ï¸  ç»“æ„åŒ–åˆ†æ:")
                print(f"   - æ ‡é¢˜æ•°é‡: {len(struct_analysis.get('headings', []))}")
                print(f"   - æ®µè½æ•°é‡: {len(struct_analysis.get('paragraphs', []))}")
                print(f"   - åˆ—è¡¨é¡¹æ•°é‡: {len(struct_analysis.get('lists', []))}")
                print(f"   - è¡¨æ ¼è¡Œæ•°é‡: {len(struct_analysis.get('tables', []))}")
                
                # æ˜¾ç¤ºæ–‡æ¡£æ ‘ç»“æ„
                if struct_analysis.get("document_tree"):
                    print(f"ğŸŒ³ æ–‡æ¡£æ ‘ç»“æ„:")
                    for i, node in enumerate(struct_analysis["document_tree"][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        print(f"   {i+1}. {node['text']} (çº§åˆ« {node['level']})")
                        if node.get('children'):
                            for child in node['children'][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªå­èŠ‚ç‚¹
                                print(f"      - {child['text']}")
            
            # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
            if "key_information" in result:
                key_info = result["key_information"]
                print(f"ğŸ”‘ å…³é”®ä¿¡æ¯:")
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                if "statistics" in key_info:
                    stats = key_info["statistics"]
                    print(f"   ğŸ“ˆ ç»Ÿè®¡:")
                    print(f"      - æ€»è¯æ•°: {stats.get('total_words', 0)}")
                    print(f"      - å”¯ä¸€è¯æ•°: {stats.get('unique_words', 0)}")
                    print(f"      - æ®µè½æ•°: {stats.get('total_paragraphs', 0)}")
                
                # æ˜¾ç¤ºå…³é”®è¯
                if "keywords" in key_info and key_info["keywords"]:
                    print(f"   ğŸ·ï¸  å…³é”®è¯ (å‰5ä¸ª):")
                    for kw in key_info["keywords"][:5]:
                        print(f"      - {kw['word']} (é¢‘ç‡: {kw['frequency']})")
                
                # æ˜¾ç¤ºå®ä½“
                if "entities" in key_info and key_info["entities"]:
                    print(f"   ğŸ¢ å®ä½“ (å‰5ä¸ª):")
                    for entity in key_info["entities"][:5]:
                        print(f"      - {entity['entity']} (é¢‘ç‡: {entity['frequency']})")
            
            # æ˜¾ç¤ºæ·±åº¦åˆ†æç»“æœ
            if depth == "deep":
                if "content_patterns" in result:
                    patterns = result["content_patterns"]
                    print(f"ğŸ” å†…å®¹æ¨¡å¼:")
                    print(f"   - æœ‰å¼•è¨€: {patterns.get('has_introduction', False)}")
                    print(f"   - æœ‰ç»“è®º: {patterns.get('has_conclusion', False)}")
                    print(f"   - æœ‰æ–¹æ³•è®º: {patterns.get('has_methodology', False)}")
                    print(f"   - é—®é¢˜æ•°é‡: {patterns.get('question_count', 0)}")
                    print(f"   - æ„Ÿå¹å·æ•°é‡: {patterns.get('exclamation_count', 0)}")
                
                if "document_characteristics" in result:
                    chars = result["document_characteristics"]
                    print(f"ğŸ“‹ æ–‡æ¡£ç‰¹å¾:")
                    print(f"   - æ–‡æ¡£é•¿åº¦: {chars.get('document_length', 'unknown')}")
                    print(f"   - ç»“æ„å¤æ‚åº¦: {chars.get('structure_complexity', 'unknown')}")
                    print(f"   - å†…å®¹å¯†åº¦: {chars.get('content_density', 'unknown')}")
                    print(f"   - æ­£å¼ç¨‹åº¦: {chars.get('formality_level', 'unknown')}")
                    print(f"   - æŠ€æœ¯æ°´å¹³: {chars.get('technical_level', 'unknown')}")
                
                if "writing_style" in result:
                    style = result["writing_style"]
                    print(f"âœï¸  å†™ä½œé£æ ¼:")
                    print(f"   - å¹³å‡å¥é•¿: {style.get('average_sentence_length', 0)} è¯")
                    print(f"   - è¯æ±‡å¤æ‚åº¦: {style.get('vocabulary_complexity', 0):.3f}")
                    print(f"   - é—®å¥æ¯”ä¾‹: {style.get('question_ratio', 0):.3f}")
                    
                    if "style_indicators" in style:
                        indicators = style["style_indicators"]
                        print(f"   - é£æ ¼æŒ‡æ ‡:")
                        print(f"     * ç®€æ´: {indicators.get('concise', False)}")
                        print(f"     * å¤æ‚: {indicators.get('complex', False)}")
                        print(f"     * äº’åŠ¨æ€§: {indicators.get('interactive', False)}")
                        print(f"     * å¼ºè°ƒæ€§: {indicators.get('emphatic', False)}")
            
            print()
            
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            print()
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")

def create_sample_document():
    """åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ–‡æ¡£ç”¨äºæµ‹è¯•"""
    sample_content = """# äººå·¥æ™ºèƒ½æ–‡æ¡£å¤„ç†ç³»ç»ŸæŠ€æœ¯æŠ¥å‘Š

## 1. é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨å¼€å‘ä¸€ä¸ªåŸºäºAIçš„æ™ºèƒ½æ–‡æ¡£å¤„ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ†æã€ä¼˜åŒ–å’Œç”ŸæˆåŠå…¬æ–‡æ¡£ã€‚

### 1.1 èƒŒæ™¯

éšç€æ•°å­—åŒ–åŠå…¬çš„æ™®åŠï¼Œæ–‡æ¡£å¤„ç†æ•ˆç‡æˆä¸ºä¼ä¸šå…³æ³¨çš„é‡ç‚¹ã€‚ä¼ ç»Ÿçš„æ–‡æ¡£å¤„ç†æ–¹å¼å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

- äººå·¥å¤„ç†æ•ˆç‡ä½ä¸‹
- æ ¼å¼ä¸ç»Ÿä¸€
- è´¨é‡å‚å·®ä¸é½
- å®¡é˜…æµç¨‹ç¹ç

### 1.2 ç›®æ ‡

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªæ™ºèƒ½åŒ–çš„æ–‡æ¡£å¤„ç†å¹³å°ï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

1. æ·±åº¦æ–‡æ¡£ç†è§£
2. æ™ºèƒ½åœºæ™¯æ¨æ–­
3. è‡ªåŠ¨å†…å®¹ç”Ÿæˆ
4. è™šæ‹Ÿè§’è‰²å®¡ç¨¿

## 2. æŠ€æœ¯æ–¹æ¡ˆ

### 2.1 ç³»ç»Ÿæ¶æ„

ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

- æ–‡æ¡£è§£ææ¨¡å—
- åœºæ™¯æ¨æ–­å¼•æ“
- å†…å®¹ç”Ÿæˆå™¨
- è™šæ‹Ÿå®¡ç¨¿ç³»ç»Ÿ

### 2.2 æ ¸å¿ƒç®—æ³•

æˆ‘ä»¬ä½¿ç”¨äº†ä»¥ä¸‹å…ˆè¿›æŠ€æœ¯ï¼š

* è‡ªç„¶è¯­è¨€å¤„ç† (NLP)
* æœºå™¨å­¦ä¹  (ML)
* æ·±åº¦å­¦ä¹  (DL)
* çŸ¥è¯†å›¾è°± (KG)

## 3. å®éªŒç»“æœ

ç»è¿‡æµ‹è¯•ï¼Œç³»ç»Ÿåœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼š

| æŒ‡æ ‡ | å‡†ç¡®ç‡ | å¬å›ç‡ | F1åˆ†æ•° |
|------|--------|--------|--------|
| æ–‡æ¡£åˆ†ç±» | 95.2% | 93.8% | 94.5% |
| å†…å®¹ç”Ÿæˆ | 89.7% | 91.3% | 90.5% |
| è´¨é‡è¯„ä¼° | 92.1% | 88.9% | 90.5% |

## 4. ç»“è®º

æœ¬ç³»ç»ŸæˆåŠŸå®ç°äº†æ™ºèƒ½åŒ–æ–‡æ¡£å¤„ç†çš„ç›®æ ‡ï¼Œä¸ºä¼ä¸šæ•°å­—åŒ–è½¬å‹æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚

### 4.1 ä¸»è¦è´¡çŒ®

1. æå‡ºäº†åˆ›æ–°çš„æ–‡æ¡£ç†è§£æ¡†æ¶
2. å®ç°äº†é«˜ç²¾åº¦çš„åœºæ™¯æ¨æ–­ç®—æ³•
3. æ„å»ºäº†å¤šè§’è‰²è™šæ‹Ÿå®¡ç¨¿ç³»ç»Ÿ

### 4.2 æœªæ¥å·¥ä½œ

- æ‰©å±•æ”¯æŒæ›´å¤šæ–‡æ¡£æ ¼å¼
- ä¼˜åŒ–ç®—æ³•æ€§èƒ½
- å¢å¼ºç”¨æˆ·ä½“éªŒ

---

**å‚è€ƒæ–‡çŒ®**

1. Smith, J. (2023). "AI in Document Processing". Journal of AI Research.
2. å¼ ä¸‰ (2023). "æ™ºèƒ½æ–‡æ¡£ç³»ç»Ÿè®¾è®¡". è®¡ç®—æœºç§‘å­¦æ‚å¿—.

**è”ç³»æ–¹å¼**: ai-team@company.com
"""
    
    with open("test_document.txt", "w", encoding="utf-8") as f:
        f.write(sample_content)
    
    print("ğŸ“ å·²åˆ›å»ºç¤ºä¾‹æ–‡æ¡£: test_document.txt")

if __name__ == "__main__":
    # å¦‚æœæµ‹è¯•æ–‡æ¡£ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ª
    if not os.path.exists("test_document.txt"):
        create_sample_document()
    
    # è¿è¡Œæµ‹è¯•
    test_enhanced_parser()
