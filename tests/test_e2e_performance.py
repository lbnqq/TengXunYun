#!/usr/bin/env python3
"""
æ€§èƒ½å’Œå‹åŠ›ç«¯åˆ°ç«¯æµ‹è¯•
æµ‹è¯•APIåœ¨ä¸åŒè´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import sys
import os
import time
import json
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, List, Tuple
from test_e2e_framework import E2ETestFramework

class PerformanceTests:
    """æ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, framework: E2ETestFramework):
        self.framework = framework
        self.api_tester = framework.api_tester
    
    def measure_api_response_time(self, tables: List[Dict], fill_data: List[Dict], iterations: int = 10) -> Dict[str, float]:
        """æµ‹é‡APIå“åº”æ—¶é—´"""
        response_times = []
        
        for i in range(iterations):
            start_time = time.time()
            success, result = self.api_tester.test_table_fill_api(tables, fill_data)
            end_time = time.time()
            
            if success:
                response_times.append(end_time - start_time)
            else:
                print(f"   ç¬¬{i+1}æ¬¡è¯·æ±‚å¤±è´¥: {result}")
        
        if not response_times:
            return {"error": "æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥"}
        
        return {
            "min": min(response_times),
            "max": max(response_times),
            "avg": statistics.mean(response_times),
            "median": statistics.median(response_times),
            "std": statistics.stdev(response_times) if len(response_times) > 1 else 0,
            "success_rate": len(response_times) / iterations * 100
        }
    
    def test_small_table_performance(self) -> bool:
        """æµ‹è¯•å°è¡¨æ ¼æ€§èƒ½"""
        print("   æµ‹è¯•å°è¡¨æ ¼æ€§èƒ½ (10è¡Œæ•°æ®)")
        
        tables = [{
            "columns": ["ID", "å§“å", "éƒ¨é—¨"],
            "data": [[f"ID{i:02d}", "", ""] for i in range(10)]
        }]
        
        fill_data = [
            {"ID": f"ID{i:02d}", "å§“å": f"å‘˜å·¥{i:02d}", "éƒ¨é—¨": "æŠ€æœ¯éƒ¨" if i % 2 == 0 else "å¸‚åœºéƒ¨"}
            for i in range(10)
        ]
        
        metrics = self.measure_api_response_time(tables, fill_data, iterations=20)
        
        if "error" in metrics:
            print(f"   å°è¡¨æ ¼æ€§èƒ½æµ‹è¯•å¤±è´¥: {metrics['error']}")
            return False
        
        print(f"   å°è¡¨æ ¼æ€§èƒ½æŒ‡æ ‡:")
        print(f"     å¹³å‡å“åº”æ—¶é—´: {metrics['avg']:.3f}ç§’")
        print(f"     æœ€å°å“åº”æ—¶é—´: {metrics['min']:.3f}ç§’")
        print(f"     æœ€å¤§å“åº”æ—¶é—´: {metrics['max']:.3f}ç§’")
        print(f"     æˆåŠŸç‡: {metrics['success_rate']:.1f}%")
        
        # æ€§èƒ½æ ‡å‡†ï¼šå°è¡¨æ ¼å¹³å‡å“åº”æ—¶é—´åº”å°äº1ç§’
        return metrics['avg'] < 1.0 and metrics['success_rate'] >= 95.0
    
    def test_medium_table_performance(self) -> bool:
        """æµ‹è¯•ä¸­ç­‰è¡¨æ ¼æ€§èƒ½"""
        print("   æµ‹è¯•ä¸­ç­‰è¡¨æ ¼æ€§èƒ½ (100è¡Œæ•°æ®)")
        
        tables = [{
            "columns": ["ID", "å§“å", "éƒ¨é—¨", "èŒä½", "è–ªèµ„"],
            "data": [[f"ID{i:03d}", "", "", "", ""] for i in range(100)]
        }]
        
        fill_data = [
            {
                "ID": f"ID{i:03d}",
                "å§“å": f"å‘˜å·¥{i:03d}",
                "éƒ¨é—¨": "æŠ€æœ¯éƒ¨" if i % 2 == 0 else "å¸‚åœºéƒ¨",
                "èŒä½": "å·¥ç¨‹å¸ˆ" if i % 3 == 0 else "ç»ç†",
                "è–ªèµ„": str(5000 + i * 100)
            }
            for i in range(100)
        ]
        
        metrics = self.measure_api_response_time(tables, fill_data, iterations=10)
        
        if "error" in metrics:
            print(f"   ä¸­ç­‰è¡¨æ ¼æ€§èƒ½æµ‹è¯•å¤±è´¥: {metrics['error']}")
            return False
        
        print(f"   ä¸­ç­‰è¡¨æ ¼æ€§èƒ½æŒ‡æ ‡:")
        print(f"     å¹³å‡å“åº”æ—¶é—´: {metrics['avg']:.3f}ç§’")
        print(f"     æœ€å°å“åº”æ—¶é—´: {metrics['min']:.3f}ç§’")
        print(f"     æœ€å¤§å“åº”æ—¶é—´: {metrics['max']:.3f}ç§’")
        print(f"     æˆåŠŸç‡: {metrics['success_rate']:.1f}%")
        
        # æ€§èƒ½æ ‡å‡†ï¼šä¸­ç­‰è¡¨æ ¼å¹³å‡å“åº”æ—¶é—´åº”å°äº3ç§’
        return metrics['avg'] < 3.0 and metrics['success_rate'] >= 90.0
    
    def test_large_table_performance(self) -> bool:
        """æµ‹è¯•å¤§è¡¨æ ¼æ€§èƒ½"""
        print("   æµ‹è¯•å¤§è¡¨æ ¼æ€§èƒ½ (500è¡Œæ•°æ®)")
        
        tables = [{
            "columns": ["ID", "å§“å", "éƒ¨é—¨", "èŒä½", "è–ªèµ„", "å…¥èŒæ—¥æœŸ"],
            "data": [[f"ID{i:04d}", "", "", "", "", ""] for i in range(500)]
        }]
        
        fill_data = [
            {
                "ID": f"ID{i:04d}",
                "å§“å": f"å‘˜å·¥{i:04d}",
                "éƒ¨é—¨": ["æŠ€æœ¯éƒ¨", "å¸‚åœºéƒ¨", "è´¢åŠ¡éƒ¨", "äººäº‹éƒ¨"][i % 4],
                "èŒä½": ["å·¥ç¨‹å¸ˆ", "ç»ç†", "æ€»ç›‘", "ä¸“å‘˜"][i % 4],
                "è–ªèµ„": str(5000 + i * 50),
                "å…¥èŒæ—¥æœŸ": f"2024-{(i % 12) + 1:02d}-01"
            }
            for i in range(500)
        ]
        
        metrics = self.measure_api_response_time(tables, fill_data, iterations=5)
        
        if "error" in metrics:
            print(f"   å¤§è¡¨æ ¼æ€§èƒ½æµ‹è¯•å¤±è´¥: {metrics['error']}")
            return False
        
        print(f"   å¤§è¡¨æ ¼æ€§èƒ½æŒ‡æ ‡:")
        print(f"     å¹³å‡å“åº”æ—¶é—´: {metrics['avg']:.3f}ç§’")
        print(f"     æœ€å°å“åº”æ—¶é—´: {metrics['min']:.3f}ç§’")
        print(f"     æœ€å¤§å“åº”æ—¶é—´: {metrics['max']:.3f}ç§’")
        print(f"     æˆåŠŸç‡: {metrics['success_rate']:.1f}%")
        
        # æ€§èƒ½æ ‡å‡†ï¼šå¤§è¡¨æ ¼å¹³å‡å“åº”æ—¶é—´åº”å°äº10ç§’
        return metrics['avg'] < 10.0 and metrics['success_rate'] >= 80.0
    
    def test_concurrent_requests(self) -> bool:
        """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
        print("   æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½ (10ä¸ªå¹¶å‘è¯·æ±‚)")
        
        tables = [{
            "columns": ["ID", "æ•°æ®", "æ—¶é—´æˆ³"],
            "data": [["1", "", ""], ["2", "", ""], ["3", "", ""]]
        }]
        
        fill_data = [
            {"ID": "1", "æ•°æ®": "æµ‹è¯•æ•°æ®1", "æ—¶é—´æˆ³": "2024-01-01 10:00:00"},
            {"ID": "2", "æ•°æ®": "æµ‹è¯•æ•°æ®2", "æ—¶é—´æˆ³": "2024-01-01 10:01:00"},
            {"ID": "3", "æ•°æ®": "æµ‹è¯•æ•°æ®3", "æ—¶é—´æˆ³": "2024-01-01 10:02:00"}
        ]
        
        def single_request():
            start_time = time.time()
            success, result = self.api_tester.test_table_fill_api(tables, fill_data)
            end_time = time.time()
            return success, end_time - start_time
        
        # æ‰§è¡Œå¹¶å‘è¯·æ±‚
        concurrent_requests = 10
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
            futures = [executor.submit(single_request) for _ in range(concurrent_requests)]
            results = []
            
            for future in as_completed(futures):
                try:
                    success, response_time = future.result()
                    results.append((success, response_time))
                except Exception as e:
                    results.append((False, 0))
        
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        successful_requests = sum(1 for success, _ in results if success)
        response_times = [rt for success, rt in results if success]
        
        if not response_times:
            print("   å¹¶å‘è¯·æ±‚æµ‹è¯•å¤±è´¥: æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥")
            return False
        
        success_rate = successful_requests / concurrent_requests * 100
        avg_response_time = statistics.mean(response_times)
        throughput = successful_requests / total_time
        
        print(f"   å¹¶å‘è¯·æ±‚æ€§èƒ½æŒ‡æ ‡:")
        print(f"     æˆåŠŸè¯·æ±‚æ•°: {successful_requests}/{concurrent_requests}")
        print(f"     æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"     å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print(f"     æ€»æ‰§è¡Œæ—¶é—´: {total_time:.3f}ç§’")
        print(f"     ååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        
        # æ€§èƒ½æ ‡å‡†ï¼šå¹¶å‘æˆåŠŸç‡åº”å¤§äº80%ï¼Œå¹³å‡å“åº”æ—¶é—´å°äº2ç§’
        return success_rate >= 80.0 and avg_response_time < 2.0
    
    def test_memory_usage(self) -> bool:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("   æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ")
        
        try:
            import psutil
            
            # è·å–å½“å‰è¿›ç¨‹ä¿¡æ¯
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            print(f"   åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.2f} MB")
            
            # æ‰§è¡Œå¤šæ¬¡å¤§æ•°æ®è¯·æ±‚
            large_tables = [{
                "columns": ["ID"] + [f"æ•°æ®{i}" for i in range(10)],
                "data": [[f"ROW{j:04d}"] + [f"æ•°æ®{j}_{i}" for i in range(10)] for j in range(200)]
            }]

            large_fill_data = [
                dict({"ID": f"ROW{j:04d}"}, **{f"æ•°æ®{i}": f"å¡«å……å€¼{j}_{i}" for i in range(10)})
                for j in range(200)
            ]
            
            # æ‰§è¡Œ5æ¬¡å¤§æ•°æ®è¯·æ±‚
            for i in range(5):
                success, result = self.api_tester.test_table_fill_api(large_tables, large_fill_data)
                if not success:
                    print(f"   ç¬¬{i+1}æ¬¡å¤§æ•°æ®è¯·æ±‚å¤±è´¥")
                
                current_memory = process.memory_info().rss / 1024 / 1024
                print(f"   ç¬¬{i+1}æ¬¡è¯·æ±‚åå†…å­˜: {current_memory:.2f} MB")
            
            final_memory = process.memory_info().rss / 1024 / 1024
            memory_increase = final_memory - initial_memory
            
            print(f"   æœ€ç»ˆå†…å­˜ä½¿ç”¨: {final_memory:.2f} MB")
            print(f"   å†…å­˜å¢é•¿: {memory_increase:.2f} MB")
            
            # å†…å­˜æ ‡å‡†ï¼šå†…å­˜å¢é•¿åº”å°äº100MB
            return memory_increase < 100.0
            
        except ImportError:
            print("   psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
            return True
        except Exception as e:
            print(f"   å†…å­˜æµ‹è¯•å¼‚å¸¸: {str(e)}")
            return False

def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æ€§èƒ½å’Œå‹åŠ›ç«¯åˆ°ç«¯æµ‹è¯•")
    
    framework = E2ETestFramework(port=5004)  # ä½¿ç”¨ä¸åŒç«¯å£
    
    try:
        if not framework.setup():
            print("âŒ æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥")
            return False
        
        # åˆ›å»ºæµ‹è¯•å®ä¾‹
        perf_tests = PerformanceTests(framework)
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            ("å°è¡¨æ ¼æ€§èƒ½", perf_tests.test_small_table_performance),
            ("ä¸­ç­‰è¡¨æ ¼æ€§èƒ½", perf_tests.test_medium_table_performance),
            ("å¤§è¡¨æ ¼æ€§èƒ½", perf_tests.test_large_table_performance),
            ("å¹¶å‘è¯·æ±‚æ€§èƒ½", perf_tests.test_concurrent_requests),
            ("å†…å­˜ä½¿ç”¨æµ‹è¯•", perf_tests.test_memory_usage),
        ]
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for test_name, test_func in test_cases:
            framework.run_test(test_name, test_func)
        
        # æ‰“å°æµ‹è¯•æ‘˜è¦
        framework.print_summary()
        
        # è¿”å›æµ‹è¯•ç»“æœ
        report = framework.generate_report()
        return report['summary']['failed'] == 0 and report['summary']['errors'] == 0
        
    finally:
        framework.teardown()

if __name__ == "__main__":
    success = run_performance_tests()
    sys.exit(0 if success else 1)
