#!/usr/bin/env python3
"""
æµ‹è¯•LLMå®¢æˆ·ç«¯åŠŸèƒ½
Test LLM client functionality
"""

import sys
import os
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

def test_multi_llm_client_initialization():
    """æµ‹è¯•å¤šLLMå®¢æˆ·ç«¯åˆå§‹åŒ–"""
    print("ğŸ¤– æµ‹è¯•å¤šLLMå®¢æˆ·ç«¯åˆå§‹åŒ–")
    print("-" * 40)
    
    try:
        from src.llm_clients.multi_llm import MultiLLMClient
        
        # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¸éœ€è¦çœŸå®APIå¯†é’¥ï¼‰
        client = MultiLLMClient()
        
        print("    âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•è·å–å¯ç”¨æ¨¡å‹
        models = client.get_available_models()
        print(f"    - å¯ç”¨æ¨¡å‹æ•°é‡: {len(models)}")
        print(f"    - æ¨¡å‹åˆ—è¡¨: {models[:3]}...")  # æ˜¾ç¤ºå‰3ä¸ª
        
        # æµ‹è¯•åŸºç¡€é…ç½®
        print(f"    - APIç«¯ç‚¹æ•°é‡: {len(client.api_endpoints)}")
        print(f"    - é»˜è®¤æ¨¡å‹é…ç½®: {len(client.default_models)}")
        
        return True
        
    except Exception as e:
        print(f"    âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def test_llm_client_generate():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯ç”ŸæˆåŠŸèƒ½"""
    print("\nğŸ’¬ æµ‹è¯•LLMå®¢æˆ·ç«¯ç”ŸæˆåŠŸèƒ½")
    print("-" * 40)
    
    try:
        from src.llm_clients.multi_llm import MultiLLMClient
        
        client = MultiLLMClient()
        
        # æµ‹è¯•ç®€å•ç”Ÿæˆï¼ˆä¸ä¼šçœŸæ­£è°ƒç”¨APIï¼Œå› ä¸ºæ²¡æœ‰å¯†é’¥ï¼‰
        test_prompt = "è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚"
        
        print("    ğŸ“ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        result = client.generate(test_prompt, model="auto")
        
        print("    âœ… ç”Ÿæˆæ–¹æ³•è°ƒç”¨æˆåŠŸ")
        print(f"    - å“åº”é•¿åº¦: {len(result)} å­—ç¬¦")
        print(f"    - å“åº”é¢„è§ˆ: {result[:100]}...")
        
        # æµ‹è¯•å¸¦é€‰é¡¹çš„ç”Ÿæˆ
        print("    ğŸ”§ æµ‹è¯•å¸¦é€‰é¡¹çš„ç”Ÿæˆ...")
        options = {
            "max_tokens": 500,
            "temperature": 0.7
        }
        result2 = client.generate(test_prompt, model="auto", options=options)
        
        print("    âœ… å¸¦é€‰é¡¹ç”ŸæˆæˆåŠŸ")
        print(f"    - å“åº”é•¿åº¦: {len(result2)} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"    âŒ ç”ŸæˆåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def test_chat_completion():
    """æµ‹è¯•èŠå¤©å®ŒæˆåŠŸèƒ½"""
    print("\nğŸ’­ æµ‹è¯•èŠå¤©å®ŒæˆåŠŸèƒ½")
    print("-" * 40)
    
    try:
        from src.llm_clients.multi_llm import MultiLLMClient
        
        client = MultiLLMClient()
        
        # æµ‹è¯•èŠå¤©æ¶ˆæ¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€‚"}
        ]
        
        print("    ğŸ’¬ æµ‹è¯•èŠå¤©å®Œæˆ...")
        result = client.chat_completion(messages, model="auto")
        
        print("    âœ… èŠå¤©å®Œæˆè°ƒç”¨æˆåŠŸ")
        print(f"    - å“åº”ç»“æ„: {list(result.keys())}")
        
        if 'choices' in result and result['choices']:
            choice = result['choices'][0]
            message = choice.get('message', {})
            content = message.get('content', '')
            
            print(f"    - æ¶ˆæ¯è§’è‰²: {message.get('role', 'unknown')}")
            print(f"    - å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"    - å†…å®¹é¢„è§ˆ: {content[:100]}...")
        
        if 'usage' in result:
            usage = result['usage']
            print(f"    - Tokenä½¿ç”¨: {usage}")
        
        return True
        
    except Exception as e:
        print(f"    âŒ èŠå¤©å®Œæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def test_api_endpoints():
    """æµ‹è¯•APIç«¯ç‚¹é…ç½®"""
    print("\nğŸ”— æµ‹è¯•APIç«¯ç‚¹é…ç½®")
    print("-" * 40)
    
    try:
        from src.llm_clients.multi_llm import MultiLLMClient
        
        client = MultiLLMClient()
        
        print("    ğŸ“Š APIç«¯ç‚¹çŠ¶æ€:")
        for endpoint_name, config in client.api_endpoints.items():
            has_key = bool(config.get('key'))
            url = config.get('url', 'N/A')
            
            print(f"      - {endpoint_name}:")
            print(f"        * å·²é…ç½®å¯†é’¥: {'âœ…' if has_key else 'âŒ'}")
            print(f"        * APIåœ°å€: {url}")
            print(f"        * é»˜è®¤æ¨¡å‹: {client.default_models.get(endpoint_name, 'N/A')}")
        
        # æµ‹è¯•æ¨¡å‹é€‰æ‹©é€»è¾‘
        print("    ğŸ¯ æµ‹è¯•æ¨¡å‹é€‰æ‹©:")
        test_models = ["auto", "qiniu/deepseek-v3", "together/mixtral", "invalid/model"]
        
        for model in test_models:
            try:
                # è¿™é‡Œåªæµ‹è¯•æ¨¡å‹è§£æé€»è¾‘ï¼Œä¸å®é™…è°ƒç”¨API
                print(f"      - æ¨¡å‹ '{model}': å¯è§£æ")
            except Exception as e:
                print(f"      - æ¨¡å‹ '{model}': è§£æå¤±è´¥ - {e}")
        
        return True
        
    except Exception as e:
        print(f"    âŒ APIç«¯ç‚¹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\nâš ï¸ æµ‹è¯•é”™è¯¯å¤„ç†")
    print("-" * 40)
    
    try:
        from src.llm_clients.multi_llm import MultiLLMClient
        
        client = MultiLLMClient()
        
        # æµ‹è¯•æ— æ•ˆè¾“å…¥å¤„ç†
        print("    ğŸš« æµ‹è¯•æ— æ•ˆè¾“å…¥å¤„ç†...")
        
        # ç©ºæ¶ˆæ¯
        result1 = client.generate("", model="auto")
        print(f"      - ç©ºæç¤ºå¤„ç†: {'âœ…' if result1 else 'âŒ'}")
        
        # æ— æ•ˆæ¨¡å‹
        result2 = client.generate("æµ‹è¯•", model="invalid_model")
        print(f"      - æ— æ•ˆæ¨¡å‹å¤„ç†: {'âœ…' if 'Error' in result2 or result2 else 'âŒ'}")
        
        # æµ‹è¯•APIå¯†é’¥ç¼ºå¤±çš„æƒ…å†µ
        print("    ğŸ”‘ æµ‹è¯•APIå¯†é’¥ç¼ºå¤±å¤„ç†...")
        
        # ç”±äºæ²¡æœ‰çœŸå®APIå¯†é’¥ï¼Œæ‰€æœ‰è°ƒç”¨éƒ½åº”è¯¥è¿”å›é”™è¯¯ä¿¡æ¯
        messages = [{"role": "user", "content": "æµ‹è¯•"}]
        result3 = client.chat_completion(messages, model="qiniu/deepseek-v3")
        
        has_error_handling = (
            'Error' in str(result3) or 
            'failed' in str(result3).lower() or
            'not configured' in str(result3).lower()
        )
        print(f"      - APIå¯†é’¥ç¼ºå¤±å¤„ç†: {'âœ…' if has_error_handling else 'âŒ'}")
        
        return True
        
    except Exception as e:
        print(f"    âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def test_base_llm_compatibility():
    """æµ‹è¯•åŸºç¡€LLMå®¢æˆ·ç«¯å…¼å®¹æ€§"""
    print("\nğŸ”„ æµ‹è¯•åŸºç¡€LLMå®¢æˆ·ç«¯å…¼å®¹æ€§")
    print("-" * 40)
    
    try:
        from src.llm_clients.base_llm import BaseLLMClient
        from src.llm_clients.multi_llm import MultiLLMClient
        
        # æ£€æŸ¥ç»§æ‰¿å…³ç³»
        client = MultiLLMClient()
        is_base_instance = isinstance(client, BaseLLMClient)
        
        print(f"    ğŸ—ï¸ ç»§æ‰¿å…³ç³»æ£€æŸ¥: {'âœ…' if is_base_instance else 'âŒ'}")
        
        # æ£€æŸ¥å¿…éœ€æ–¹æ³•
        required_methods = ['generate']
        missing_methods = []
        
        for method in required_methods:
            if not hasattr(client, method):
                missing_methods.append(method)
        
        print(f"    ğŸ“‹ å¿…éœ€æ–¹æ³•æ£€æŸ¥: {'âœ…' if not missing_methods else 'âŒ'}")
        if missing_methods:
            print(f"      ç¼ºå¤±æ–¹æ³•: {missing_methods}")
        
        # æµ‹è¯•æ–¹æ³•è°ƒç”¨
        try:
            result = client.generate("æµ‹è¯•")
            print(f"    ğŸ¯ æ–¹æ³•è°ƒç”¨æµ‹è¯•: {'âœ…' if result is not None else 'âŒ'}")
        except Exception as e:
            print(f"    ğŸ¯ æ–¹æ³•è°ƒç”¨æµ‹è¯•: âŒ - {e}")
        
        return True
        
    except Exception as e:
        print(f"    âŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹LLMå®¢æˆ·ç«¯åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("å¤šLLMå®¢æˆ·ç«¯åˆå§‹åŒ–", test_multi_llm_client_initialization),
        ("LLMå®¢æˆ·ç«¯ç”ŸæˆåŠŸèƒ½", test_llm_client_generate),
        ("èŠå¤©å®ŒæˆåŠŸèƒ½", test_chat_completion),
        ("APIç«¯ç‚¹é…ç½®", test_api_endpoints),
        ("é”™è¯¯å¤„ç†", test_error_handling),
        ("åŸºç¡€LLMå…¼å®¹æ€§", test_base_llm_compatibility)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
        
        print()
    
    print("=" * 60)
    print(f"ğŸ¯ æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰LLMå®¢æˆ·ç«¯æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
