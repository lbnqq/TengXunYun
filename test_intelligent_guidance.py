#!/usr/bin/env python3
"""
æµ‹è¯•æ™ºèƒ½å¼•å¯¼ç³»ç»ŸåŠŸèƒ½
Test intelligent guidance system functionality
"""

import sys
import os
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

def test_scenario_inference():
    """æµ‹è¯•åœºæ™¯æ¨æ–­åŠŸèƒ½"""
    print("ğŸ§  æµ‹è¯•åœºæ™¯æ¨æ–­åŠŸèƒ½")
    print("-" * 40)
    
    try:
        from src.core.guidance import EnhancedScenarioInferenceModule
        
        # åˆ›å»ºåœºæ™¯æ¨æ–­æ¨¡å—ï¼ˆä¸éœ€è¦LLMå®¢æˆ·ç«¯è¿›è¡ŒåŸºç¡€æµ‹è¯•ï¼‰
        inference_module = EnhancedScenarioInferenceModule(llm_client=None)
        
        # æµ‹è¯•æ–‡æ¡£æ ·æœ¬
        test_documents = {
            "æŠ€æœ¯æŠ¥å‘Š": """
            # ç³»ç»Ÿæ€§èƒ½åˆ†ææŠ¥å‘Š
            
            ## æ‘˜è¦
            æœ¬æŠ¥å‘Šåˆ†æäº†æ–°ç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡å’Œä¼˜åŒ–å»ºè®®ã€‚
            
            ## æ–¹æ³•è®º
            æˆ‘ä»¬ä½¿ç”¨äº†åŸºå‡†æµ‹è¯•å’Œè´Ÿè½½æµ‹è¯•æ¥è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚
            
            ## å®éªŒç»“æœ
            æµ‹è¯•ç»“æœæ˜¾ç¤ºç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„å“åº”æ—¶é—´ä¸º50msã€‚
            
            ## ç»“è®º
            ç³»ç»Ÿæ€§èƒ½æ»¡è¶³é¢„æœŸè¦æ±‚ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ã€‚
            """,
            
            "äº§å“ææ¡ˆ": """
            # æ–°äº§å“å¼€å‘ææ¡ˆ
            
            ## äº§å“æ¦‚è¿°
            æˆ‘ä»¬æè®®å¼€å‘ä¸€ä¸ªAIé©±åŠ¨çš„å®¢æˆ·æœåŠ¡å¹³å°ã€‚
            
            ## ç”¨æˆ·æ•…äº‹
            ä½œä¸ºå®¢æˆ·æœåŠ¡ä»£è¡¨ï¼Œæˆ‘å¸Œæœ›èƒ½å¤Ÿå¿«é€Ÿè·å¾—å®¢æˆ·é—®é¢˜çš„æ™ºèƒ½å»ºè®®ã€‚
            
            ## æŠ€æœ¯æ ˆ
            å‰ç«¯ä½¿ç”¨Reactï¼Œåç«¯ä½¿ç”¨Python Flaskï¼ŒAIæ¨¡å‹ä½¿ç”¨GPTã€‚
            
            ## å¼€å‘è®¡åˆ’
            é¢„è®¡6ä¸ªæœˆå®ŒæˆMVPï¼Œéœ€è¦5åå¼€å‘äººå‘˜ã€‚
            
            ## å•†ä¸šä»·å€¼
            é¢„æœŸèƒ½å¤Ÿæé«˜å®¢æˆ·æ»¡æ„åº¦30%ï¼Œé™ä½æœåŠ¡æˆæœ¬20%ã€‚
            """,
            
            "ä¼šè®®çºªè¦": """
            # é¡¹ç›®è¿›åº¦ä¼šè®®çºªè¦
            
            ## å‚ä¼šäººå‘˜
            - å¼ ä¸‰ï¼ˆé¡¹ç›®ç»ç†ï¼‰
            - æå››ï¼ˆæŠ€æœ¯è´Ÿè´£äººï¼‰
            - ç‹äº”ï¼ˆäº§å“ç»ç†ï¼‰
            
            ## ä¼šè®®è®®ç¨‹
            1. é¡¹ç›®è¿›åº¦å›é¡¾
            2. æŠ€æœ¯éš¾ç‚¹è®¨è®º
            3. ä¸‹å‘¨è®¡åˆ’
            
            ## è®¨è®ºè¦ç‚¹
            - å½“å‰è¿›åº¦ç¬¦åˆé¢„æœŸ
            - æ•°æ®åº“æ€§èƒ½éœ€è¦ä¼˜åŒ–
            - å‰ç«¯ç•Œé¢éœ€è¦è°ƒæ•´
            
            ## å†³ç­–äº‹é¡¹
            - å¢åŠ ä¸€åæ•°æ®åº“ä¸“å®¶
            - è°ƒæ•´UIè®¾è®¡æ–¹æ¡ˆ
            
            ## è¡ŒåŠ¨é¡¹
            - å¼ ä¸‰ï¼šè”ç³»HRæ‹›è˜æ•°æ®åº“ä¸“å®¶ï¼ˆæœ¬å‘¨äº”å‰ï¼‰
            - æå››ï¼šå®Œæˆæ•°æ®åº“ä¼˜åŒ–æ–¹æ¡ˆï¼ˆä¸‹å‘¨ä¸‰å‰ï¼‰
            - ç‹äº”ï¼šæä¾›æ–°çš„UIè®¾è®¡ç¨¿ï¼ˆä¸‹å‘¨ä¸€å‰ï¼‰
            """
        }
        
        # æµ‹è¯•æ¯ä¸ªæ–‡æ¡£çš„åœºæ™¯æ¨æ–­
        for doc_type, content in test_documents.items():
            print(f"  ğŸ“„ æµ‹è¯•æ–‡æ¡£ç±»å‹: {doc_type}")
            
            # æ‰§è¡Œåœºæ™¯æ¨æ–­
            result = inference_module.infer_scenario_and_roles(content)
            
            if "error" in result:
                print(f"    âŒ æ¨æ–­å¤±è´¥: {result['error']}")
                continue
            
            print(f"    âœ… æ¨æ–­æˆåŠŸ")
            print(f"    - æ–‡æ¡£ç±»å‹: {result.get('document_type', 'unknown')}")
            print(f"    - åœºæ™¯: {result.get('scenario', 'unknown')}")
            print(f"    - ä½œè€…è§’è‰²: {result.get('author_role', 'unknown')}")
            print(f"    - ç›®æ ‡å—ä¼—: {result.get('target_audience', 'unknown')}")
            print(f"    - æ–‡æ¡£ç›®çš„: {result.get('document_purpose', 'unknown')}")
            print(f"    - æ­£å¼ç¨‹åº¦: {result.get('formality_level', 'unknown')}")
            
            # æ˜¾ç¤ºç½®ä¿¡åº¦
            confidence_scores = result.get('confidence_scores', {})
            overall_confidence = confidence_scores.get('overall', 0.0)
            print(f"    - æ•´ä½“ç½®ä¿¡åº¦: {overall_confidence:.2f}")
            
            # æ˜¾ç¤ºæ”¯æŒè¯æ®
            evidence = result.get('supporting_evidence', [])
            if evidence:
                print(f"    - æ”¯æŒè¯æ®: {', '.join(evidence[:3])}")
            
            print()
        
        return True
        
    except Exception as e:
        print(f"    âŒ åœºæ™¯æ¨æ–­æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def test_scenario_recommendations():
    """æµ‹è¯•åœºæ™¯å»ºè®®åŠŸèƒ½"""
    print("ğŸ’¡ æµ‹è¯•åœºæ™¯å»ºè®®åŠŸèƒ½")
    print("-" * 40)
    
    try:
        from src.core.guidance import EnhancedScenarioInferenceModule
        
        inference_module = EnhancedScenarioInferenceModule(llm_client=None)
        
        # æµ‹è¯•ä¸åŒåœºæ™¯çš„å»ºè®®
        test_scenarios = [
            "Technical Report",
            "Product Proposal", 
            "Meeting Minutes",
            "Business Plan",
            "Research Paper"
        ]
        
        for scenario in test_scenarios:
            print(f"  ğŸ“‹ æµ‹è¯•åœºæ™¯: {scenario}")
            
            # è·å–åœºæ™¯å»ºè®®
            recommendations = inference_module.get_scenario_recommendations(scenario)
            
            if "error" in recommendations:
                print(f"    âŒ è·å–å»ºè®®å¤±è´¥: {recommendations['error']}")
                continue
            
            print(f"    âœ… è·å–å»ºè®®æˆåŠŸ")
            print(f"    - æè¿°: {recommendations.get('description', 'N/A')}")
            print(f"    - æ¨èå®¡é˜…è§’è‰²: {', '.join(recommendations.get('recommended_reviewer_roles', []))}")
            print(f"    - é»˜è®¤å®¡é˜…é‡ç‚¹: {', '.join(recommendations.get('default_review_focus', []))}")
            print(f"    - å»ºè®®å·¥å…·: {', '.join(recommendations.get('suggested_tools', []))}")
            
            # è·å–ä¸‹ä¸€æ­¥å»ºè®®
            next_steps = inference_module.suggest_next_steps_and_roles(scenario)
            print(f"    - ä¸‹ä¸€æ­¥å»ºè®®: {next_steps.get('suggestion', 'N/A')}")
            
            print()
        
        return True
        
    except Exception as e:
        print(f"    âŒ åœºæ™¯å»ºè®®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def test_confirmation_prompts():
    """æµ‹è¯•ç¡®è®¤æç¤ºç”Ÿæˆ"""
    print("ğŸ’¬ æµ‹è¯•ç¡®è®¤æç¤ºç”Ÿæˆ")
    print("-" * 40)
    
    try:
        from src.core.guidance import EnhancedScenarioInferenceModule
        
        inference_module = EnhancedScenarioInferenceModule(llm_client=None)
        
        # æ¨¡æ‹Ÿæ¨æ–­ç»“æœ
        mock_inference_result = {
            "scenario": "Technical Report",
            "author_role": "technical_lead",
            "target_audience": "technical_team",
            "confidence_scores": {"overall": 0.85},
            "supporting_evidence": [
                "Contains technical terminology",
                "Includes performance metrics",
                "Has methodology section"
            ],
            "alternative_scenarios": [
                {"scenario": "Research Paper", "confidence": 0.72}
            ]
        }
        
        # ç”Ÿæˆå¢å¼ºç¡®è®¤æç¤º
        print("  ğŸ“ ç”Ÿæˆå¢å¼ºç¡®è®¤æç¤º...")
        enhanced_prompt = inference_module.generate_enhanced_confirmation_prompt(mock_inference_result)
        
        print("    âœ… å¢å¼ºæç¤ºç”ŸæˆæˆåŠŸ")
        print("    æç¤ºå†…å®¹:")
        print("    " + "\n    ".join(enhanced_prompt.split("\n")[:10]))  # æ˜¾ç¤ºå‰10è¡Œ
        print("    ...")
        
        # ç”Ÿæˆä¼ ç»Ÿç¡®è®¤æç¤ºï¼ˆå‘åå…¼å®¹ï¼‰
        print("  ğŸ“ ç”Ÿæˆä¼ ç»Ÿç¡®è®¤æç¤º...")
        traditional_prompt = inference_module.generate_user_confirmation_prompt({
            "inferred_scenario": "Technical Report",
            "supporting_evidence": "technical terminology and performance metrics",
            "inferred_reporter_role": "Technical Lead",
            "inferred_reader_role": "Technical Team"
        })
        
        print("    âœ… ä¼ ç»Ÿæç¤ºç”ŸæˆæˆåŠŸ")
        print("    æç¤ºå†…å®¹:")
        print("    " + "\n    ".join(traditional_prompt.split("\n")[:5]))  # æ˜¾ç¤ºå‰5è¡Œ
        
        return True
        
    except Exception as e:
        print(f"    âŒ ç¡®è®¤æç¤ºæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def test_inference_quality_validation():
    """æµ‹è¯•æ¨æ–­è´¨é‡éªŒè¯"""
    print("ğŸ” æµ‹è¯•æ¨æ–­è´¨é‡éªŒè¯")
    print("-" * 40)
    
    try:
        from src.core.guidance import EnhancedScenarioInferenceModule
        
        inference_module = EnhancedScenarioInferenceModule(llm_client=None)
        
        # æµ‹è¯•ä¸åŒè´¨é‡çš„æ¨æ–­ç»“æœ
        test_cases = [
            {
                "name": "é«˜è´¨é‡æ¨æ–­",
                "result": {
                    "confidence_scores": {"overall": 0.9},
                    "supporting_evidence": ["evidence1", "evidence2", "evidence3", "evidence4"],
                    "alternative_scenarios": []
                }
            },
            {
                "name": "ä¸­ç­‰è´¨é‡æ¨æ–­",
                "result": {
                    "confidence_scores": {"overall": 0.6},
                    "supporting_evidence": ["evidence1", "evidence2"],
                    "alternative_scenarios": [{"confidence": 0.5}]
                }
            },
            {
                "name": "ä½è´¨é‡æ¨æ–­",
                "result": {
                    "confidence_scores": {"overall": 0.3},
                    "supporting_evidence": ["evidence1"],
                    "alternative_scenarios": [{"confidence": 0.25}]
                }
            }
        ]
        
        for test_case in test_cases:
            print(f"  ğŸ“Š æµ‹è¯•: {test_case['name']}")
            
            quality_report = inference_module.validate_inference_quality(test_case["result"])
            
            print(f"    - æ•´ä½“è´¨é‡: {quality_report['overall_quality']}")
            print(f"    - ç½®ä¿¡åº¦è¯„ä¼°: {quality_report['confidence_assessment']}")
            print(f"    - è¯æ®å¼ºåº¦: {quality_report['evidence_strength']}")
            print(f"    - ä¸€è‡´æ€§æ£€æŸ¥: {quality_report['consistency_check']}")
            
            if quality_report["improvement_suggestions"]:
                print(f"    - æ”¹è¿›å»ºè®®: {', '.join(quality_report['improvement_suggestions'])}")
            
            print()
        
        return True
        
    except Exception as e:
        print(f"    âŒ è´¨é‡éªŒè¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"    è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ™ºèƒ½å¼•å¯¼ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("åœºæ™¯æ¨æ–­", test_scenario_inference),
        ("åœºæ™¯å»ºè®®", test_scenario_recommendations),
        ("ç¡®è®¤æç¤º", test_confirmation_prompts),
        ("è´¨é‡éªŒè¯", test_inference_quality_validation)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
        
        print()
    
    print("=" * 60)
    print(f"ğŸ¯ æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ™ºèƒ½å¼•å¯¼ç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
